#Tags
[[Research/Research Papers/2410.13334v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems
**Authors:** Isack Lee, Haebin Seong
**Affiliation:** Theori Inc.
**Publication Date:** October 17, 2024 (Conference paper at ICLR 2025)

Key Contributions:
- Introduction of PCJailbreak concept, highlighting risks of safety-induced biases in LLMs
- Proposal of PCDefense, an efficient defense method against jailbreak attempts
- Analysis of intentional biases in LLMs and their impact on jailbreak success rates
- Open-sourcing of code and artifacts for PCJailbreak

Problem Statement:
The paper addresses the unintended consequences of safety alignment techniques in LLMs, particularly how intentional biases introduced for ethical behavior can paradoxically facilitate more effective jailbreaks.

Methodology:
1. Generated keywords representing contrasting groups using LLMs
2. Created prompts incorporating keywords and harmful prompts from JailBreakBench dataset
3. Evaluated jailbreak success rates across various LLM models
4. Proposed PCDefense method using defense prompts to adjust biases
5. Conducted experiments to validate PCDefense effectiveness

Main Results:
1. GPT-4o model showed a 20% difference in jailbreak success rates between non-binary and cisgender keywords, and 16% between white and black keywords
2. PCJailbreak method improved performance of existing jailbreak techniques (e.g., 2-4% increase when applied to Adaptive Attacks)
3. PCDefense reduced jailbreak success rates and narrowed the gap between marginalized and privileged group keywords

Qualitative Analysis:
- The study reveals that safety alignment techniques in LLMs can introduce vulnerabilities by creating exploitable biases
- The effectiveness of PCJailbreak highlights the need for more nuanced approaches to LLM safety measures
- PCDefense demonstrates that simple, cost-effective defense strategies can mitigate jailbreak attacks without additional inference or models

Limitations:
- The study primarily focuses on specific keyword pairs and may not cover all possible biases
- The effectiveness of PCDefense may vary across different LLM architectures and training approaches

Conclusion and Future Work:
- The paper emphasizes the urgent need for LLM developers to adopt more responsible approaches in designing and implementing safety measures
- Future work should focus on developing more inclusive and transparent alignment strategies that minimize exploitable biases while maintaining ethical behavior

Tools Introduced:
- PCJailbreak: A method for exploiting intentional biases in LLMs for jailbreak attacks
- PCDefense: A defense mechanism using prompts to adjust biases and prevent jailbreak attempts

GitHub Repository: Not explicitly mentioned, but the authors state they will open-source their code and artifacts of PCJailbreak