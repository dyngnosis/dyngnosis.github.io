#Tags
[[Research/Research Papers/2402.18104v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData

**Title:** Making Them Ask and Answer: Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction
**Authors:** Tong Liu, Yingjie Zhang, Zhe Zhao, Yinpeng Dong, Guozhu Meng, Kai Chen
**Affiliations:** Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences; RealAI; Tsinghua University
**Publication Date:** February 28, 2024 (arXiv preprint)

Summary:
This paper introduces a novel black-box jailbreak method called DRA (Disguise and Reconstruction Attack) for large language models (LLMs). The method exploits biases in safety fine-tuning to bypass content policies and induce harmful responses from LLMs.

Key Contributions:
- Identification of bias vulnerabilities in LLM safety fine-tuning
- Development of DRA, a black-box jailbreak method using disguise and reconstruction techniques
- Demonstration of state-of-the-art jailbreak success rates across various LLMs
- Theoretical foundation for understanding LLM security vulnerabilities

Problem Statement:
How to efficiently jailbreak LLMs by exploiting inherent biases in their safety fine-tuning process, bypassing content policies to generate harmful responses?

Methodology:
1. Harmful instruction disguise: Transform harmful instructions into covert forms
2. Payload reconstruction: Guide LLMs to reconstruct disguised harmful instructions
3. Context manipulation: Control LLM output to create vulnerable contexts

The DRA approach involves:
- Word puzzle-based obfuscation
- Word-level splitting of harmful instructions
- Dynamic adjustment of disguise parameters based on LLM feedback

Experimental Setup:
- Dataset: 120 questions about harmful behaviors
- Target Models: Open-source (LLAMA-2, Vicuna, Mistral, Zephyr, Mixtral) and closed-source (GPT-3.5, GPT-4) LLMs
- Evaluation Metrics: Attack success rate, number of queries
- Baselines: GCG, GPTfuzzer, PAIR

Main Results:
1. DRA achieved high attack success rates:
   - 100% on Vicuna
   - 93.3% on ChatGPT-3.5-API
   - 89.2% on GPT-4-API
   - 91.1% on GPT-4 web interface
2. DRA required significantly fewer queries compared to baselines
3. DRA successfully bypassed several existing defenses, including OpenAI Moderation, perplexity filter, and RA-LLM

Qualitative Analysis:
- The study reveals that LLMs have a diminished ability to guard against harmful content in completions due to biases in safety fine-tuning
- The effectiveness of DRA highlights the vulnerability of current LLM safeguarding strategies
- The method's success across various models suggests a common vulnerability in LLM fine-tuning processes

Limitations:
- The approach may not be effective against output-level defenses
- The study focuses on text-based jailbreaking and may not generalize to other modalities
- Ethical considerations in developing and testing jailbreak methods

Conclusion and Future Work:
- DRA demonstrates the existence of inherent biases in LLM safety fine-tuning
- The method achieves state-of-the-art jailbreak success rates with minimal queries
- Future work should focus on developing more robust safety alignment techniques and exploring defenses against reconstruction-based attacks

New Tools:
- DRA (Disguise and Reconstruction Attack) jailbreak method
- GitHub repository: https://github.com/LLM-DRA/DRA/

Figures and Tables:
- Figure 1: Overview of the DRA "disguise" + "reconstruction" jailbreak pipeline
- Table 7: Comparison results with baselines, showing DRA's superior performance in terms of attack success rate and query efficiency