#Tags
[[Research/Research Papers/2406.05498v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** SELFDEFEND: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner
**Authors:** Xunguang Wang, Daoyuan Wu, Zhenlan Ji, Zongjie Li, Pingchuan Ma, Shuai Wang, Yingjiu Li, Yang Liu, Ning Liu, Juergen Rahmel
**Affiliations:** The Hong Kong University of Science and Technology, University of Oregon, Nanyang Technological University, City University of Hong Kong, HSBC
**Publication Date:** June 8, 2024

Summary:
This paper introduces SELFDEFEND, a generic LLM jailbreak defense framework inspired by shadow stacks. It establishes a shadow LLM defense instance to protect the target LLM and collaborate for checkpoint-based access control. The approach is effective against various jailbreak attacks while incurring minimal delays and being compatible with both open-source and closed-source LLMs.

Key Contributions:
- Introduction of SELFDEFEND, a novel jailbreak defense framework
- Empirical validation of LLMs' capability to identify harmful prompts/intentions
- Demonstration of significant attack success rate (ASR) reduction using SELFDEFEND
- Development of tuned open-source defense models using data distillation
- Evaluation of defense effectiveness, delay impact, and robustness against targeted attacks

Problem Statement:
Existing jailbreak defenses struggle to handle all types of attacks, minimize delays, and work with both open-source and closed-source LLMs. SELFDEFEND aims to address these challenges with a practical and effective solution.

Methodology:
1. Establish a shadow LLM defense instance alongside the target LLM
2. Use dedicated detection prompts (Pdirect or Pintent) to identify harmful content
3. Implement checkpoint-based access control for collaborative defense
4. Evaluate using GPT-3.5/4 models across various jailbreak attacks
5. Fine-tune open-source models (Llama-2-7b) using data distillation for improved defense

Main Results:
1. SELFDEFEND reduces ASR by 8.97-95.74% (avg. 60%) for GPT-3.5 and 36.36-100% (avg. 83%) for GPT-4
2. Negligible impact on normal queries (2-3% reduction in ASR)
3. Low extra delay for normal prompts (0-0.01 seconds) and attack scenarios (max 0.25 seconds)
4. Tuned models outperform four SOTA defenses and match GPT-4-based SELFDEFEND performance
5. Robustness against targeted GCG and prompt injection attacks

Qualitative Analysis:
- SELFDEFEND's effectiveness stems from leveraging both the target LLM's safety alignment and the defense LLM's dedicated detection
- The framework's flexibility allows for adaptation to various LLM architectures and attack types
- The approach provides a balance between security and usability, addressing practical concerns in LLM deployment

Limitations:
- Potential increase in false positives when using multiple shadow stacks
- Reliance on the quality of the defense LLM's detection capabilities
- Possible vulnerabilities to advanced, unseen attack methods

Conclusion and Future Work:
SELFDEFEND demonstrates a practical and effective approach to defending LLMs against jailbreak attacks. Future work may focus on:
1. Improving detection accuracy and reducing false positives
2. Adapting the framework to emerging attack vectors
3. Exploring integration with other security measures for comprehensive LLM protection

Tools Introduced:
- SELFDEFEND framework (GitHub repository not provided in the paper)

Relevant Figures:
Figure 1: High-level overview of the SELFDEFEND framework
Figure 3: Extra delay caused by SELFDEFEND for normal and jailbreak prompts
Figure 4: Training procedure for fine-tuning the defense model