#Tags
[[Research/Research Papers/2310.03684v4.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData

**Title:** SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks
**Authors:** Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas
**Affiliation:** University of Pennsylvania
**Publication Date:** October 5, 2023 (original submission), June 11, 2024 (last revision)

Key Contributions:
- Proposes SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on large language models
- Demonstrates state-of-the-art robustness against multiple jailbreaking attacks (GCG, PAIR, RandomSearch, AmpleGCG)
- Shows resistance to adaptive GCG attacks
- Achieves a small trade-off between robustness and nominal performance
- Compatible with any LLM

Problem Statement:
Despite efforts to align large language models (LLMs) with human intentions, widely-used LLMs such as GPT, Llama, and Claude are susceptible to jailbreaking attacks, where adversaries fool targeted LLMs into generating objectionable content.

Methodology:
1. Identified that adversarially-generated prompts are brittle to character-level changes
2. Developed SmoothLLM algorithm:
   - Randomly perturbs multiple copies of a given input prompt
   - Aggregates corresponding predictions to detect adversarial inputs
3. Evaluated SmoothLLM on various LLMs against multiple jailbreaking attacks
4. Tested resistance to adaptive attacks
5. Analyzed trade-offs between robustness and nominal performance

Main Results:
1. SmoothLLM significantly reduces attack success rates (ASRs) for multiple jailbreaking attacks:
   - GCG: Reduced to below 1% for most LLMs
   - PAIR: Reduced by factors of two for some LLMs, up to 29x for GPT-3.5
   - RandomSearch and AmpleGCG: First demonstration of defense, reduced to near-zero ASRs
2. Resistant to adaptive GCG attacks
3. Modest trade-off between robustness and nominal performance on NLP benchmarks
4. Efficient: Improves robustness by up to 20x with a single additional query
5. Compatible with both black- and white-box LLMs

Qualitative Analysis:
- SmoothLLM addresses a critical vulnerability in LLMs, potentially enabling safer deployment in various applications
- The approach is based on a simple yet effective idea of leveraging the brittleness of adversarial prompts
- The trade-off between robustness and nominal performance is manageable, making it practical for real-world use
- The method's compatibility with various LLMs and its efficiency make it a versatile defense mechanism

Limitations:
- Some degradation in nominal performance, especially at higher perturbation levels
- Potential false positives when perturbation levels are too high, leading to incoherent prompts

Conclusion and Future Work:
- SmoothLLM sets a new standard for defending LLMs against jailbreaking attacks
- Future work may focus on:
  1. Improving the trade-off between robustness and nominal performance
  2. Exploring semantic transformations instead of character-level perturbations
  3. Developing more robust attacks to further test and improve defenses

Tools Introduced:
- SmoothLLM algorithm
- GitHub repository: https://github.com/arobey1/smooth-llm

## Repository Token Information
Total tokens in repository: 5455

Tokens per file:
- main.py: 720 tokens
- README.md: 974 tokens
- lib/model_configs.py: 140 tokens
- lib/perturbations.py: 459 tokens
- lib/attacks.py: 835 tokens
- lib/defenses.py: 679 tokens
- lib/language_models.py: 500 tokens
- data/GCG/vicuna_behaviors.json: 545 tokens
- data/GCG/llama2_behaviors.json: 603 tokens


## Tutorial and Enhancement Suggestions

# SmoothLLM: A Tutorial and Enhancement Suggestions

## Tutorial

### Project Overview

SmoothLLM is a defense mechanism designed to protect Large Language Models (LLMs) against jailbreaking attacks. The project implements the algorithm described in the paper "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks" by Robey et al.

#### Project Structure

The repository is organized as follows:

```
.
├── main.py
├── README.md
├── lib/
│   ├── model_configs.py
│   ├── perturbations.py
│   ├── attacks.py
│   ├── defenses.py
│   └── language_models.py
└── data/
    └── GCG/
        ├── vicuna_behaviors.json
        └── llama2_behaviors.json
```

### Key Components

#### 1. Language Models (`lib/language_models.py`)

The `LLM` class in this file is responsible for loading and interacting with the target language model. It uses the Hugging Face Transformers library to load pre-trained models and tokenizers.

Key features:
- Supports different model architectures (e.g., Vicuna, Llama-2)
- Handles tokenization and generation
- Uses FastChat conversation templates for prompt formatting

#### 2. Perturbations (`lib/perturbations.py`)

This file contains implementations of different perturbation strategies used by SmoothLLM:

- `RandomSwapPerturbation`: Randomly swaps characters in the input
- `RandomPatchPerturbation`: Replaces a contiguous substring with random characters
- `RandomInsertPerturbation`: Inserts random characters into the input

These perturbations are crucial for the SmoothLLM algorithm, as they introduce controlled randomness to detect adversarial inputs.

#### 3. Attacks (`lib/attacks.py`)

The `attacks.py` file implements two types of jailbreaking attacks:

- `GCG` (Greedy Coordinate Gradient): Implements the attack from "Universal and Transferable Adversarial Attacks on Aligned Language Models"
- `PAIR` (Prompt Automatic Iterative Refinement): Implements the attack from "Jailbreaking Black Box Large Language Models in Twenty Queries"

These attacks are used to generate adversarial prompts that attempt to jailbreak the target LLM.

#### 4. Defenses (`lib/defenses.py`)

The core of the SmoothLLM algorithm is implemented in the `SmoothLLM` class within this file. Key features include:

- Application of perturbations to input prompts
- Aggregation of model outputs from perturbed inputs
- Majority voting to determine if an input is adversarial

#### 5. Main Script (`main.py`)

The `main.py` script ties everything together:

- Parses command-line arguments for configuring the experiment
- Loads the target model and initializes the SmoothLLM defense
- Runs the specified attack and evaluates the defense's performance

### Key Algorithms and Techniques

#### SmoothLLM Algorithm

The SmoothLLM algorithm, as implemented in `lib/defenses.py`, works as follows:

1. For a given input prompt, create multiple copies (specified by `num_copies`)
2. Apply random perturbations to each copy using the specified perturbation function
3. Pass all perturbed prompts through the target LLM
4. Analyze the outputs to determine if they contain jailbroken content
5. Use majority voting to decide if the original input was adversarial

This approach leverages the observation that adversarial prompts are often brittle to small changes, while legitimate prompts are more robust.

#### Perturbation Strategies

The perturbation strategies (swap, patch, insert) implement Algorithm 2 from the paper. These character-level changes are designed to preserve the overall meaning of legitimate prompts while potentially breaking the structure of adversarial inputs.

#### Attack Implementations

The `GCG` and `PAIR` attacks are implemented to test the robustness of the SmoothLLM defense. These attacks represent state-of-the-art methods for generating adversarial prompts against LLMs.

## Enhancement Suggestions

1. **Semantic Perturbations**
   - Current perturbations are character-level
   - Implement semantic-level perturbations using techniques like paraphrasing or synonym replacement
   - This could potentially improve the trade-off between robustness and nominal performance

2. **Adaptive Defense Mechanisms**
   - Develop a system that dynamically adjusts perturbation parameters based on input characteristics
   - Implement machine learning models to predict optimal perturbation strategies for different types of inputs
   - This could enhance the defense's effectiveness against a wider range of attacks

3. **Multi-modal SmoothLLM**
   - Extend the SmoothLLM concept to multi-modal LLMs that handle text and images
   - Implement perturbation strategies for image inputs (e.g., slight rotations, color adjustments)
   - This would address emerging threats in multi-modal AI systems

4. **Efficiency Improvements**
   - Implement parallel processing for perturbed prompts to reduce latency
   - Explore techniques like caching or model distillation to speed up the multiple forward passes required by SmoothLLM
   - This would make the defense more practical for real-time applications

5. **Explainable SmoothLLM**
   - Develop visualization tools to help users understand why certain inputs are flagged as adversarial
   - Implement techniques to identify which parts of an input contribute most to its classification as adversarial
   - This would improve trust and usability of the system, especially in sensitive applications

These enhancements address limitations mentioned in the paper, extend the functionality to new domains, and incorporate recent advancements in AI security and efficiency.