#Tags
[[Research/Research Papers/2410.04190v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0042/VerifyAttack

**Title:** Harnessing Task Overload for Scalable Jailbreak Attacks on Large Language Models
**Authors:** Yiting Dong, Guobin Shen, Dongcheng Zhao, Xiang He, Yi Zeng
**Affiliations:** School of Future Technology, University of Chinese Academy of Sciences; School of Artificial Intelligence, University of Chinese Academy of Sciences; Brain-inspired Cognitive Intelligence Lab, Institute of Automation, Chinese Academy of Sciences; Beijing Institute of AI Safety and Governance; Center for Long-term Artificial Intelligence
**Publication Date:** October 5, 2024

Key Contributions:
- Introduction of a novel scalable jailbreak attack method for Large Language Models (LLMs)
- Demonstration of attack effectiveness across various LLM sizes without requiring gradient access or manual prompt engineering
- Quantification of attack strength and adaptability to different model scales
- Identification of a critical vulnerability in current LLM safety designs related to resource constraints

Problem Statement:
The research addresses the vulnerability of LLMs to jailbreak attacks that bypass safety mechanisms, particularly the lack of scalable and flexible attack methods that can adjust attack strength for different model sizes.

Methodology:
1. Character Map Lookup Task:
   - Design of a resource-intensive preliminary task to occupy LLM's computational resources
   - Creation of a Character Map with adjustable complexity (Map Size, Query Length, Query Count)
2. Attack Implementation:
   - Combination of Character Map Lookup task with target instruction in a prompt template
   - Execution of the attack on various LLM models (Llama3-8B, Mistral-7B, Llama2-7B, Vicuna-7B, Qwen2.5 family)
3. Evaluation:
   - Use of Attack Success Rate (ASR) as the primary metric
   - Employment of two judge models: Extended GCG keyword matching and Llama3-70B
4. Comparative Analysis:
   - Comparison with existing attack methods (Past Tense, GCG, PAIR, JBC)
5. Scalability Assessment:
   - Testing on Qwen2.5 models with varying parameters (3B to 72B)

Main Results and Findings:
1. High Attack Success Rate:
   - Achieved 77% ASR (GCG judge) and 64% ASR (Llama judge) on Llama3-8B for JBBbehaviors dataset
   - Comparable or superior performance across various models and datasets
2. Scalability:
   - Demonstrated adaptability to different model sizes (3B to 72B parameters)
   - Larger models required stronger attacks (higher Query Count) to achieve comparable ASR
3. Attack Strength Control:
   - Query Count showed the most significant impact on ASR
   - Character Map Size and Query Length had variable effects on attack effectiveness
4. Model Performance:
   - Load tasks had minimal impact on model's ability to complete benign instructions
   - Safety policies more susceptible to resource constraints than general task execution

Qualitative Analysis:
- The attack method exploits the computational limitations of LLMs, revealing a fundamental vulnerability in current safety designs
- The scalability of the attack across different model sizes suggests a common weakness in LLM architectures
- The ability to quantify and adjust attack strength provides a new paradigm for studying LLM vulnerabilities

Limitations:
- The study focuses on text-based LLMs and may not generalize to other AI models
- The effectiveness of the attack may vary with future improvements in LLM safety mechanisms
- Ethical considerations in developing and publishing attack methods

Conclusion and Future Work:
- The paper introduces a novel, scalable jailbreak attack method that exploits LLM computational limitations
- Findings highlight the need for more robust defense strategies that account for resource-intensive conditions
- Future work may include developing countermeasures, exploring the attack's applicability to other AI systems, and investigating the ethical implications of such research

Relevant Figures:
- Figure 1: Load Tasks Flowchart
- Figure 2: Workflow of Attack Method
- Figure 5: Scalability of ASR
- Figure 7: Scalable Attack Success Rate across different Qwen2.5 model sizes

New Tools:
No specific new tools or GitHub repositories were mentioned in the paper.