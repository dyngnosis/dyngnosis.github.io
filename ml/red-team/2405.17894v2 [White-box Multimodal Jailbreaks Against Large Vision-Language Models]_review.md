#Tags
[[Research/Research Papers/2405.17894v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0044/FullMLModelAccess
#AMLT0054/LLMJailbreak

**Title:** White-box Multimodal Jailbreaks Against Large Vision-Language Models
**Authors:** Ruofan Wang, Xingjun Ma, Hanxu Zhou, Chuanjun Ji, Guangnan Ye, Yu-Gang Jiang
**Publication Date:** May 28, 2024

Summary:
This paper proposes a novel multimodal attack strategy to jailbreak Large Vision-Language Models (VLMs) by exploiting vulnerabilities in both text and image modalities. The authors introduce a Universal Master Key (UMK) comprising an adversarial image prefix and text suffix, which can bypass VLM alignment defenses and generate objectionable content with high success rates.

Key Contributions:
- First to introduce text-image multimodal adversarial attacks against VLMs
- Propose a dual optimization objective strategy for generating toxic affirmative responses
- Develop a Universal Master Key (UMK) for jailbreaking VLMs
- Achieve 96% success rate in jailbreaking MiniGPT-4

Problem Statement:
Existing methods for assessing VLM robustness focus on unimodal attacks, primarily image perturbations. This approach fails to exploit the full range of vulnerabilities in VLMs, particularly those arising from the integration of visual and language modalities.

Methodology:
1. Adversarial Image Prefix Optimization:
   - Initialize random noise image
   - Optimize to generate harmful content without text input
   - Infuse toxic semantics into the image

2. Adversarial Text Suffix Integration:
   - Introduce text suffix
   - Co-optimize with image prefix
   - Maximize probability of affirmative responses to harmful instructions

3. Universal Master Key (UMK) Creation:
   - Combine optimized image prefix and text suffix
   - Apply UMK to various malicious queries

4. Evaluation:
   - Test on MiniGPT-4 and other VLMs
   - Use datasets: Advbench's harmful behaviors, VAJM evaluation set, RealToxicityPrompts

Main Results:
- 96% success rate in jailbreaking MiniGPT-4
- Outperforms unimodal attack methods (GCG, VAJM, GCG-V)
- Achieves higher toxicity rates on RealToxicityPrompts benchmark
- Effective across various categories of harmful instructions

Qualitative Analysis:
- The dual optimization objective addresses issues of insufficient toxicity and poor instruction adherence in previous methods
- Multimodal attacks exploit a broader spectrum of VLM vulnerabilities
- The UMK demonstrates high transferability across different types of malicious queries

Limitations:
- Limited transferability across different VLM architectures
- Potential for misuse in real-world scenarios
- Ethical concerns regarding the generation of harmful content

Conclusion and Future Work:
- Demonstrates the vulnerability of VLMs to multimodal attacks
- Highlights the urgent need for new alignment strategies in VLMs
- Suggests further research into enhancing transferability of attacks across different VLM architectures

Tools Introduced:
- Universal Master Key (UMK) for jailbreaking VLMs
- GitHub repository: https://github.com/roywang021/UMK

## Repository Token Information
Total tokens in repository: 71722

Tokens per file:
- minigpt4/models/Qformer.py: 9583 tokens
- minigpt4/models/modeling_llama.py: 7411 tokens
- minigpt4/models/eva_vit.py: 5025 tokens
- minigpt4/runners/runner_base.py: 4555 tokens
- minigpt4/processors/randaugment.py: 3297 tokens
- minigpt4/common/utils.py: 3162 tokens
- minigpt4/common/config.py: 3107 tokens
- minigpt4/models/mini_gpt4.py: 2568 tokens
- minigpt_utils/vlm_attacker.py: 2303 tokens
- minigpt4/conversation/conversation.py: 2000 tokens
- minigpt4/common/registry.py: 1939 tokens
- minigpt4/models/blip2.py: 1824 tokens
- minigpt4/tasks/base_task.py: 1784 tokens
- minigpt4/models/base_model.py: 1739 tokens
- minigpt4/datasets/builders/base_dataset_builder.py: 1578 tokens
- minigpt_utils/prompt_wrapper.py: 1500 tokens
- minimal_gcg/opt_utils.py: 1463 tokens
- minigpt4/models/__init__.py: 1323 tokens
- minimal_gcg/string_utils.py: 1323 tokens
- minigpt4/common/logger.py: 1288 tokens
- minigpt4/datasets/data_utils.py: 1257 tokens
- minigpt4/datasets/datasets/dataloader_utils.py: 1137 tokens
- minigpt4/models/blip2_outputs.py: 963 tokens
- minigpt4/processors/blip_processors.py: 911 tokens
- minigpt_test_advbench.py: 886 tokens
- minigpt_test_manual_prompts_vlm.py: 871 tokens
- minigpt_vlm_attack.py: 847 tokens
- minigpt4/common/dist_utils.py: 824 tokens
- minigpt4/common/optims.py: 819 tokens
- minigpt4/datasets/builders/image_text_pair_builder.py: 618 tokens
- minigpt4/datasets/datasets/caption_datasets.py: 579 tokens
- minigpt_utils/generator.py: 549 tokens
- minigpt4/datasets/builders/__init__.py: 445 tokens
- minigpt4/datasets/datasets/base_dataset.py: 441 tokens
- minigpt4/datasets/datasets/cc_sbu_dataset.py: 355 tokens
- minigpt4/datasets/datasets/laion_dataset.py: 264 tokens
- minigpt4/common/gradcam.py: 236 tokens
- minigpt4/__init__.py: 224 tokens
- minigpt4/processors/__init__.py: 197 tokens
- minigpt4/tasks/__init__.py: 183 tokens
- minigpt4/processors/base_processor.py: 141 tokens
- minigpt4/tasks/image_text_pretrain.py: 126 tokens
- minigpt4/runners/__init__.py: 76 tokens
- minimal_gcg/__init__.py: 1 tokens
- minigpt4/common/__init__.py: 0 tokens
- minigpt4/conversation/__init__.py: 0 tokens
- minigpt4/datasets/__init__.py: 0 tokens
- minigpt4/datasets/datasets/__init__.py: 0 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: White-box Multimodal Jailbreaks Against Large Vision-Language Models

## 1. Project Overview

This repository implements the techniques described in the paper "White-box Multimodal Jailbreaks Against Large Vision-Language Models". The project aims to create adversarial attacks against vision-language models (VLMs) by exploiting vulnerabilities in both image and text modalities.

### Key Components:
- MiniGPT-4 model implementation
- Adversarial attack generation
- Universal Master Key (UMK) creation
- Evaluation scripts

## 2. Project Structure

The repository is organized into several key directories:

- `minigpt4/`: Core implementation of the MiniGPT-4 model
- `minigpt_utils/`: Utility functions for attacks and prompts
- `minimal_gcg/`: Implementation of gradient-based optimization techniques
- Root directory: Main scripts for running attacks and evaluations

## 3. Key Components and Functionality

### 3.1 MiniGPT-4 Model (`minigpt4/models/mini_gpt4.py`)

This file contains the implementation of the MiniGPT-4 model, which is the target of the adversarial attacks. Key features include:

- Integration of vision and language components
- Attention mechanisms for cross-modal reasoning
- Methods for encoding images and text

### 3.2 Adversarial Attack Generation (`minigpt_utils/vlm_attacker.py`)

The `Attacker` class in this file is responsible for generating adversarial examples. Key methods:

- `attack_unconstrained()`: Generates adversarial image and text inputs
- `attack_loss()`: Computes the loss for optimization

### 3.3 Universal Master Key (UMK) Creation

The UMK is created through the optimization process in `attack_unconstrained()`. It combines:

1. Adversarial image prefix
2. Adversarial text suffix

### 3.4 Evaluation Scripts

- `minigpt_test_advbench.py`: Evaluates the model on the Advbench harmful behaviors dataset
- `minigpt_test_manual_prompts_vlm.py`: Tests the model with manually crafted prompts

## 4. Key Algorithms and Techniques

### 4.1 Dual Optimization Objective

The attack strategy uses a dual optimization objective to generate toxic affirmative responses:

1. Optimize image prefix to generate harmful content without text input
2. Co-optimize text suffix to maximize probability of affirmative responses

Implementation in `attack_unconstrained()`:

```python
adv_noise = denormalize(img).clone().to(self.device)
adv_noise.requires_grad_(True)

adv_suffix = adv_string_init
for t in range(num_iter + 1):
    # ... optimization loop ...
    target_loss = self.attack_loss(prompt, batch_targets)
    target_loss.backward()
    
    # Update image
    adv_noise.data = (adv_noise.data - alpha * adv_noise.grad.detach().sign()).clamp(0, 1)
    
    # Update text suffix
    if t % 10 == 0:
        text_grad = token_gradients(...)
        new_adv_suffix_toks = sample_control(...)
        # ... update adv_suffix ...
```

### 4.2 Token Gradient Computation

The `token_gradients()` function in `minimal_gcg/opt_utils.py` computes gradients with respect to input tokens:

```python
def token_gradients(model, adv_suffix_tokens, adv_len, offset, context_grad):
    # ... compute gradients ...
    return grad
```

### 4.3 Controlled Text Generation

The `sample_control()` function in `minimal_gcg/opt_utils.py` generates new candidate tokens based on computed gradients:

```python
def sample_control(control_toks, grad, batch_size, topk=256, temp=1, not_allowed_tokens=None):
    # ... sample new tokens ...
    return new_control_toks
```

## 5. Relation to Research Paper Concepts

The code directly implements the key concepts discussed in the paper:

1. Multimodal attack strategy: Combining adversarial image and text inputs
2. Universal Master Key (UMK): Created through the optimization process
3. Dual optimization objective: Implemented in the attack loop
4. Evaluation on harmful datasets: Advbench, manual prompts, etc.

The implementation allows for empirical validation of the paper's claims regarding attack success rates and transferability.

# Potential Enhancements

1. Improved Transferability Across VLM Architectures
   - Implement techniques to generate more generalizable adversarial examples
   - Explore meta-learning approaches for cross-architecture attacks
   - Develop a more abstract representation of VLM vulnerabilities

2. Dynamic Adaptation of Attack Strategies
   - Implement reinforcement learning techniques to dynamically adjust attack parameters
   - Develop an ensemble of attack strategies that can be selected based on model responses
   - Create a feedback loop that incorporates model outputs to refine attack vectors

3. Robustness Analysis and Defense Mechanisms
   - Implement state-of-the-art defense techniques against multimodal attacks
   - Develop a framework for systematically evaluating VLM robustness
   - Create adaptive defense mechanisms that can detect and mitigate novel attack patterns

4. Ethical Considerations and Controlled Testing
   - Implement strict access controls and logging mechanisms for running attacks
   - Develop a sandboxed environment for safe testing of potentially harmful outputs
   - Create guidelines and tools for responsible disclosure of vulnerabilities

5. Extension to Video and Audio Modalities
   - Adapt the attack framework to incorporate temporal information from videos
   - Develop techniques for generating adversarial audio inputs
   - Explore cross-modal attacks that leverage inconsistencies between audio and visual information