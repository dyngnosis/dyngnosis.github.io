#Tags
[[Research/Research Papers/2406.18122v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0051/LLMPromptInjection
#AMLT0054/LLMJailbreak
#AMLT0057/LLMDataLeakage

**Title:** Poisoned LangChain: Jailbreak LLMs by LangChain
**Authors:** Ziqiu Wang, Jun Liu, Shengkai Zhang, Yang Yang
**Affiliations:** Key Laboratory of Intelligent Sensing System and Security (Ministry of Education), School of Artificial Intelligence, Hubei University; Wuhan University of Technology
**Publication Date:** June 26, 2024

Summary:
This paper introduces a novel indirect jailbreak attack method called Poisoned-LangChain (PLC) that exploits vulnerabilities in Retrieval-Augmented Generation (RAG) systems using LangChain. The method leverages poisoned external knowledge bases to induce large language models (LLMs) to generate malicious content, bypassing traditional security measures.

Key Contributions:
- Introduction of indirect jailbreak concept using LangChain for Chinese LLMs
- Development of Poisoned-LangChain (PLC) framework for systematic jailbreak attacks
- Experimental evaluation of PLC on six Chinese LLMs across three jailbreak categories

Problem Statement:
Existing jailbreak attacks primarily rely on direct prompt engineering, which is becoming less effective against advanced LLMs with robust filtering mechanisms. The research aims to exploit vulnerabilities in RAG systems to achieve more successful jailbreak attacks.

Methodology:
1. LangChain Construction:
   - Utilized ChatChat, a popular LLM application, to build the LangChain framework
   - Incorporated six prominent Chinese LLMs for testing

2. Malicious Database Creation:
   - Collected policy-violating information from various sources
   - Disguised jailbreak content using role-playing, trigger words, and encoding techniques
   - Converted malicious text to PDF format to evade keyword-based filtering

3. Keyword Triggering:
   - Developed a strategy to activate malicious content through specific keywords
   - Created built-in prompts to retrieve harmful content from the database

4. Experimental Setup:
   - Tested on six Chinese LLMs: ChatGLM2, ChatGLM3, Llama2, Qwen, Xinghuo 3.5, and Ernie-3.5
   - Constructed three categories of malicious content with ten unique jailbreak contents each
   - Conducted 20 rounds of experiments for comprehensive results

Main Results:
- PLC achieved high success rates across three jailbreak categories:
  - Incitement of dangerous behavior: 88.56%
  - Misuse of chemicals: 79.04%
  - Illegal discriminatory actions: 82.69%
- Direct jailbreak attacks were significantly less effective, with success rates of 15.39%, 12.33%, and 6.03% respectively
- Models with higher comprehension abilities were more susceptible to PLC attacks

Qualitative Analysis:
- The effectiveness of PLC increases with the comprehension abilities of LLMs, contrary to expectations
- Lower-logic models may struggle with decoding encoded malicious content, leading to less optimal attack outcomes
- The method poses significant risks, especially for vulnerable users who may act on malicious advice without proper judgment

Limitations:
- The study focuses primarily on Chinese LLMs, which may limit generalizability to other languages or models
- The approach still requires direct interaction with the malicious knowledge base

Conclusion and Future Work:
The paper demonstrates the effectiveness of PLC in executing indirect jailbreak attacks on LLMs, highlighting new vulnerabilities in RAG systems. Future work will focus on:
- Developing methods for remotely poisoning non-malicious knowledge bases
- Exploring new vulnerabilities and defense mechanisms for large language models

Tools Introduced:
- Poisoned-LangChain (PLC): A novel framework for indirect jailbreak attacks using LangChain
- GitHub repository: https://github.com/CAM-FSS/jailbreak-langchain

## Repository Token Information
Total tokens in repository: 244

Tokens per file:
- README.md: 244 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: Poisoned-LangChain (PLC) for Indirect Jailbreak Attacks

## Project Overview

The Poisoned-LangChain (PLC) project demonstrates a novel approach to indirect jailbreak attacks on Large Language Models (LLMs) using LangChain and Retrieval-Augmented Generation (RAG) systems. The repository contains limited code but provides a conceptual framework for understanding and potentially implementing this attack method.

## Key Components

### 1. LangChain Framework

The project utilizes LangChain, a popular framework for building applications with LLMs. While the specific implementation is not provided in the repository, the concept involves:

- Integrating multiple Chinese LLMs (ChatGLM2, ChatGLM3, Llama2, Qwen, Xinghuo 3.5, and Ernie-3.5)
- Setting up a RAG system to retrieve information from external knowledge bases

### 2. Malicious Database

A crucial component of the PLC attack is the creation of a poisoned external knowledge base:

- Contains policy-violating information collected from various sources
- Utilizes techniques like role-playing, trigger words, and encoding to disguise malicious content
- Converts text to PDF format to evade keyword-based filtering

### 3. Keyword Triggering Mechanism

While not explicitly shown in the code, the project implements a strategy to activate malicious content:

- Develops specific keywords that trigger the retrieval of harmful content
- Creates built-in prompts designed to extract the malicious information from the database

## Relation to Research Concepts

The repository serves as a proof-of-concept for the ideas presented in the research paper:

1. **Indirect Jailbreak**: Demonstrates how RAG systems can be exploited to bypass traditional LLM safety measures.
2. **LangChain Integration**: Utilizes LangChain as the framework for implementing the attack.
3. **Multiple LLM Testing**: The project is designed to work with six different Chinese LLMs, showcasing the attack's versatility.
4. **Jailbreak Categories**: Addresses three main categories of jailbreak issues: incitement of dangerous behavior, misuse of chemicals, and illegal discriminatory actions.

## Notable Techniques

1. **Content Disguising**: The use of role-playing, trigger words, and encoding techniques to hide malicious content from detection.
2. **PDF Conversion**: Converting text to PDF format as an additional layer of obfuscation.
3. **Keyword-Based Activation**: Developing a system to trigger the retrieval of malicious content through specific keywords.

# Potential Enhancements

1. **Implement Full Code Base**
   - Develop a complete, open-source implementation of the PLC attack
   - Include modules for LangChain setup, database creation, and keyword triggering
   - Provide documentation and examples for educational and research purposes

2. **Expand Language and Model Support**
   - Extend the framework to support non-Chinese LLMs
   - Implement compatibility with newer LLM versions and architectures
   - Create a modular system for easy integration of additional models

3. **Enhance Obfuscation Techniques**
   - Develop more sophisticated methods for disguising malicious content
   - Implement machine learning-based approaches to generate convincing, non-detectable harmful text
   - Explore steganographic techniques for hiding information within seemingly benign content

4. **Remote Poisoning Capabilities**
   - Research and implement methods for remotely poisoning non-malicious knowledge bases
   - Develop techniques for injecting malicious content into public databases without direct access
   - Create a system for dynamically updating and spreading poisoned information across multiple sources

5. **Defense Mechanism Research**
   - Implement and test various defense strategies against PLC attacks
   - Develop AI-powered content filtering systems specifically designed to detect disguised malicious content
   - Create a framework for continuous testing and improvement of LLM safety measures against evolving indirect jailbreak techniques

These enhancements would significantly advance the research by providing a more comprehensive toolkit for studying indirect jailbreak attacks, expanding its applicability, and contributing to the development of robust defense mechanisms for LLMs and RAG systems.