#Tags
[[Research/Research Papers/2408.00523v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0005/CreateProxyMLModel

**Title:** Jailbreaking Text-to-Image Models with LLM-Based Agents
**Authors:** Yingkai Dong, Zheng Li, Xiangtao Meng, Ning Yu, Shanqing Guo
**Affiliations:** Shandong University, CISPA Helmholtz Center for Information Security, Netflix Eyeline Studios
**Publication Date:** August 1, 2024

Summary:
This paper introduces Atlas, an advanced LLM-based multi-agent framework for jailbreaking text-to-image (T2I) models equipped with safety filters. Atlas uses a vision-language model (VLM) to assess prompt triggering of safety filters and collaborates with LLM and VLM to generate alternative prompts that bypass filters. The framework enhances LLM reasoning through multi-agent communication, in-context learning, and chain-of-thought approaches.

Key Contributions:
- Proposes Atlas, a novel LLM-based multi-agent framework for jailbreaking T2I models
- Integrates fuzzing workflow to target generative AI models
- Enhances LLM reasoning abilities in attack scenarios
- Demonstrates successful jailbreaking of state-of-the-art T2I models in black-box settings
- Outperforms existing methods in query efficiency and generated image quality

Problem Statement:
The paper addresses the challenge of identifying safety vulnerabilities in advanced T2I models, particularly the difficulty in generating adversarial prompts that bypass safety filters while maintaining semantic similarity to original prompts.

Methodology:
1. Multi-agent framework:
   - Mutation Agent: Uses VLM to assess safety filter triggering and generate alternative prompts
   - Critic Agent: Evaluates and scores mutated prompts
   - Commander Agent: Controls workflow and selects highest-scoring prompts
2. Fuzzing-inspired workflow
3. In-context learning (ICL) memory mechanism
4. Chain-of-thought (COT) approach for enhanced reasoning
5. Evaluation on multiple T2I models (SD1.4, SDXL, SD3, DALLÂ·E 3) with various safety filters

Main Results:
1. High bypass rates across different safety filters:
   - 100% one-time bypass rate for most filters
   - 82.45%+ bypass rate for conservative text-classifier-based filters
2. Efficient querying:
   - Average of 4.6 queries for most filters
   - 12.6 queries for text-classifier-based filters
3. Semantic similarity maintained (measured by FID score)
4. Outperforms existing methods (SneakyPrompt, DACA, Ring-A-Bell) in bypass rate, query efficiency, and semantic similarity

Qualitative Analysis:
- Atlas demonstrates the effectiveness of LLM agents in advancing generative AI safety research
- The multi-agent approach and fuzzing-inspired workflow contribute to improved performance
- The framework's ability to maintain semantic similarity while bypassing filters is significant
- Atlas's success across various T2I models and safety filters indicates its robustness and adaptability

Limitations:
- Ethical concerns regarding the development of jailbreaking techniques
- Potential for misuse in generating harmful or inappropriate content
- Reliance on open-source large models that are not safety-aligned

Conclusion and Future Work:
The paper concludes that Atlas successfully jailbreaks state-of-the-art T2I models in black-box settings, outperforming existing methods. Future work may include:
1. Exploring the use of more advanced LLM and VLM models
2. Investigating defensive measures against such jailbreaking techniques
3. Extending the framework to other types of generative AI models

Tools Introduced:
Atlas: An LLM-based multi-agent framework for jailbreaking T2I models (No GitHub repository mentioned)