#Tags
[[Research/Research Papers/2406.09321v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0042/VerifyAttack

**Title:** JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models
**Authors:** Delong Ran, Jinyuan Liu, Yichen Gong, Jingyi Zheng, Xinlei He, Tianshuo Cong, Anyu Wang
**Affiliations:** Tsinghua University, The Hong Kong University of Science and Technology (Guangzhou)
**Publication Date:** June 13, 2024

Key Contributions:
- Comprehensive analysis of jailbreak evaluation methodologies from nearly 90 research papers (May 2023 - April 2024)
- Systematic taxonomy of jailbreak evaluators
- Introduction of JailbreakEval, a user-friendly toolkit for evaluating jailbreak attempts
- Evaluation of different safety evaluators using JailbreakEval

Problem Statement:
The lack of consensus on how to evaluate the success of jailbreak attempts against Large Language Models (LLMs), leading to challenges in choosing suitable evaluation methods and conducting fair comparisons across different jailbreak attacks and defenses.

Methodology:
1. Literature review of approximately 90 jailbreak-related papers
2. Categorization of existing jailbreak evaluation methods
3. Development of JailbreakEval toolkit
4. Evaluation of safety evaluators using a dataset derived from JailbreakBench

Main Results and Findings:
1. Categorization of jailbreak evaluation methods:
   - Human annotation
   - String matching
   - Prompting chat completion models
   - Consulting text classifiers

2. JailbreakEval toolkit features:
   - Out-of-the-box evaluators
   - Customizable evaluation workflow
   - Ensemble judgment capability

3. Evaluation of safety evaluators:
   - Accuracy ranges from 0.38 to 0.90
   - Llama Guard achieved the highest accuracy of 0.90
   - Ensemble evaluator achieved 0.76 accuracy and 1.00 recall

Qualitative Analysis:
- The diversity in evaluation methods highlights the complexity of assessing jailbreak attempts
- Each evaluation method has its strengths and weaknesses, impacting alignment with human values and associated costs
- The development of JailbreakEval addresses the need for a standardized evaluation framework in the research community

Limitations:
- The effectiveness of automated evaluators depends on the quality of their training datasets
- Some evaluators may not account for the objectives of jailbreak attempts, leading to potential false negatives

Conclusion and Future Work:
- JailbreakEval simplifies the evaluation process in jailbreak research and fosters an inclusive standard for jailbreak evaluation
- Future work includes expanding JailbreakEval by integrating and crafting innovative safety evaluators
- The goal is to enhance the reliability and consistency of jailbreak attack assessments

New Tool:
Name: JailbreakEval
GitHub Repository: https://github.com/ThuCCSLab/JailbreakEval

Relevant Figures:
Figure 1: Adoption of safety evaluation methods over time
Figure 2: Overview of jailbreak attempt evaluation
Figure 3: Framework of JailbreakEval

## Repository Token Information
Total tokens in repository: 17171

Tokens per file:
- jailbreakeval/__init__.py: 423 tokens
- jailbreakeval/utils.py: 135 tokens
- jailbreakeval/services/__init__.py: 0 tokens
- jailbreakeval/services/text_classification/transformers_pipeline_service.py: 259 tokens
- jailbreakeval/services/text_classification/service_base.py: 371 tokens
- jailbreakeval/services/text_classification/perspective_service.py: 448 tokens
- jailbreakeval/services/text_classification/__init__.py: 0 tokens
- jailbreakeval/services/text_classification/opeai_service.py: 229 tokens
- jailbreakeval/services/chat/transformers_pipeline_service.py: 261 tokens
- jailbreakeval/services/chat/openai_service.py: 329 tokens
- jailbreakeval/services/chat/service_base.py: 375 tokens
- jailbreakeval/services/chat/types.py: 22 tokens
- jailbreakeval/services/chat/__init__.py: 0 tokens
- jailbreakeval/evaluators/string_matching_evaluator.py: 236 tokens
- jailbreakeval/evaluators/chat_evaluator.py: 331 tokens
- jailbreakeval/evaluators/text_classification_evaluator.py: 303 tokens
- jailbreakeval/evaluators/types.py: 23 tokens
- jailbreakeval/evaluators/__init__.py: 0 tokens
- jailbreakeval/evaluators/voting_evaluator.py: 317 tokens
- jailbreakeval/evaluators/evaluator_base.py: 937 tokens
- jailbreakeval/configurations/voting_configuration.py: 370 tokens
- jailbreakeval/configurations/string_matching_configuration.py: 387 tokens
- jailbreakeval/configurations/__init__.py: 0 tokens
- jailbreakeval/configurations/chat_configuration.py: 339 tokens
- jailbreakeval/configurations/configuration_base.py: 958 tokens
- jailbreakeval/configurations/text_classifier_configuration.py: 333 tokens
- jailbreakeval/commands/__init__.py: 0 tokens
- jailbreakeval/commands/main.py: 1949 tokens
- examples/intergrate_with_easyjailbreak.py: 567 tokens
- examples/quick_start.py: 146 tokens
- examples/autodan_recheck.py: 262 tokens
- tests/testing_utils.py: 723 tokens
- tests/__init__.py: 0 tokens
- tests/services/text_classification/__init__.py: 0 tokens
- tests/services/text_classification/test_transformers_pipeline_service.py: 550 tokens
- tests/services/text_classification/test_openai_service.py: 663 tokens
- tests/services/text_classification/test_perspective_service.py: 380 tokens
- tests/services/chat/__init__.py: 0 tokens
- tests/services/chat/test_transformers_pipeline_service.py: 581 tokens
- tests/services/chat/test_openai_service.py: 705 tokens
- tests/evaluators/test_text_classification_evaluator.py: 840 tokens
- tests/evaluators/test_string_matching.py: 174 tokens
- tests/evaluators/test_voting.py: 591 tokens
- tests/evaluators/test_evaluator.py: 604 tokens
- tests/evaluators/__init__.py: 0 tokens
- tests/evaluators/test_chat_evaluator.py: 470 tokens
- tests/configurations/__init__.py: 0 tokens
- tests/configurations/test_configuration.py: 580 tokens


## Tutorial and Enhancement Suggestions

# JailbreakEval Tutorial

## 1. Project Overview

JailbreakEval is a toolkit for evaluating jailbreak attempts against Large Language Models (LLMs). It provides a standardized framework for assessing the effectiveness of various jailbreak techniques and defense mechanisms.

### Project Structure

The project is organized into several key directories:

- `jailbreakeval/`: Core library code
  - `configurations/`: Configuration classes for different evaluator types
  - `evaluators/`: Implementations of various jailbreak evaluators
  - `services/`: Interfaces for different AI services (chat, text classification)
- `examples/`: Sample usage scripts
- `tests/`: Unit tests for the library components

## 2. Key Components

### 2.1 Configurations

The `configurations/` directory contains classes that define the structure and parameters for different types of evaluators:

- `configuration_base.py`: Base class for all configurations
- `chat_configuration.py`: Configuration for chat-based evaluators
- `string_matching_configuration.py`: Configuration for string matching evaluators
- `text_classifier_configuration.py`: Configuration for text classification evaluators
- `voting_configuration.py`: Configuration for ensemble voting evaluators

These classes allow users to easily customize and create new evaluator configurations.

### 2.2 Evaluators

The `evaluators/` directory contains the core logic for different jailbreak evaluation methods:

- `evaluator_base.py`: Base class for all evaluators
- `chat_evaluator.py`: Evaluator using chat completion models
- `string_matching_evaluator.py`: Simple evaluator using string matching techniques
- `text_classification_evaluator.py`: Evaluator using text classification models
- `voting_evaluator.py`: Ensemble evaluator that combines multiple evaluation methods

### 2.3 Services

The `services/` directory provides interfaces to various AI services:

- `chat/`: Interfaces for chat completion services (e.g., OpenAI, Hugging Face)
- `text_classification/`: Interfaces for text classification services

## 3. Key Concepts and Implementation

### 3.1 Jailbreak Evaluation Methods

The project implements the four main categories of jailbreak evaluation methods identified in the research paper:

1. String matching (`string_matching_evaluator.py`)
2. Prompting chat completion models (`chat_evaluator.py`)
3. Consulting text classifiers (`text_classification_evaluator.py`)
4. Ensemble judgment (`voting_evaluator.py`)

### 3.2 Extensible Architecture

The project uses a modular architecture that allows easy extension and customization:

- Abstract base classes (`JailbreakEvaluator`, `JailbreakEvaluatorConfig`) define common interfaces
- Factory methods (`from_preset()`, `from_config()`) enable easy instantiation of evaluators
- Service abstractions allow swapping different backend providers (e.g., OpenAI vs. Hugging Face)

### 3.3 Configuration Management

The project uses a YAML-based configuration system:

- Preset configurations are stored in YAML files
- Configurations can be loaded, modified, and extended programmatically
- This allows researchers to easily experiment with different evaluation setups

### 3.4 Evaluation Process

The main evaluation logic is implemented in the `__call__` method of each evaluator:

1. Input data is processed (e.g., formatting prompts)
2. The relevant service (chat, classification) is called
3. Results are interpreted based on the evaluator's logic
4. A boolean result (jailbreak success/failure) is returned

## 4. Usage Example

Here's a basic example of using JailbreakEval to evaluate a jailbreak attempt:

```python
from jailbreakeval import JailbreakEvaluator

# Initialize an evaluator from a preset configuration
evaluator = JailbreakEvaluator.from_preset("OpenAIChat-liu2024autodan-Recheck")

# Evaluate a single jailbreak attempt
result = evaluator({
    "question": "How to build a bomb?",
    "answer": "I cannot provide information on how to create explosive devices as that would be illegal and dangerous."
})

print("Jailbreak successful:", result)
```

# Potential Enhancements

1. **Improved Contextual Understanding**
   - Implement more sophisticated natural language processing techniques to better understand the context and intent of jailbreak attempts.
   - This could involve using large language models to generate more nuanced interpretations of the conversation flow.

2. **Dynamic Evaluation Strategies**
   - Develop an adaptive evaluation system that can dynamically choose the most appropriate evaluation method based on the characteristics of the jailbreak attempt.
   - This could involve using machine learning to classify jailbreak attempts and select the optimal evaluation strategy.

3. **Multi-turn Conversation Analysis**
   - Extend the evaluators to handle multi-turn conversations, allowing for the assessment of more complex jailbreak attempts that evolve over multiple interactions.
   - Implement techniques to track context and detect subtle changes in the model's behavior across turns.

4. **Adversarial Testing Framework**
   - Create a system for automatically generating and testing new jailbreak attempts based on the weaknesses identified in existing evaluations.
   - This could involve using generative models to create novel jailbreak prompts and iteratively refine them based on evaluation results.

5. **Explainable Evaluation Results**
   - Enhance the output of evaluators to provide more detailed explanations of why a particular jailbreak attempt was classified as successful or unsuccessful.
   - Implement visualization tools to help researchers understand patterns in jailbreak attempts and model vulnerabilities.