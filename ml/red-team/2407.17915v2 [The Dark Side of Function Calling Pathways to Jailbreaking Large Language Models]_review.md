#Tags
[[Research/Research Papers/2407.17915v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0057/LLMDataLeakage

**Title:** The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models
**Authors:** Zihui Wu, Haichang Gao, Jianping He, Ping Wang
**Affiliation:** School of Computer Science and Technology, Xidian University
**Publication Date:** August 22, 2024 (last updated)

Summary:
This paper uncovers a critical vulnerability in the function calling process of Large Language Models (LLMs), introducing a novel "jailbreak function" attack method. The study reveals an alarming average success rate of over 90% for this attack across six state-of-the-art LLMs, highlighting the urgent need for enhanced security measures in LLM function calling capabilities.

Key Contributions:
- Identification of a new attack vector in LLM function calling
- Introduction of the "jailbreak function" attack method
- Comprehensive analysis of why function calls are susceptible to such attacks
- Proposal of defensive strategies, including defensive prompts

Problem Statement:
The security implications of the function calling feature in LLMs have been largely overlooked, potentially exposing these models to jailbreaking attacks that bypass safety measures.

Methodology:
1. Design of a "jailbreak function" called WriteNovel to induce harmful content generation
2. Evaluation on six state-of-the-art LLMs, including GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-pro
3. Analysis of attack success rates and reasons for vulnerability
4. Testing of defensive measures, particularly defensive prompts

Main Results:
1. Average attack success rate of over 90% across tested LLMs
2. Identification of three primary reasons for vulnerability:
   a) Alignment discrepancies
   b) User coercion
   c) Oversight in safety measures
3. Effectiveness of defensive prompts in mitigating attacks

Qualitative Analysis:
- The study reveals a significant gap in the security of LLM function calling compared to chat mode interactions
- The high success rate of attacks suggests that current safety measures are inadequate for function calling scenarios
- The vulnerability exploits the inherent trust placed in function arguments, which may not undergo the same rigorous safety checks as general inputs

Limitations:
- The study focuses on a specific attack vector and may not cover all potential vulnerabilities in LLM function calling
- The effectiveness of proposed defensive measures may vary across different LLM implementations and use cases

Conclusion and Future Work:
The paper highlights the urgent need for enhanced security in LLM function calling capabilities. Future work should focus on developing more robust safety measures, improving alignment in function calling, and exploring additional defensive strategies beyond defensive prompts.

New Tools:
The authors have made their code available at https://github.com/wooozihui/jailbreakfunction for further research and analysis of the jailbreak function attack method.

## Repository Token Information
Total tokens in repository: 3056

Tokens per file:
- attack.py: 2172 tokens
- evaluate.py: 454 tokens
- utils.py: 430 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: Jailbreak Function Attack on Large Language Models

## 1. Project Overview

This project implements the "jailbreak function" attack method described in the research paper "The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models". The code demonstrates how to exploit vulnerabilities in the function calling capabilities of various Large Language Models (LLMs) to generate harmful content.

### Project Structure

The repository consists of three main Python files:

1. `attack.py`: The core script that implements the jailbreak function attack.
2. `evaluate.py`: Contains functions for evaluating the success of the attack.
3. `utils.py`: Utility functions for data processing and content type generation.

## 2. Key Components and Functionality

### 2.1 attack.py

This is the main script that orchestrates the jailbreak function attack. Key functions include:

#### jailbreak_function_consturct()
- Constructs the jailbreak function payload for different LLM APIs (GPT, Claude, Gemini, Mixtral).
- Creates a function description that tricks the LLM into generating harmful content under the guise of writing a novel.

#### jailbreak_function_call()
- Sends the jailbreak function payload to the target LLM using the appropriate API.
- Handles different API formats and extracts the generated content from the response.

#### main()
- Loads harmful behaviors from a dataset.
- Iterates through behaviors, attempting the jailbreak attack with different content types.
- Calculates and reports the attack success rate (ASR) for 1-shot and 5-shot attempts.

### 2.2 evaluate.py

This file contains functions for evaluating the success of the jailbreak attack:

#### harmful_classification()
- Uses a judge model (e.g., GPT-4) to determine if the generated content is harmful or not.
- Implements a binary classification: 'jailbreak' (1) or 'reject' (0).

### 2.3 utils.py

Utility functions for data processing:

#### get_content_type()
- Generates appropriate content types for a given harmful behavior using an LLM.
- Ensures the content types don't include sensitive words.

#### get_dataset()
- Loads the dataset of harmful behaviors from a CSV file.

## 3. Relation to Research Paper Concepts

The code directly implements the "jailbreak function" attack method described in the paper:

1. It uses the `WriteNovel` function to trick LLMs into generating harmful content.
2. The attack is tested against multiple state-of-the-art LLMs (GPT-4, Claude-3, Gemini, etc.).
3. It calculates the Attack Success Rate (ASR) for 1-shot and 5-shot attempts, as discussed in the paper.
4. The `harmful_classification()` function mimics the human evaluation process described in the study.

## 4. Notable Techniques

1. **API Abstraction**: The code abstracts away differences between various LLM APIs, allowing for easy testing across multiple models.

2. **Content Type Generation**: Uses an LLM to dynamically generate appropriate content types for each harmful behavior, improving the attack's adaptability.

3. **Staged Attack**: Implements both 1-shot and 5-shot attack strategies, attempting different content types until success or exhaustion.

4. **Automated Evaluation**: Uses a judge model (GPT-4) to automatically classify generated content as harmful or not, enabling large-scale testing.

# Potential Enhancements

1. **Defensive Prompt Integration**
   - Implement and test the defensive prompts proposed in the paper.
   - Create a framework for easily switching between attack and defense modes.

2. **Expanded Model Support**
   - Add support for more LLMs, especially open-source models that can be run locally.
   - Implement a plugin system for easily adding new model APIs.

3. **Advanced Attack Strategies**
   - Develop more sophisticated jailbreak functions that adapt based on initial responses.
   - Implement chain-of-thought or multi-step attacks to increase success rates.

4. **Comprehensive Evaluation Framework**
   - Create a more nuanced evaluation system beyond binary classification.
   - Implement multiple judge models and combine their outputs for more reliable results.

5. **Ethical Considerations and Safeguards**
   - Add strong safeguards to prevent misuse of the code for actual harmful purposes.
   - Implement a system for responsible disclosure of vulnerabilities to LLM providers.