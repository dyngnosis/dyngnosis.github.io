#Tags
[[Research/Research Papers/2410.13691v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** Jailbreaking LLM-Controlled Robots
**Authors:** Alexander Robey, Zachary Ravichandran, Vijay Kumar, Hamed Hassani, George J. Pappas
**Affiliation:** School of Engineering and Applied Science, University of Pennsylvania
**Publication Date:** October 17, 2024

Summary:
This paper introduces RoboPAIR, the first algorithm designed to jailbreak LLM-controlled robots. The authors demonstrate that RoboPAIR can elicit harmful physical actions from LLM-controlled robots in various scenarios, highlighting the risks of deploying LLMs in robotics.

Key Contributions:
- Introduction of RoboPAIR, an algorithm for jailbreaking LLM-controlled robots
- Demonstration of successful jailbreaks in white-box, gray-box, and black-box settings
- Creation of three new datasets of harmful robotic actions
- First successful jailbreak of a deployed commercial robotic system (Unitree Go2)

Problem Statement:
The paper addresses the vulnerability of LLM-controlled robots to jailbreaking attacks, which could potentially cause physical harm in the real world.

Methodology:
1. Development of RoboPAIR algorithm
2. Testing in three scenarios:
   a. White-box: NVIDIA Dolphins self-driving LLM
   b. Gray-box: Clearpath Robotics Jackal UGV with GPT-4o planner
   c. Black-box: Unitree Robotics Go2 robot dog with GPT-3.5
3. Creation of datasets for harmful robotic actions
4. Evaluation of attack success rates across different jailbreaking methods

Main Results:
- RoboPAIR achieved 100% attack success rates in many cases
- Successful jailbreaks were demonstrated across all three scenarios
- Static baselines (in-context learning, template-based, and code injection attacks) also showed high success rates

Qualitative Analysis:
- The paper reveals that the risks of jailbroken LLMs extend beyond text generation to physical actions
- The success of jailbreaks on commercial systems like the Unitree Go2 highlights the urgent need for improved safety measures in deployed robotic systems
- The authors emphasize the importance of developing robot-specific filters and defense mechanisms

Limitations:
- The study focuses on a limited number of robotic systems and may not generalize to all LLM-controlled robots
- The ethical implications of conducting such research are discussed, but long-term consequences remain uncertain

Conclusion and Future Work:
- The paper concludes that addressing the vulnerability of LLM-controlled robots to jailbreaking attacks is critical for safe deployment in real-world applications
- Future work should focus on developing robust defenses against these attacks, including physical safety filters and context-aware alignment techniques

New Tools:
RoboPAIR: An algorithm designed to jailbreak LLM-controlled robots (GitHub repository not provided in the paper)

Relevant Figures:
Figure 1: Illustration of jailbreaking LLM-controlled robots
Figure 2: Example of a robotic jailbreak
Figure 3: Comparison of jailbreaking success rates across different tasks and methods
Figure 4: Threat models for robotic jailbreaking