#Tags
[[Research/Research Papers/2407.08441v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0054/LLMJailbreak
#AMLT0043/CraftAdversarialData
#AMLT0031/ErodeMLModelIntegrity
#AMLT0042/VerifyAttack

**Title:** Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation
**Authors:** Riccardo Cantini, Giada Cosenza, Alessio Orsino, Domenico Talia
**Affiliation:** University of Calabria
**Publication Date:** July 11, 2024

Summary:
This study investigates the presence of biases in Large Language Models (LLMs) and evaluates their adversarial robustness against jailbreak prompts designed to elicit biased responses. The research explores various biases, including gender, ethnicity, sexual orientation, religion, socioeconomic status, disability, and age.

Key Contributions:
- Proposes a two-step methodology for assessing LLM safety and robustness against bias elicitation
- Introduces a safety score metric that jointly evaluates model fairness and robustness
- Evaluates the effectiveness of various jailbreak techniques across different model scales
- Provides insights into LLM behavior under bias elicitation attempts

Problem Statement:
The study addresses the challenge of assessing the true resilience of LLMs against biases and stereotypes, questioning whether these models are genuinely bias-free despite advanced capabilities and alignment processes.

Methodology:
1. Initial safety evaluation using standard prompts:
   - Create sentence completion tasks for various bias categories
   - Compute bias-specific safety scores for each category
   - Analyze model behavior in terms of declining responses, debiasing content, and favoring stereotypes or counterstereotypes

2. Adversarial analysis using jailbreak prompts:
   - Apply jailbreak techniques (role-playing, machine translation, obfuscation, prompt injection, reward incentive) to bias categories deemed safe in the initial assessment
   - Evaluate the effectiveness of jailbreak attacks across different model scales

Models evaluated:
- Small: Gemma2B, Phi-3 mini, StableLM21.6B
- Medium: Gemma7B, Llama38B, Mistral7B
- Large: Llama370B, GPT-3.5 Turbo, Gemini Pro

Main Results:
1. No model was entirely safe against all jailbreak attacks, with safety scores falling below the critical threshold after adversarial analysis.
2. Large models generally exhibited greater robustness, fairness, and safety compared to smaller models.
3. GPT-3.5 Turbo, despite its size, showed the lowest safety scores and high vulnerability to various attacks.
4. Gemini Pro demonstrated the highest overall safety but was still susceptible to certain attacks.
5. Role-playing and obfuscation attacks were particularly effective across multiple models.
6. Machine translation attacks were less effective against models with superior reasoning capabilities in low-resource languages.

Qualitative Analysis:
- The study reveals that even models with sophisticated alignment processes can be manipulated to produce biased or inappropriate responses.
- The effectiveness of jailbreak attacks varies across model scales, suggesting that larger models may have more robust safety measures but are not immune to all attacks.
- The research highlights the need for a layered defense approach in LLM development to counteract diverse and evolving threats.

Limitations:
- The study focuses on a specific set of bias categories and may not cover all possible biases in LLMs.
- The effectiveness of jailbreak techniques may vary with different prompt engineering strategies.

Conclusion and Future Work:
The paper concludes that current LLMs, despite advanced capabilities, can still be manipulated to produce biased responses. The authors emphasize the importance of enhancing mitigation techniques to address safety issues and promote more sustainable and inclusive AI development. Future work may involve developing more sophisticated defense mechanisms and exploring additional bias categories and jailbreak techniques.

Figures and Tables:
- Figure 2: Heatmaps depicting robustness, fairness, and safety scores across bias categories for each model
- Figure 3: Comparison of overall robustness, fairness, and safety across different model scales
- Figure 5: Effectiveness of jailbreak attacks across various models
- Table 3: Minimum safety scores obtained using jailbreak attacks for each bias category

New Tools:
The paper does not introduce specific new tools or GitHub repositories.