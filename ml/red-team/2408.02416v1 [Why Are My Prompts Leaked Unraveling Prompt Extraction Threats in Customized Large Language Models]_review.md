#Tags
[[Research/Research Papers/2408.02416v1.pdf]]

#AMLT0056/LLMMetaPromptExtraction
#AMLT0057/LLMDataLeakage
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData

**Title:** Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models
**Authors:** Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, Haoyang Li
**Affiliation:** The Hong Kong Polytechnic University, Hong Kong, China
**Publication Date:** August 5, 2024

Key Contributions:
- Analysis of prompt leakage mechanisms in LLMs
- Exploration of scaling laws in prompt extraction
- Proposal of two hypotheses explaining how LLMs expose prompts
- Investigation of alignment effectiveness against prompt extraction attacks
- Development of defense strategies against prompt extraction

Problem Statement:
The paper addresses the growing concern of prompt leakage in customized large language models, which undermines the intellectual property of prompt-based services and can lead to downstream attacks.

Methodology:
1. Analyzed factors influencing prompt extraction: model sizes, prompt lengths, and prompt types
2. Proposed two hypotheses: convincing premise and parallel translation
3. Evaluated LLM alignments' effectiveness against prompt extraction attacks
4. Developed and tested defense strategies based on prompt engineering

Main Results:
1. Larger LLMs are more vulnerable to implicit intent attacks
2. Prompt extraction rate negatively correlates with prompt length
3. Natural language prompts are more easily memorized and translated
4. LLMs construct straightforward linking paths from prompts to generated contexts in attention mechanisms
5. Current LLMs, including those with safety alignments, are highly vulnerable to prompt extraction attacks
6. Proposed defense strategies achieved 83.8% and 71.0% drop in prompt extraction rate for Llama2-7B and GPT-3.5, respectively

Qualitative Analysis:
- The study reveals a significant vulnerability in customized LLMs, highlighting the need for improved security measures in prompt-based services
- The findings suggest that current alignment techniques are insufficient to protect against prompt extraction attacks
- The proposed defense strategies show promise in mitigating prompt leakage without significantly impacting model performance

Limitations:
- The study primarily focuses on existing adversarial prompts, which may limit the generalizability of the findings
- The effectiveness of the proposed defense strategies may vary across different LLM architectures and sizes

Conclusion and Future Work:
- The paper demonstrates the vulnerability of LLMs to prompt extraction attacks and proposes effective defense strategies
- Future work should focus on developing more robust alignment methods and exploring additional defense techniques against prompt extraction

Tools Introduced:
- PEAD: A benchmark dataset for evaluating prompt extraction attacks
- SPLIt (Single Prompt Linking Indicator): A set of indicators designed to trace attention connections between prompts and generated texts

GitHub Repository: https://github.com/liangzid/PromptExtractionEval