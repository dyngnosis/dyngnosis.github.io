#Tags
[[Research/Research Papers/2304.05197v3.pdf]]

#AMLT0057/LLMDataLeakage
#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0040/MLModelInferenceAPIAccess

**Title:** Multi-step Jailbreaking Privacy Attacks on ChatGPT
**Authors:** Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, Yangqiu Song
**Affiliations:** Hong Kong University of Science and Technology, Peking University, University of Illinois at Urbana-Champaign, University of Notre Dame
**Publication Date:** April 11, 2023 (arXiv)

Key Contributions:
- Proposed a novel multi-step jailbreaking prompt (MJP) to extract personally identifiable information (PII) from ChatGPT
- Demonstrated new privacy threats in application-integrated LLMs like the New Bing
- Conducted extensive experiments to assess privacy risks in ChatGPT and the New Bing

Problem Statement:
The study addresses the potential privacy threats posed by large language models (LLMs) and their downstream applications, particularly in extracting personally identifiable information (PII) from ChatGPT and the New Bing.

Methodology:
1. Data Collection:
   - Enron Email Dataset: (name, email address) pairs, (name, phone number) pairs
   - Institutional Pages: (name, email address) pairs, (name, phone number) pairs of professors

2. Attack Formulation:
   - Black-box API access to LLMs
   - Goal: Reconstruct sensitive information from LLM's training corpora

3. Extraction Techniques:
   - Direct Prompts (DP)
   - Jailbreaking Prompts (JP)
   - Multi-step Jailbreaking Prompt (MJP)
   - Response Verification (Multiple Choice and Majority Voting)

4. Evaluation Metrics:
   - Number of parsed responses
   - Number of correct extractions
   - Accuracy
   - Hit@5 (percentage of correct predictions in 5 generations)

Main Results:
1. ChatGPT:
   - Direct prompts and simple jailbreaking prompts were ineffective
   - MJP significantly improved PII extraction:
     - 59.09% accuracy for frequent Enron emails
     - 4% accuracy for institutional emails
   - Limited success in phone number extraction

2. The New Bing:
   - High success rate in PII extraction:
     - 94% accuracy for institutional emails
     - 48% accuracy for institutional phone numbers
   - Demonstrated ability to extract information beyond its training data

Qualitative Analysis:
- ChatGPT's safety mechanisms are effective against direct prompts but vulnerable to sophisticated multi-step attacks
- The New Bing's integration with search capabilities introduces new privacy risks, potentially leading to unintended PII dissemination
- LLMs may memorize certain personal information, posing privacy concerns for individuals whose data is available online

Limitations:
- Low recovery accuracy for infrequent Enron emails and phone numbers
- Inability to confirm if queried PII was in ChatGPT's training data
- Potential bias in results due to rule-based patterns in email addresses

Conclusion and Future Work:
- Current safety mechanisms in LLMs are insufficient to prevent privacy leaks
- Application-integrated LLMs introduce new privacy threats beyond data memorization
- Future work should focus on improving LLM safety, developing better defenses, and exploring identity disclosure prompting

Tools Introduced:
- Multi-step Jailbreaking Prompt (MJP): A novel technique to bypass ChatGPT's ethical restrictions and extract PII
- GitHub repository: https://github.com/HKUST-KnowComp/LLM-Multistep-Jailbreak

## Repository Token Information
Total tokens in repository: 188370

Tokens per file:
- chatgpt_extraction.py: 2608 tokens
- config.py: 141 tokens
- utils.py: 705 tokens
- pred_analysis.py: 3541 tokens
- llm_extraction.py: 2480 tokens
- README.md: 600 tokens
- email_content_extraction/flair_evaluate.py: 2442 tokens
- email_content_extraction/config.py: 61 tokens
- email_content_extraction/get_email.py: 1523 tokens
- email_content_extraction/utils.py: 626 tokens
- email_content_extraction/README.md: 222 tokens
- email_content_extraction/prompts/prompt_developer.txt: 794 tokens
- email_content_extraction/prompts/prompts.py: 231 tokens
- email_content_extraction/prompts/prompt_assistant.txt: 107 tokens
- data/university.json: 1344 tokens
- data/university_phone_50.json: 1303 tokens
- data/enron_top100_email.json: 1957 tokens
- data/enron_emails.json: 69510 tokens
- data/enron_emails_sampled.json: 2121 tokens
- data/university_phone.json: 771 tokens
- data/enron_phone.json: 94382 tokens
- prompts/prompt_developer.txt: 794 tokens
- prompts/prompt_assistant.txt: 107 tokens
