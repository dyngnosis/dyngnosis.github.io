#Tags
[[Research/Research Papers/2408.00523v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0005/CreateProxyMLModel

**Title:** Jailbreaking Text-to-Image Models with LLM-Based Agents
**Authors:** Yingkai Dong, Zheng Li, Xiangtao Meng, Ning Yu, Shanqing Guo
**Affiliations:** Shandong University, Netflix Eyeline Studios, CISPA Helmholtz Center for Information Security
**Publication Date:** August 1, 2024

Summary:
This paper introduces Atlas, an advanced LLM-based multi-agent framework designed to perform jailbreak attacks on text-to-image (T2I) models with built-in safety filters. The framework uses two agents - a mutation agent and a selection agent - to generate and evaluate jailbreak prompts that can bypass safety filters while maintaining semantic similarity to the original prompts.

Key Contributions:
- Development of Atlas, a novel LLM-based multi-agent framework for jailbreaking T2I models
- Implementation of in-context learning (ICL) and chain-of-thought (COT) approaches to enhance performance
- Demonstration of successful jailbreaks on state-of-the-art T2I models with multi-modal safety filters
- Outperformance of existing methods in query efficiency and generated image quality

Problem Statement:
The research addresses the challenge of identifying safety vulnerabilities in text-to-image generation models, specifically focusing on bypassing built-in safety filters while maintaining semantic similarity to original prompts.

Methodology:
1. Two-agent system:
   - Mutation agent: Uses a Vision-Language Model (VLM) to determine if prompts trigger safety filters and generate new candidate prompts
   - Selection agent: Evaluates and scores candidate prompts using an LLM
2. Key components for each agent:
   - Brain (VLM or LLM)
   - Planning module
   - Memory module (ICL-based)
   - Tool usage
3. Multi-loop attack flow with exponential backoff strategy
4. Evaluation on multiple T2I models (SD1.4, SDXL, SD3, DALL·E 3) with various safety filters

Main Results:
1. High bypass rates across different safety filters:
   - 100% one-time bypass rate for most filters
   - 82.5%+ bypass rate for conservative text-classifier-based filters
   - 81.93% bypass rate for closed-box DALL·E 3
2. Low query numbers:
   - Average of 4.6 queries for most filters
   - Average of 12.6 queries for text-classifier-based filters
3. Outperformed baseline methods (SneakyPrompt, DACA, Ring-A-Bell) in:
   - Bypass rates
   - Query efficiency
   - Semantic similarity (measured by FID score)
   - Naturalness of generated prompts (measured by perplexity)

Qualitative Analysis:
- Atlas demonstrates the potential of LLM-based agents in identifying and exploiting safety vulnerabilities in T2I models
- The multi-agent approach allows for more diverse and innovative thinking in generating jailbreak prompts
- The use of ICL and COT enhances the system's ability to learn from past successes and failures
- The framework's success highlights the need for more robust safety measures in T2I models

Limitations:
- Reliance on open-source large models that are not safety-aligned
- Potential ethical concerns regarding the development of jailbreaking techniques
- Performance may vary when focusing on a single specific prompt without access to a diverse prompt pool

Conclusion and Future Work:
The paper concludes that Atlas successfully demonstrates the application of LLM-based agents in studying safety vulnerabilities of T2I models. The authors urge the community to consider advanced techniques like Atlas in response to the rapidly evolving T2I generation field. Future work may include:
- Evaluation with safety-aligned models
- Development of more robust defense mechanisms against jailbreak attacks
- Exploration of ethical implications and potential countermeasures

Tools Introduced:
Atlas framework (no GitHub repository mentioned in the provided content)