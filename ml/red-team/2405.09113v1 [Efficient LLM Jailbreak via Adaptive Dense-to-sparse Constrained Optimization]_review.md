#Tags
[[Research/Research Papers/2405.09113v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization
**Authors:** Kai Hu, Weichen Yu, Tianjun Yao, Xiang Li, Wenhe Liu, Lijun Yu, Yining Li, Kai Chen, Zhiqiang Shen, Matt Fredrikson
**Affiliations:** Carnegie Mellon University, Shanghai AI Laboratory, Mohamed bin Zayed University of AI
**Publication date:** 15 May 2024 (arXiv preprint)

Key Contributions:
- Introduction of a novel token-level attack method called Adaptive Dense-to-Sparse Constrained Optimization (ADC)
- Relaxation of discrete jailbreak optimization into continuous optimization
- Progressive increase in sparsity of optimizing vectors
- Bridging the gap between discrete and continuous space optimization
- Achieving state-of-the-art attack success rates on multiple LLMs

Problem Statement:
The paper addresses the vulnerability of large language models (LLMs) to jailbreaking attacks that can generate harmful content. It aims to develop a more effective and efficient token-level attack method compared to existing approaches.

Methodology:
1. Relaxation of discrete optimization into continuous optimization
2. Adaptive dense-to-sparse constraint on dense token optimization
3. Optimizer design to escape local minima
4. Multiple initialization starts for optimization
5. Evaluation on AdvBench and HarmBench datasets
6. Comparison with existing methods like GCG, AutoPrompt, PAIR, TAP, and AutoDan

Main Results:
1. ADC outperforms GCG on Llama2-chat-7B with 77.7% vs 53.8% attack success rate (ASR)
2. ADC+ achieves higher Exact Match (EM) scores on AdvBench Strings dataset
3. On HarmBench, ADC+ achieves state-of-the-art results on 7 out of 8 LLMs
4. Significant improvements in ASR on LLama2-7B-chat (89.5% vs 34.5%) and Qwen-7B-chat (99.0% vs 79.5%)
5. 35% ASR on adversarially trained Zephyr R2D2, where other token-level methods fail

Qualitative Analysis:
- The adaptive dense-to-sparse approach effectively bridges the gap between discrete and continuous optimization
- The method shows superior performance across various LLM architectures and sizes
- ADC demonstrates effectiveness even against adversarially trained models like Zephyr R2D2

Limitations:
- Performance on closed-source LLMs (e.g., GPT-3.5 and GPT-4) not yet reported
- Potential ethical concerns regarding the development of more effective jailbreaking techniques

Conclusion and Future Work:
The paper presents ADC as a highly effective token-level jailbreak method for LLMs, outperforming existing techniques in both effectiveness and efficiency. Future work may include:
1. Evaluation on closed-source LLMs
2. Development of defensive measures against ADC-like attacks
3. Exploration of the method's applicability to other NLP tasks

Tools Introduced:
- Adaptive Dense-to-Sparse Constrained Optimization (ADC) method
- ADC+ variant integrating GCG for improved efficiency

Note: The authors mention that code will be made available, but no specific GitHub repository is provided in the paper.