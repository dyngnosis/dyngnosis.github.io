#Tags
[[Research/Research Papers/2402.08567v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0018/BackdoorMLModel
#AMLT0020/PoisonTrainingData
#AMLT0031/ErodeMLModelIntegrity
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast

**Authors:** Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, Min Lin

**Publication Date:** February 13, 2024 (v1), June 3, 2024 (v2)

Summary:
This paper introduces the concept of "infectious jailbreak" in multi-agent environments using multimodal large language models (MLLMs). The authors demonstrate that by jailbreaking a single agent with an adversarial image, the jailbreak can spread exponentially fast to infect nearly all agents in the system without further intervention from the adversary.

Key Contributions:
- Introduction of the "infectious jailbreak" concept for MLLMs in multi-agent systems
- Theoretical analysis of infectious dynamics in randomized pairwise chat environments
- Empirical demonstration of infectious jailbreak in simulations with up to one million LLaVA-1.5 agents
- Derivation of a principle for determining if a defense mechanism can restrain infectious jailbreak spread

Problem Statement:
The paper addresses the safety concerns of deploying MLLM agents at scale, specifically the vulnerability to adversarial attacks that can spread rapidly through a multi-agent system.

Methodology:
1. Formalization of infectious dynamics in randomized pairwise chat environments
2. Implementation of MLLM agents using LLaVA-1.5 and CLIP for retrieval-augmented generation
3. Crafting of adversarial images using momentum iterative fast gradient sign method (MI-FGSM)
4. Simulation of multi-agent environments with up to one million agents
5. Evaluation of infection rates and spread under various conditions

Main Results:
1. Infectious jailbreak can spread to almost all agents in O(log N) chat rounds, where N is the number of agents
2. The method is effective across different MLLM architectures (LLaVA-1.5 7B/13B, InstructBLIP 7B)
3. Infection rates remain high even with increased chat diversity and larger perturbation budgets
4. The attack is robust to common image corruptions and various hyperparameter settings

Qualitative Analysis:
- The infectious jailbreak exploits the interaction between agents and their memory storage capabilities
- The rapid spread of the jailbreak poses significant safety concerns for large-scale deployment of MLLM agents
- The attack's effectiveness across different model architectures suggests a fundamental vulnerability in current MLLM designs

Limitations:
- The study uses a simplified multi-agent interaction model (randomized pairwise chat)
- The exact match criteria for determining harmful outputs may underestimate the actual effectiveness of the attack
- The paper does not provide a practical defense mechanism against infectious jailbreak

Conclusion and Future Work:
The authors conclude that the infectious jailbreak presents a severe safety issue for multi-agent MLLM systems. They derive a principle for determining if a defense can provably restrain the spread but note that designing a practical defense remains an open question for future research.

Relevant Figures:
Figure 1: Visualization of infectious jailbreak spreading through one million agents
Figure 2: Pipeline of randomized pairwise chat and infectious jailbreak
Figure 4: Analysis of infection dynamics in successful and failure cases

New Tools:
The authors have made their code available at https://github.com/sail-sg/Agent-Smith, which likely includes implementations of their infectious jailbreak methods and multi-agent simulation environment.

## Repository Token Information
Total tokens in repository: 800802

Tokens per file:
- README.md: 1402 tokens
- simulation/simulation_batch.py: 2781 tokens
- simulation/agent_high_batch.py: 885 tokens
- simulation/simulation_test_batch_aug.py: 3589 tokens
- simulation/simulation_test_batch.py: 3008 tokens
- simulation/agent_batch.py: 884 tokens
- simulation/conversion.py: 3357 tokens
- attack/validate.py: 3089 tokens
- attack/optimize.py: 5641 tokens
- attack/demo.txt: 14 tokens
- attack/accelerate_config.yaml: 193 tokens
- data/million_villagers_1024.json: 388711 tokens
- data/million_villagers_1024_test.json: 387121 tokens
- data/attack_image/targets/sampling_targets1.txt: 127 tokens
