#Tags
[[Research/Research Papers/2408.15207v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection
#AMLT0054/LLMJailbreak

**Title:** Investigating Coverage Criteria in Large Language Models: An In-Depth Study Through Jailbreak Attacks
**Authors:** Shide Zhou, Tianlin Li, Kailong Wang, Yihao Huang, Ling Shi, Yang Liu, Haoyu Wang
**Publication Date:** August 27, 2024

Key Contributions:
- Comprehensive empirical analysis of conventional coverage criteria effectiveness in identifying LLM vulnerabilities
- Clustering analysis of LLM hidden states to differentiate query types
- Assessment of coverage criteria across criterion level, layer level, and token level
- Development of a real-time jailbreak attack detection method using neural activation features

Problem Statement:
The study addresses the insufficient pre-deployment testing of Large Language Models (LLMs) and the need for more rigorous evaluation methods to identify vulnerabilities, particularly jailbreak attacks.

Methodology:
1. Clustering analysis of LLM hidden states
2. Evaluation of coverage criteria across three dimensions:
   - Criterion level
   - Layer level
   - Token level
3. Development and testing of a jailbreak attack detection classifier
4. Experiments conducted on multiple LLM architectures: OPT-125M, Llama-2-7B-Chat, Pythia-12B, and Gemma-2-27B-it
5. Datasets used: Alpaca-gpt4, JailBreakV-28k, TruthfulQA, and AdvBench

Main Results and Findings:
1. Significant disparities in neuron activation patterns between normal and jailbreak queries
2. Neuron Coverage (NC) and Top-K Neuron Coverage (TKNC) identified as most effective coverage criteria for LLMs
3. Attention layers more effective than MLP layers for coverage analysis
4. Testing at the last token of the original query proves most effective
5. Real-time jailbreak attack detection classifier achieves 96.33% average accuracy

Qualitative Analysis:
- The study demonstrates the potential of using internal states of LLMs to design effective coverage criteria
- Attention layers' superior performance in capturing input features suggests their importance in understanding LLM behavior
- The effectiveness of testing at the last token of the original query indicates the importance of considering the full context in LLM evaluation

Limitations:
- Study focused primarily on jailbreak attacks, may not generalize to all types of LLM vulnerabilities
- Limited to four LLM architectures, may need further validation on other models
- Real-world applicability of the proposed detection method needs further investigation

Conclusion and Future Work:
- The study advances understanding of LLM security testing and lays a foundation for developing more resilient AI systems
- Proposed real-time detection method shows promise for future LLM-integrated systems
- Future work may include expanding the study to other types of LLM vulnerabilities and testing on a broader range of LLM architectures

Relevant Figures:
- Figure 1: Clustering experiment analysis results
- Figure 3: Probability density plot of maximum neuron activation values across model blocks
- Figure 4: RCG results based on NC and TKNC for different blocks of the target LLMs
- Figure 5: RCG results calculated based on NC and TKNC for different tokens in the target LLMs

New Tools:
The paper introduces a novel jailbreak attack detection classifier based on neural activation features. However, no specific tool name or GitHub repository is mentioned.