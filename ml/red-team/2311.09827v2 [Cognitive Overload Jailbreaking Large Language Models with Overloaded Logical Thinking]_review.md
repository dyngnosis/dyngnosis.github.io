#Tags
[[Research/Research Papers/2311.09827v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData

**Title:** Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking
**Authors:** Nan Xu, Fei Wang, Ben Zhou, Bangzheng Li, Chaowei Xiao, Muhao Chen
**Affiliations:** University of Southern California, University of Pennsylvania, University of Wisconsin Madison, University of California, Davis
**Publication Date:** November 16, 2023 (updated February 29, 2024)

Abstract Summary:
This paper investigates a novel category of jailbreak attacks targeting the cognitive structure and processes of Large Language Models (LLMs). The authors analyze LLM vulnerabilities to multilingual cognitive overload, veiled expression, and effect-to-cause reasoning. The proposed cognitive overload is a black-box attack that doesn't require knowledge of model architecture or access to model weights.

Key Contributions:
- Introduction of cognitive overload as a new jailbreaking technique for LLMs
- Analysis of LLM vulnerabilities to multilingual prompts, veiled expressions, and effect-to-cause reasoning
- Evaluation of the proposed attacks on various open-source and proprietary LLMs
- Investigation of defense strategies against cognitive overload attacks

Problem Statement:
The research addresses the vulnerability of LLMs to jailbreak attacks that exploit their cognitive structure and processes, even after safety alignment measures have been implemented.

Methodology:
1. Multilingual Cognitive Overload:
   - Evaluated LLMs' responses to harmful prompts in various languages
   - Tested language switching scenarios in two-turn conversations
2. Veiled Expression:
   - Used Mistral-7B-Instruct to generate paraphrases of malicious prompts
   - Tested three types of paraphrasing: plain, without sensitive words, and with sensitive words replaced
3. Effect-to-Cause Reasoning:
   - Prompted LLMs to infer scenarios where illegal actions could be performed without punishment
4. Evaluation:
   - Used AdvBench and MasterKey datasets for jailbreaking evaluation
   - Tested on open-source LLMs (Llama 2, Vicuna, WizardLM, Guanaco, MPT) and ChatGPT
5. Defense Strategies:
   - Evaluated in-context defense and defensive instructions

Main Results and Findings:
1. Multilingual Cognitive Overload:
   - LLMs are more vulnerable to non-English adversarial prompts
   - Language switching increases attack effectiveness
2. Veiled Expression:
   - Replacing sensitive words with neutral alternatives effectively bypasses safety mechanisms
3. Effect-to-Cause Reasoning:
   - LLMs tend to follow malicious reasoning instructions, compromising safety
4. Defense Strategies:
   - In-context defense and defensive instructions show limited effectiveness against cognitive overload attacks

Qualitative Analysis:
- The study reveals that current LLM safety mechanisms are insufficient against sophisticated jailbreak attacks
- Cognitive overload exploits the complex reasoning capabilities of LLMs, making it challenging to implement effective defenses
- The vulnerability to non-English prompts highlights the need for multilingual safety considerations in LLM development

Limitations:
- Limited evaluation on proprietary LLMs due to API access constraints
- Focus on measuring harmful content generation without considering response quality

Conclusion and Future Work:
- Cognitive overload jailbreaks successfully elicit unsafe behaviors from aligned LLMs
- Existing defense strategies are inadequate against these attacks
- Future work should focus on developing more robust safety mechanisms that consider cognitive load and multilingual contexts

Relevant Figures:
- Figure 1: Illustration of harmful responses to malicious instructions using cognitive overload
- Figure 2: Effectiveness of monolingual cognitive overload attacks on various LLMs
- Figure 5: Effectiveness of veiled expression attacks on aligned LLMs

New Tools:
No specific new tools were introduced in this paper. The research primarily used existing LLMs and datasets for evaluation.