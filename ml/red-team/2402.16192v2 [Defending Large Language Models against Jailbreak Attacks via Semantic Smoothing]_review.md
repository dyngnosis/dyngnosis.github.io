#Tags
[[Research/Research Papers/2402.16192v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing
**Authors:** Jiabao Ji, Bairu Hou, Alexander Robey, George J. Pappas, Hamed Hassani, Yang Zhang, Eric Wong, Shiyu Chang
**Publication Date:** March 1, 2024

Summary:
This paper introduces SEMANTIC SMOOTH, a novel defense framework against jailbreak attacks on large language models (LLMs). The method uses semantics-preserving transformations and a learnable policy network to aggregate predictions from multiple transformed versions of input prompts, achieving state-of-the-art robustness while maintaining strong nominal performance.

Key Contributions:
- Introduction of SEMANTIC SMOOTH, a broadly-applicable jailbreaking defense framework
- Development of semantics-preserving transformations for input prompts
- Implementation of a learnable policy network for adaptive transformation selection
- Demonstration of favorable trade-off between robustness and nominal performance
- Quantitative analysis of the GCG attack's effectiveness using semantic transformations

Problem Statement:
Existing defenses against jailbreak attacks on LLMs often rely on uninterpretable heuristics, suffer from trade-offs between robustness and nominal performance, or are susceptible to adaptive attacks. The research aims to develop a defense that addresses these limitations while providing robustness against both token-level and prompt-level attacks.

Methodology:
1. Semantic Transformations: Seven types of transformations (e.g., paraphrasing, summarization, translation) are applied to input prompts.
2. Smoothing Framework: Multiple transformed copies of input prompts are generated and fed into the LLM.
3. Adaptive Policy: A learnable policy network selects transformations based on input characteristics.
4. Aggregation: Predictions from transformed inputs are aggregated to produce the final output.

Experimental Setup:
- Jailbreak Attacks: GCG, PAIR, and AutoDAN
- Baseline Defenses: LLMFILTER, ERASE AND CHECK, PARAPHRASE DEFENSE, INCONTEXT DEFENSE, SMOOTHLLM
- Datasets: InstructionFollow, AlpacaEval
- Models: LLaMA-2-7b, Vicuna-13b, GPT-3.5-turbo

Main Results:
1. SEMANTIC SMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks.
2. The method maintains strong nominal performance on instruction-following benchmarks.
3. SEMANTIC SMOOTH outperforms baselines in terms of trade-off between robustness and nominal performance.
4. The learned policy network demonstrates adaptive selection of transformations based on input type.

Qualitative Analysis:
- Semantic transformations provide insights into the working mechanism of nonsensical strings generated by GCG attacks.
- The learned policy tends to favor more substantial transformations (e.g., summarize, format) for adversarial inputs and minimal changes (e.g., spell check, verb tense) for benign inputs.

Limitations:
- Increased computational cost due to multiple transformations and aggregation of responses.
- Effectiveness relies on the targeted LLM's ability to perform semantic transformations accurately.

Conclusion and Future Work:
SEMANTIC SMOOTH demonstrates a promising approach to defending LLMs against jailbreak attacks while maintaining strong nominal performance. Future work may focus on expanding the set of semantic transformations and further optimizing the policy network for different types of attacks and LLMs.

Tools Introduced:
SEMANTIC SMOOTH - GitHub repository: https://github.com/UCSB-NLP-Chang/SemanticSmooth

## Repository Token Information
Total tokens in repository: 229695

Tokens per file:
- transfer_attack.py: 663 tokens
- adaptive_attack.py: 1175 tokens
- environment.yaml: 2381 tokens
- README.md: 629 tokens
- lib/tasks.py: 2203 tokens
- lib/judges.py: 2106 tokens
- lib/ensemble_policy.py: 362 tokens
- lib/log.py: 955 tokens
- lib/config_util.py: 552 tokens
- lib/model_configs.py: 543 tokens
- lib/perturbations.py: 3199 tokens
- lib/__init__.py: 0 tokens
- lib/utils.py: 3 tokens
- lib/prompts.py: 1514 tokens
- lib/defenses.py: 5484 tokens
- lib/language_models.py: 1728 tokens
- lib/hparams_registry.py: 1199 tokens
- lib/misc.py: 110 tokens
- lib/strongreject/strongreject_evaluator.py: 1083 tokens
- lib/strongreject/__init__.py: 19 tokens
- lib/strongreject/simple_jailbreak_runner.py: 413 tokens
- lib/attack_autogan/download_models.py: 160 tokens
- lib/attack_autogan/AutoDAN.py: 3279 tokens
- lib/attack_autogan/string_utils.py: 1321 tokens
- lib/attack_autogan/opt_utils.py: 3037 tokens
- lib/attack_autogan/__init__.py: 39 tokens
- lib/attack_pair/judges.py: 765 tokens
- lib/attack_pair/common.py: 498 tokens
- lib/attack_pair/__init__.py: 2756 tokens
- lib/attack_pair/system_prompts.py: 1747 tokens
- lib/attack_pair/language_models.py: 988 tokens
- lib/instructionfollowing/instructions.py: 11330 tokens
- lib/instructionfollowing/instructions_test.py: 11104 tokens
- lib/instructionfollowing/__init__.py: 16 tokens
- lib/instructionfollowing/evaluation_main.py: 1836 tokens
- lib/instructionfollowing/instructions_util_test.py: 975 tokens
- lib/instructionfollowing/instructions_util.py: 6326 tokens
- lib/instructionfollowing/instructions_registry.py: 1698 tokens
- data/prompt-template-llama-2/translate-french.txt: 89 tokens
- data/prompt-template-llama-2/verbtense.txt: 112 tokens
- data/prompt-template-llama-2/spellcheck.txt: 140 tokens
- data/prompt-template-llama-2/baseline-paraphrase.txt: 27 tokens
- data/prompt-template-llama-2/summarize.txt: 108 tokens
- data/prompt-template-llama-2/paraphrase.txt: 107 tokens
- data/prompt-template-llama-2/format-summarize.txt: 164 tokens
- data/prompt-template-llama-2/synonym.txt: 124 tokens
- data/baselinedefense/baseline-paraphrase.txt: 27 tokens
- data/prompt-template-vicuna/translate-french.txt: 80 tokens
- data/prompt-template-vicuna/verbtense.txt: 102 tokens
- data/prompt-template-vicuna/spellcheck.txt: 110 tokens
- data/prompt-template-vicuna/baseline-paraphrase.txt: 27 tokens
- data/prompt-template-vicuna/summarize.txt: 99 tokens
- data/prompt-template-vicuna/paraphrase.txt: 99 tokens
- data/prompt-template-vicuna/format-summarize.txt: 151 tokens
- data/prompt-template-vicuna/synonym.txt: 116 tokens
- data/GCG/vicuna.json: 2983 tokens
- data/AlpacaEval/text_davinci_003_outputs.json: 114115 tokens
- data/prompt-template-gpt-3.5-turbo-0613/translate-french.txt: 80 tokens
- data/prompt-template-gpt-3.5-turbo-0613/verbtense.txt: 102 tokens
- data/prompt-template-gpt-3.5-turbo-0613/spellcheck.txt: 110 tokens
- data/prompt-template-gpt-3.5-turbo-0613/baseline-paraphrase.txt: 27 tokens
- data/prompt-template-gpt-3.5-turbo-0613/summarize.txt: 99 tokens
- data/prompt-template-gpt-3.5-turbo-0613/paraphrase.txt: 99 tokens
- data/prompt-template-gpt-3.5-turbo-0613/format-summarize.txt: 151 tokens
- data/prompt-template-gpt-3.5-turbo-0613/synonym.txt: 116 tokens
- data/AutoDAN/vicuna.json: 33478 tokens
- data/AutoDAN/autodan_initial_prompt.txt: 757 tokens
- config/config.yaml: 197 tokens
- config/defense/eraseandcheck.yaml: 73 tokens
- config/defense/incontext.yaml: 22 tokens
- config/defense/smoothllm.yaml: 96 tokens
- config/defense/llmfilter.yaml: 29 tokens
- config/defense/semanticsmooth.yaml: 170 tokens
- config/defense/paraphrasedefense.yaml: 57 tokens
- config/llm/vicuna-vllm.yaml: 68 tokens
- config/llm/llama-2.yaml: 77 tokens
- config/llm/gpt-3.5-turbo-1106.yaml: 77 tokens
- config/llm/gpt-3.5-turbo.yaml: 66 tokens
- config/llm/gpt-3.5-turbo-0613.yaml: 77 tokens
- config/llm/vicuna.yaml: 74 tokens
- config/llm/llama-2-vllm.yaml: 80 tokens
- config/task/GCG.yaml: 53 tokens
- config/task/InstructionFollow.yaml: 58 tokens
- config/task/AlpacaEval.yaml: 64 tokens
- config/task/StrongReject.yaml: 57 tokens
- config/task/advbench.yaml: 62 tokens
- config/task/OpenBookQA.yaml: 48 tokens
- config/task/PiQA.yaml: 47 tokens
- config/task/PAIR.yaml: 54 tokens
- config/task/AutoDAN.yaml: 56 tokens
- config/attacker/autodan.yaml: 11 tokens
- config/attacker/pair.yaml: 27 tokens
