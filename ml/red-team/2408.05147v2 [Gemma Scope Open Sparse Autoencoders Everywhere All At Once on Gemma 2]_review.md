#Tags
[[Research/Research Papers/2408.05147v2.pdf]]

#AMLT0002/AcquirePublicMLArtifacts
#AMLT0005/CreateProxyMLModel
#AMLT0013/DiscoverMLModelOntology
#AMLT0037/DataFromLocalSystem
#AMLT0040/MLModelInferenceAPIAccess
#AMLT0044/FullMLModelAccess

**Title:** Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2
**Authors:** Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant Varma, János Kramár, Anca Dragan, Rohin Shah, Neel Nanda
**Affiliation:** Google DeepMind
**Publication Date:** August 9, 2024

Summary:
This paper introduces Gemma Scope, an open suite of JumpReLU sparse autoencoders (SAEs) trained on all layers and sub-layers of Gemma 2 language models. The authors release SAE weights for Gemma 2 2B, 9B, and select layers of 27B base models, as well as some SAEs trained on instruction-tuned Gemma 2 9B. The goal is to facilitate more ambitious safety and interpretability research by making comprehensive SAE training more accessible to the broader research community.

Key Contributions:
- Release of Gemma Scope, a comprehensive suite of JumpReLU SAEs for Gemma 2 models
- Evaluation of SAE quality using standard metrics
- Comparison of SAEs trained on base and instruction-tuned models
- Release of weights and tutorial for easy access and use by researchers

Problem Statement:
The high cost of training comprehensive suites of sparse autoencoders limits research applications outside of industry, hindering progress in safety and interpretability research for large language models.

Methodology:
1. Training SAEs:
   - Used JumpReLU activation function
   - Trained on all layers and sub-layers of Gemma 2 2B and 9B, select layers of 27B
   - Trained on 4-16B tokens of text
   - Used various widths (16.4K to 1M latents) and sparsity levels

2. Evaluation:
   - Sparsity-fidelity trade-off analysis
   - Impact of sequence position on SAE performance
   - Comparison of SAEs trained on base vs. instruction-tuned models
   - Evaluation on different subsets of The Pile dataset
   - Analysis of low precision inference impact

3. Infrastructure:
   - Used TPUv3 and TPUv5p for training
   - Implemented distributed data loading and shared server system for efficient training

Main Results:
1. SAEs show consistent performance across different layers and model sizes
2. Residual stream SAEs have higher delta loss compared to MLP and attention SAEs
3. SAEs trained on base models transfer well to instruction-tuned models
4. Performance varies across different subsets of The Pile dataset
5. Low precision (bfloat16) inference has negligible impact on SAE performance

Qualitative Analysis:
- The comprehensive nature of Gemma Scope enables more ambitious applications of interpretability, such as circuit analysis across multiple layers
- The transfer of SAEs from base to instruction-tuned models suggests that fine-tuning may primarily involve re-weighting existing features rather than learning entirely new ones
- The variation in performance across different data subsets highlights the importance of considering domain-specific factors when applying SAEs

Limitations:
- The study focuses on a single family of language models (Gemma 2)
- The evaluation metrics for SAE quality are still an active area of research and debate
- The computational resources required for training and using comprehensive SAE suites may still be a barrier for some researchers

Conclusion and Future Work:
The authors conclude that Gemma Scope provides a valuable resource for the research community to advance interpretability and safety research in large language models. They propose several open problems that could be tackled using Gemma Scope, including:
- Exploring the structure and relationships between SAE features
- Using SAEs to improve performance on real-world tasks
- Red-teaming SAEs to validate their interpretability claims
- Conducting scalable circuit analysis in larger models
- Investigating the effects of fine-tuning on model internals

Tools Introduced:
- Gemma Scope: A suite of JumpReLU SAEs for Gemma 2 models
  Repository: https://huggingface.co/google/gemma-scope
- Interactive demo: https://www.neuronpedia.org/gemma-scope

Relevant Figures:
- Figure 2: Sparsity-fidelity trade-off for Gemma 2 2B and 9B SAEs
- Figure 8: Comparison of SAEs trained on base and instruction-tuned models
- Figure 9: Delta loss per Pile subset for different SAE configurations