#Tags
[[Research/Research Papers/2308.03825v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0051/LLMPromptInjection
#AMLT0057/LLMDataLeakage
#AMLT0031/ErodeMLModelIntegrity

**Title:** "Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models
**Authors:** Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, Yang Zhang
**Affiliations:** CISPA Helmholtz Center for Information Security, NetApp
**Publication Date:** To appear in ACM Conference on Computer and Communications Security, October 14, 2024

Summary:
This paper presents a comprehensive analysis of jailbreak prompts for Large Language Models (LLMs), focusing on their characteristics, distribution, and effectiveness in bypassing safeguards. The study introduces JailbreakHub, a framework for collecting and evaluating jailbreak prompts, and analyzes 1,405 prompts from December 2022 to December 2023.

Key Contributions:
- Introduction of JailbreakHub framework for jailbreak prompt analysis
- Identification of 131 jailbreak communities and their attack strategies
- Creation of a question set with 107,250 samples across 13 forbidden scenarios
- Evaluation of jailbreak prompt effectiveness on six popular LLMs
- Analysis of the evolution and distribution of jailbreak prompts across platforms

Problem Statement:
The research addresses the growing concern of jailbreak prompts as a primary attack vector for bypassing LLM safeguards and eliciting harmful content, potentially leading to misuse of AI systems.

Methodology:
1. Data Collection: Gathered prompts from Reddit, Discord, websites, and open-source datasets
2. Prompt Analysis: Used graph-based community detection to identify jailbreak communities
3. Response Evaluation: Created a forbidden question set and tested it on six LLMs
4. Effectiveness Assessment: Evaluated jailbreak prompts' success rates and analyzed their characteristics

Main Results:
1. Identified 131 jailbreak communities with diverse attack strategies
2. Found that jailbreak prompts are shifting from web communities to prompt-aggregation websites
3. Discovered 28 user accounts consistently optimizing jailbreak prompts over 100 days
4. Identified five highly effective jailbreak prompts with 0.95 attack success rates on ChatGPT and GPT-4
5. Observed that Political Lobbying (0.855 ASR) is the most vulnerable scenario across six LLMs

Qualitative Analysis:
- Jailbreak prompts employ various techniques, including prompt injection, privilege escalation, and virtualization
- The effectiveness of jailbreak prompts varies across different forbidden scenarios and LLMs
- LLMs trained with Reinforcement Learning from Human Feedback (RLHF) show some resistance to forbidden questions but are still vulnerable to jailbreak prompts

Limitations:
- The study focuses on prompts collected from December 2022 to December 2023, which may not reflect the most recent developments
- The effectiveness of jailbreak prompts may change as LLM vendors update their models and safeguards

Conclusion and Future Work:
The paper highlights the ongoing challenge of jailbreak prompts in compromising LLM safety measures. It emphasizes the need for more robust defense mechanisms and continuous monitoring of emerging jailbreak techniques. Future work should focus on developing adaptive safeguards and improving the resilience of LLMs against evolving jailbreak strategies.

Tools Introduced:
JailbreakHub: A framework for collecting, characterizing, and evaluating in-the-wild jailbreak prompts
GitHub: https://github.com/verazuo/jailbreak_llms

## Repository Token Information
Total tokens in repository: 7898

Tokens per file:
- README.md: 1700 tokens
- data/README.md: 1699 tokens
- code/ChatGLMEval/ChatGLMEval.py: 2198 tokens
- code/ChatGLMEval/run_evaluator.py: 412 tokens
- code/ChatGLMEval/few_shot_examples.py: 1889 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: Jailbreak LLMs Repository

## Project Overview

This repository contains code and data for analyzing and evaluating jailbreak prompts on Large Language Models (LLMs). The project, called JailbreakHub, aims to characterize in-the-wild jailbreak prompts and assess their effectiveness in bypassing LLM safeguards.

## Project Structure

The repository is organized as follows:

```
jailbreak_llms/
├── README.md
├── data/
│   └── README.md
└── code/
    └── ChatGLMEval/
        ├── ChatGLMEval.py
        ├── run_evaluator.py
        └── few_shot_examples.py
```

- `README.md`: Contains project information, data statistics, and usage instructions.
- `data/`: Stores datasets of jailbreak prompts and forbidden questions.
- `code/ChatGLMEval/`: Contains the main evaluation code for jailbreak prompts.

## Key Components

### 1. ChatGLMEval Class (ChatGLMEval.py)

This is the core component for evaluating jailbreak prompts. It uses the ChatGLM-6B model to determine whether a given response provides an answer to a forbidden question.

Key functions:
- `__init__()`: Initializes the model and sets up evaluation parameters.
- `check()`: Evaluates a question-response pair and determines if it answers the question.
- `build_individual_prompt()`: Constructs prompts for evaluation, including few-shot examples.

### 2. Evaluator Runner (run_evaluator.py)

This script orchestrates the evaluation process:
- Loads the ChatGLMEval instance
- Reads input data containing questions and responses
- Applies the evaluator to each question-response pair
- Saves the results

### 3. Few-Shot Examples (few_shot_examples.py)

Contains a dictionary of example question-response pairs used for few-shot learning in the evaluation process.

## Relation to Research Concepts

1. **Jailbreak Prompt Evaluation**: The code implements the evaluation framework described in the paper, using ChatGLM to assess whether responses to forbidden questions are successful jailbreaks.

2. **Few-Shot Learning**: The evaluation uses few-shot examples to improve the accuracy of determining whether a response answers a forbidden question.

3. **Forbidden Question Categories**: The evaluation covers various scenarios mentioned in the paper, such as illegal activities, hate speech, and privacy violations.

4. **Large-Scale Analysis**: The code is designed to process large datasets of prompts and responses, enabling the comprehensive analysis described in the paper.

## Notable Techniques

1. **Temperature Control**: The ChatGLMEval class allows for temperature adjustment in the model, which can affect the randomness of generated responses.

2. **Response Length Heuristics**: The evaluation uses response length as a heuristic for quick classification of very short responses.

3. **Keyword-Based Refusal Detection**: For medium-length responses, a keyword-based approach is used to detect refusals quickly.

# Potential Enhancements

1. **Multi-Model Evaluation**
   - Extend the evaluator to support multiple LLMs beyond ChatGLM-6B.
   - Implement a comparison framework to analyze differences in jailbreak effectiveness across models.

2. **Dynamic Jailbreak Detection**
   - Develop a system to automatically identify new jailbreak patterns in real-time.
   - Implement machine learning techniques to classify and cluster emerging jailbreak strategies.

3. **Adaptive Defense Mechanisms**
   - Create a feedback loop where successful jailbreaks inform model fine-tuning.
   - Implement adversarial training techniques to improve model robustness against jailbreak attempts.

4. **Cross-Lingual Jailbreak Analysis**
   - Extend the framework to analyze jailbreak prompts in multiple languages.
   - Investigate how jailbreak effectiveness varies across different linguistic and cultural contexts.

5. **Ethical Implications Analyzer**
   - Develop a module to assess the potential harm and ethical implications of successful jailbreaks.
   - Create a scoring system for jailbreak severity based on content and potential consequences.