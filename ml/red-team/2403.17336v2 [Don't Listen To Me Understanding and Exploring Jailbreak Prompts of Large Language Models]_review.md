#Tags
[[Research/Research Papers/2403.17336v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData

**Title:** Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models

**Authors:** Zhiyuan Yu, Xiaogeng Liu, Shunning Liang, Zach Cameron, Chaowei Xiao, Ning Zhang

**Affiliations:** 
- Washington University in St. Louis
- University of Wisconsin - Madison
- John Burroughs School

**Publication Date:** March 26, 2024

Summary:
This paper investigates the phenomenon of jailbreak prompts used to bypass security restrictions in large language models (LLMs). The authors systematically analyze existing jailbreak techniques, conduct a user study on manual prompt creation, and explore the potential for automating jailbreak prompt generation.

Key Contributions:
1. Systemization of 448 in-the-wild jailbreak prompts into 5 categories and 10 unique patterns
2. Evaluation of jailbreak effectiveness on GPT-3.5, GPT-4, and PaLM-2 using proposed metrics
3. User study with 92 participants revealing the process of manually creating jailbreak prompts
4. Development of an automated framework for jailbreak prompt generation

Problem Statement:
The paper addresses the emerging threat of jailbreak attacks on LLMs, which aim to circumvent security measures and elicit harmful content. The research seeks to understand the effectiveness of existing jailbreak techniques, the process of creating new prompts, and the potential for automation.

Methodology:
1. Collection and analysis of 448 jailbreak prompts and 161 malicious queries
2. Development of metrics (Expected Maximum Harmfulness and Jailbreak Success Rate) to evaluate jailbreak effectiveness
3. User study with 92 participants to explore manual jailbreak prompt creation
4. Development of an automated framework for jailbreak prompt generation using AI assistance

Main Results:
1. "Virtual AI Simulation" and "Hybrid Strategies" categories were found to be the most effective jailbreak techniques
2. GPT-4 demonstrated higher robustness against jailbreak attempts compared to GPT-3.5 and PaLM-2
3. Participants in the user study, regardless of expertise, were often able to create successful jailbreak prompts
4. The automated framework successfully transformed 729 out of 766 previously failed prompts into effective jailbreaks

Qualitative Analysis:
- The study reveals that jailbreak prompts often exploit the conflict between user-centric design and developer-imposed restrictions in LLMs
- The success of non-expert participants in creating jailbreak prompts highlights the accessibility of this attack vector
- The identification of universal jailbreak prompts suggests shared vulnerabilities across different LLM implementations

Limitations:
1. Reliance on human annotation for evaluating jailbreak effectiveness, which may introduce subjectivity
2. Limited diversity in participant demographics, particularly in age distribution
3. Potential bias in self-reported expertise levels of participants

Conclusion and Future Work:
The paper demonstrates the widespread vulnerability of LLMs to jailbreak attacks and the ease with which such attacks can be crafted. The authors suggest further research into robust defense mechanisms and propose exploring more diverse transformation patterns for automated jailbreak generation.

Relevant Figures/Tables:
- Table 1: Systemization of Existing Jailbreak Prompts
- Table 2: Statistics of Collected Jailbreak Prompts and Malicious Queries
- Figure 2: Comparison of jailbreak effectiveness across GPT-3.5, GPT-4, and PaLM-2
- Figure 4: Quantitative results from different groups of users in the study

New Tools:
The authors developed an automated framework for jailbreak prompt generation, though no specific name or GitHub repository is mentioned in the provided content.