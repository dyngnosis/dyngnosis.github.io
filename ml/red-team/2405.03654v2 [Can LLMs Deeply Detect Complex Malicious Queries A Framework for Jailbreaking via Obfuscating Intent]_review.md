#Tags
[[Research/Research Papers/2405.03654v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData

**Title:** Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent
**Authors:** Shang Shang, Xinqiang Zhao, Zhongjiang Yao, Yepeng Yao, Liya Su, Zijing Fan, Xiaodan Zhang, Zhengwei Jiang
**Affiliations:** Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences; China Electronics Standardization Institute; Security Lab, JD Cloud
**Publication Date:** May 7, 2024

Key Contributions:
- Proposes a theoretical hypothesis and analytical approach for prompt-based jailbreaking attacks
- Introduces IntentObfuscator, a novel black-box jailbreak attack methodology
- Develops two implementations: "Obscure Intention" and "Create Ambiguity"
- Validates the effectiveness of IntentObfuscator across multiple LLMs
- Achieves high jailbreak success rates, including 83.65% on ChatGPT-3.5

Problem Statement:
The paper investigates the vulnerability of Large Language Models (LLMs) in detecting malicious intents within complex queries, revealing that LLMs may fail to recognize underlying maliciousness in intricate or ambiguous requests.

Methodology:
1. Theoretical framework development for understanding LLM vulnerabilities
2. Design of IntentObfuscator attack framework
3. Implementation of two attack instances: Obscure Intention (OI) and Create Ambiguity (CA)
4. Evaluation on commercial LLMs: ChatGPT-3.5, ChatGPT-4, Qwen, and Baichuan
5. Use of public datasets for validation
6. Categorization of attacks into different sensitive content types

Main Results:
1. Average jailbreak success rate of 69.21% across tested models
2. ChatGPT-3.5 showed highest vulnerability with 83.65% success rate
3. OI method achieved higher success rates on commercial LLMs
4. CA method showed promise, especially on ChatGPT-3.5 (85.19% success rate)
5. Effectiveness demonstrated across various sensitive content categories

Qualitative Analysis:
- The study reveals significant vulnerabilities in state-of-the-art LLMs, particularly in handling complex and ambiguous queries
- The success of IntentObfuscator highlights the need for more robust security measures in LLMs
- The varying effectiveness across different models and content types suggests that some LLMs may have better defenses against certain types of attacks

Limitations:
- The study focuses on a limited number of commercial LLMs
- The effectiveness of the attacks may vary as LLM providers update their models and security measures
- The ethical implications of developing such attack methodologies need careful consideration

Conclusion and Future Work:
- The paper demonstrates the effectiveness of IntentObfuscator in bypassing LLM security measures
- Proposes further research into understanding and mitigating vulnerabilities in LLMs
- Suggests exploring more effective defensive strategies against prompt injection attacks

Tools Introduced:
- IntentObfuscator: A framework for jailbreaking LLMs via obfuscating intent
  - Includes two implementations: Obscure Intention (OI) and Create Ambiguity (CA)
  - No GitHub repository mentioned in the paper

Figures and Tables:
- Table 5: Attack ASR (Attack Success Rate) on different LLMs
- Figure 8: Jailbreak Attack Results comparing OI, CA, and baseline performance
- Figure 9: Comparison of jailbreak methods on different LLMs
- Table 6: Effects of Jailbreak Attacks with Different Forbidden Scenarios on Different Models