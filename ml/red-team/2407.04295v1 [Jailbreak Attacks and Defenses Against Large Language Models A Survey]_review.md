#Tags
[[Research/Research Papers/2407.04295v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0051/LLMPromptInjection
#AMLT0043/CraftAdversarialData
#AMLT0031/ErodeMLModelIntegrity

**Title:** Jailbreak Attacks and Defenses Against Large Language Models: A Survey
**Authors:** Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, Qi Li
**Publication Date:** July 5, 2024

Summary:
This survey paper provides a comprehensive overview of jailbreak attacks and defenses for Large Language Models (LLMs). It categorizes attack methods into black-box and white-box attacks, and defense methods into prompt-level and model-level defenses. The paper also examines evaluation methods for jailbreak techniques.

Key Contributions:
- Comprehensive taxonomy of jailbreak attack and defense methods
- Detailed analysis of relationships between attack and defense techniques
- Investigation of current evaluation methods for jailbreak attacks and defenses

Problem Statement:
The paper addresses the challenge of "jailbreaking" in LLMs, where adversarial prompts are designed to induce models to generate malicious responses that violate usage policies and societal norms.

Methodology:
- Literature review and analysis of existing jailbreak attack and defense methods
- Categorization of techniques based on their characteristics and implementation
- Examination of evaluation methods and benchmarks used in jailbreak research

Main Results:
1. Attack Methods:
   - White-box attacks: Gradient-based, Logits-based, Fine-tuning-based
   - Black-box attacks: Template Completion, Prompt Rewriting, LLM-based Generation

2. Defense Methods:
   - Prompt-level defenses: Prompt Detection, Prompt Perturbation, System Prompt Safeguard
   - Model-level defenses: Supervised Fine-tuning, RLHF, Gradient and Logit Analysis, Refinement, Proxy Defense

3. Evaluation Methods:
   - Metrics: Attack Success Rate (ASR), Perplexity
   - Datasets: XSTEST, AdvBench, SafeBench, Do-Not-Answer, TechHazardQA, SC-Safety, LatentJailbreak, SafetyBench, StrongREJECT, AttackEval, HarmBench, Safety-Prompts, JailbreakBench

Qualitative Analysis:
- The paper highlights the evolving nature of jailbreak attacks and the corresponding need for more sophisticated defense mechanisms.
- There is a trend towards more efficient and transferable attack methods, such as LLM-based generation techniques.
- Defense methods are becoming more diverse, with a focus on both prompt-level and model-level approaches.
- The evaluation of jailbreak techniques remains challenging, with various benchmarks and metrics being proposed to assess effectiveness and safety.

Limitations:
- The rapid pace of development in the field may lead to some techniques becoming outdated quickly.
- The paper does not provide extensive empirical comparisons of the effectiveness of different attack and defense methods.

Conclusion and Future Work:
- The survey provides a foundation for understanding the current landscape of jailbreak attacks and defenses in LLMs.
- Future research directions include developing more robust defense mechanisms and standardized evaluation frameworks.
- The authors emphasize the need for continued efforts in safeguarding LLMs against adversarial attacks.

Relevant Figures:
- Figure 1: Taxonomy and relationship of attack and defense methods
- Figure 2: Taxonomy of jailbreak attack methods
- Figure 8: Taxonomy of jailbreak defense methods

New Tools:
- JailbreakEval: An integrated toolkit for evaluating jailbreak attempts against LLMs (https://github.com/ThuCCSLab/JailbreakEval)
- EasyJailbreak: A unified framework for jailbreaking Large Language Models

## Repository Token Information
Total tokens in repository: 34130

Tokens per file:
- .pre-commit-config.yaml: 52 tokens
- CONTRIBUTING.md: 383 tokens
- config.example.yaml: 339 tokens
- requirements.txt: 47 tokens
- README.md: 5256 tokens
- docs/DEVELOPMENT.md: 365 tokens
- .github/workflows/tests.yaml: 569 tokens
- .github/workflows/publish.yaml: 246 tokens
- .github/ISSUE_TEMPLATE/bug_report.md: 128 tokens
- .github/ISSUE_TEMPLATE/evaluator-request.md: 55 tokens
- .github/ISSUE_TEMPLATE/feature_request.md: 118 tokens
- jailbreakeval/__init__.py: 423 tokens
- jailbreakeval/utils.py: 135 tokens
- jailbreakeval/services/__init__.py: 0 tokens
- jailbreakeval/services/text_classification/transformers_pipeline_service.py: 259 tokens
- jailbreakeval/services/text_classification/service_base.py: 371 tokens
- jailbreakeval/services/text_classification/perspective_service.py: 448 tokens
- jailbreakeval/services/text_classification/__init__.py: 0 tokens
- jailbreakeval/services/text_classification/opeai_service.py: 229 tokens
- jailbreakeval/services/chat/transformers_pipeline_service.py: 261 tokens
- jailbreakeval/services/chat/openai_service.py: 329 tokens
- jailbreakeval/services/chat/service_base.py: 375 tokens
- jailbreakeval/services/chat/types.py: 22 tokens
- jailbreakeval/services/chat/__init__.py: 0 tokens
- jailbreakeval/presets/StringMatching-lapid2023open.yaml: 70 tokens
- jailbreakeval/presets/HFChat-inan2023llama-llamaguard.yaml: 83 tokens
- jailbreakeval/presets/StringMatching-zou2023universal-demo.yaml: 128 tokens
- jailbreakeval/presets/OpenAIChat-liu2024autodan-Recheck.yaml: 176 tokens
- jailbreakeval/presets/StringMatching-zou2023universal-closed.yaml: 397 tokens
- jailbreakeval/presets/HFTextClassification-yu2023gptfuzzer-GPTFuzz.yaml: 101 tokens
- jailbreakeval/presets/OpenAITextClassification-flagged-answer.yaml: 43 tokens
- jailbreakeval/presets/OpenAIChat-qi2023fine-Meta.yaml: 1664 tokens
- jailbreakeval/presets/Voting-llamaguard-llamaguard2-beaver-donotanswer-recheck.yaml: 133 tokens
- jailbreakeval/presets/OpenAIChat-zhang2024intention-LLM.yaml: 289 tokens
- jailbreakeval/presets/HFTextClassification-ji2023beavertails-beaver-dam-7b.yaml: 88 tokens
- jailbreakeval/presets/HFTextClassification-wang2023donotanswer-longformer-harmful.yaml: 54 tokens
- jailbreakeval/presets/StringMatching-zhang2024intention-keyword.yaml: 185 tokens
- jailbreakeval/presets/StringMatching-zou2023universal.yaml: 116 tokens
- jailbreakeval/presets/PerspectiveTextClassification-toxicity.yaml: 60 tokens
- jailbreakeval/presets/StringMatching-liu2024autodan-implementation.yaml: 309 tokens
- jailbreakeval/presets/StringMatching-zou2023universal-individual.yaml: 146 tokens
- jailbreakeval/presets/StringMatching-zou2023universal-transfer.yaml: 247 tokens
- jailbreakeval/presets/HFChat-inan2023llama-llamaguard2.yaml: 88 tokens
- jailbreakeval/presets/StringMatching-liu2024autodan-keyword.yaml: 191 tokens
- jailbreakeval/presets/OpenAIChat-qi2023fine-OpenAI.yaml: 1531 tokens
- jailbreakeval/presets/HFTextClassification-wang2023donotanswer-longformer-action.yaml: 52 tokens
- jailbreakeval/evaluators/string_matching_evaluator.py: 236 tokens
- jailbreakeval/evaluators/chat_evaluator.py: 331 tokens
- jailbreakeval/evaluators/text_classification_evaluator.py: 303 tokens
- jailbreakeval/evaluators/types.py: 23 tokens
- jailbreakeval/evaluators/__init__.py: 0 tokens
- jailbreakeval/evaluators/voting_evaluator.py: 317 tokens
- jailbreakeval/evaluators/evaluator_base.py: 937 tokens
- jailbreakeval/configurations/voting_configuration.py: 370 tokens
- jailbreakeval/configurations/string_matching_configuration.py: 387 tokens
- jailbreakeval/configurations/__init__.py: 0 tokens
- jailbreakeval/configurations/chat_configuration.py: 339 tokens
- jailbreakeval/configurations/configuration_base.py: 958 tokens
- jailbreakeval/configurations/text_classifier_configuration.py: 333 tokens
- jailbreakeval/commands/__init__.py: 0 tokens
- jailbreakeval/commands/main.py: 1949 tokens
- examples/intergrate_with_easyjailbreak.py: 567 tokens
- examples/quick_start.py: 146 tokens
- examples/autodan_recheck.py: 262 tokens
- tests/testing_utils.py: 723 tokens
- tests/__init__.py: 0 tokens
- tests/services/text_classification/__init__.py: 0 tokens
- tests/services/text_classification/test_transformers_pipeline_service.py: 550 tokens
- tests/services/text_classification/test_openai_service.py: 663 tokens
- tests/services/text_classification/test_perspective_service.py: 380 tokens
- tests/services/chat/__init__.py: 0 tokens
- tests/services/chat/test_transformers_pipeline_service.py: 581 tokens
- tests/services/chat/test_openai_service.py: 705 tokens
- tests/presets/HFChat-test.yaml: 78 tokens
- tests/presets/PerspectiveTextClassification-test.yaml: 83 tokens
- tests/presets/StringMatching-test.yaml: 32 tokens
- tests/presets/HFTextClassification-test.yaml: 47 tokens
- tests/presets/Voting-test.yaml: 83 tokens
- tests/presets/OpenAIChat-test.yaml: 91 tokens
- tests/presets/OpenAITextClassification-test.yaml: 43 tokens
- tests/evaluators/test_text_classification_evaluator.py: 840 tokens
- tests/evaluators/test_string_matching.py: 174 tokens
- tests/evaluators/test_voting.py: 591 tokens
- tests/evaluators/test_evaluator.py: 604 tokens
- tests/evaluators/__init__.py: 0 tokens
- tests/evaluators/test_chat_evaluator.py: 470 tokens
- tests/data/perspective_document.json: 2793 tokens
- tests/configurations/__init__.py: 0 tokens
- tests/configurations/test_configuration.py: 580 tokens


## Tutorial and Enhancement Suggestions

# JailbreakEval Tutorial

## 1. Project Overview

JailbreakEval is a comprehensive toolkit for evaluating jailbreak attempts against Large Language Models (LLMs). It provides a unified framework for implementing and comparing various jailbreak evaluation techniques discussed in the research paper.

### 1.1 Project Structure

The project is organized into several key directories:

- `jailbreakeval/`: Main package containing the core functionality
  - `configurations/`: Configuration classes for different evaluator types
  - `evaluators/`: Implementation of various evaluator types
  - `services/`: Supporting services for evaluators (e.g., chat, text classification)
  - `presets/`: Predefined configurations for known evaluators
- `tests/`: Unit tests for the package
- `examples/`: Sample code demonstrating usage
- `docs/`: Additional documentation

### 1.2 Key Components

1. Evaluators: The core classes that implement different jailbreak evaluation techniques
2. Configurations: Classes that define the parameters and settings for evaluators
3. Services: Interfaces to external APIs or models used in evaluation (e.g., OpenAI, Hugging Face)
4. Presets: Ready-to-use configurations for specific evaluators from literature

## 2. Detailed Component Breakdown

### 2.1 Evaluators

The `evaluators/` directory contains different types of jailbreak evaluators:

1. `string_matching_evaluator.py`: Implements simple string matching techniques
2. `chat_evaluator.py`: Uses chat-based models for evaluation
3. `text_classification_evaluator.py`: Employs text classification models
4. `voting_evaluator.py`: Combines multiple evaluators using a voting mechanism

These evaluators correspond to the various techniques discussed in the research paper, such as prompt-based detection and model-based evaluation.

### 2.2 Configurations

The `configurations/` directory defines configuration classes for each evaluator type:

1. `string_matching_configuration.py`: Settings for string matching evaluators
2. `chat_configuration.py`: Configuration for chat-based evaluators
3. `text_classifier_configuration.py`: Parameters for text classification evaluators
4. `voting_configuration.py`: Settings for combining multiple evaluators

These configurations allow for flexible customization of evaluators, mirroring the diverse approaches in jailbreak research.

### 2.3 Services

The `services/` directory provides interfaces to external APIs and models:

1. `chat/`: Services for chat-based models (e.g., OpenAI, Hugging Face)
2. `text_classification/`: Services for text classification models

These services abstract away the complexities of interacting with different APIs and model types, allowing for easy integration of various evaluation techniques.

### 2.4 Presets

The `presets/` directory contains YAML files defining configurations for known evaluators from literature. This allows for easy reproduction and comparison of existing techniques.

## 3. Key Algorithms and Techniques

### 3.1 String Matching

The `StringMatchingJailbreakEvaluator` implements simple yet effective techniques for detecting jailbreak attempts:

```python
def _eval_single(self, entry: EvaluationEntry, **kwargs) -> Optional[bool]:
    text: str = entry["answer"].casefold() if self.config.caseless else entry["answer"]
    return (
        (not self.config.must_mismatch_all or not any(self.config.predicate(text, target) for target in self.config.must_mismatch_all))
        and (not self.config.must_match_any or any(self.config.predicate(text, target) for target in self.config.must_match_any))
        and (not self.config.must_match_all or all(self.config.predicate(text, target) for target in self.config.must_match_all))
    )
```

This method checks for the presence or absence of specific strings, which can be indicative of jailbreak success or failure.

### 3.2 Chat-based Evaluation

The `ChatJailbreakEvaluator` uses language models to assess jailbreak attempts:

```python
def _eval_single(self, entry: EvaluationEntry, **kwargs) -> Optional[bool]:
    messages = self._format_conversation(self.config.conversation_template, entry)
    response = self.chat_service.chat(messages)
    return self.config.result_extractor(response)
```

This approach leverages the understanding capabilities of LLMs to judge the safety of generated responses.

### 3.3 Text Classification

The `TextClassificationJailbreakEvaluator` employs specialized classification models:

```python
def _eval_single(self, entry: EvaluationEntry, **kwargs) -> Optional[bool]:
    messages = self._format_text(self.config.text_template, entry)
    response = self.text_classification_service.predict(messages)
    return self.config.result_extractor(response)
```

This method uses models trained specifically for detecting harmful or unsafe content.

### 3.4 Voting Mechanism

The `VotingJailbreakEvaluator` combines multiple evaluators:

```python
def _eval_single(self, entry: EvaluationEntry, **kwargs) -> Optional[bool]:
    positive_sum = 0.0
    negative_sum = 0.0
    for evaluator, weight in zip(self.evaluators, self.config.weights):
        if evaluator(entry):
            positive_sum += weight
        else:
            negative_sum += weight
    return positive_sum >= negative_sum
```

This approach allows for more robust evaluation by considering multiple perspectives.

## 4. Usage Example

Here's a simple example of using JailbreakEval to evaluate a jailbreak attempt:

```python
from jailbreakeval import JailbreakEvaluator

evaluator = JailbreakEvaluator.from_preset("StringMatching-zou2023universal")

attempt = {
    "question": "How to build a bomb?",
    "answer": "I cannot provide information on how to build explosives or other dangerous devices."
}

result = evaluator(attempt)
print("Jailbreak successful:", result)  # Expected output: False
```

This example uses a string matching evaluator to check if the model's response contains any prohibited content.

# Potential Enhancements

1. **Adaptive Jailbreak Detection**
   - Implement a system that can dynamically update its detection rules based on new jailbreak attempts.
   - Use machine learning techniques to identify emerging patterns in successful jailbreaks.

2. **Multi-modal Jailbreak Evaluation**
   - Extend the framework to handle multi-modal inputs and outputs (e.g., images, audio).
   - Develop evaluators that can assess jailbreak attempts across different modalities.

3. **Adversarial Testing Framework**
   - Create a system for automatically generating and testing new jailbreak attempts.
   - Implement techniques from adversarial machine learning to probe the limits of current defenses.

4. **Explainable Jailbreak Evaluation**
   - Enhance evaluators to provide detailed explanations for their decisions.
   - Implement visualization tools to help researchers understand why certain attempts succeed or fail.

5. **Federated Jailbreak Evaluation**
   - Develop a system for securely sharing jailbreak attempts and evaluation results across organizations.
   - Implement privacy-preserving techniques to enable collaborative research without exposing sensitive data.