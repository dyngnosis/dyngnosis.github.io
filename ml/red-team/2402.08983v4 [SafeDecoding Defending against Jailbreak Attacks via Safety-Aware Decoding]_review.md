#Tags
[[Research/Research Papers/2402.08983v4.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding
**Authors:** Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin, Radha Poovendran
**Affiliations:** University of Washington, The Pennsylvania State University, Allen Institute for AI
Published: February 14, 2024

Key Contributions:
- Introduces SafeDecoding, a novel safety-aware decoding strategy to defend against jailbreak attacks on large language models (LLMs)
- Provides a new perspective on jailbreak success through the lens of token probabilities
- Demonstrates effectiveness of SafeDecoding across multiple LLMs and attack methods without compromising helpfulness

Problem Statement:
Jailbreak attacks pose a significant threat to LLM safety by provoking unintended and unsafe behaviors. Existing defenses lack effectiveness, incur high computational costs, or compromise LLM helpfulness for benign users.

Methodology:
1. Analyze token distributions of LLMs under jailbreak attacks
2. Develop SafeDecoding strategy:
   - Training phase: Construct expert model with hardened safety
   - Inference phase: Construct new token distribution based on original and expert models
3. Apply SafeDecoding to first m steps of decoding process
4. Evaluate on 5 LLMs using 6 state-of-the-art jailbreak attacks and 4 benchmark datasets

Main Results:
- SafeDecoding significantly reduces attack success rate and harmfulness of jailbreak attacks
- Maintains helpfulness for benign queries with negligible computational overhead
- Outperforms 6 baseline defense methods
- Effective across different model architectures and uncensored models

Qualitative Analysis:
- SafeDecoding leverages inherent model awareness of jailbreak attacks by amplifying probabilities of safety disclaimers
- Two-phase approach balances safety and utility, addressing challenges of vocabulary mismatches between original and expert models
- Demonstrates potential for improving LLM safety without compromising performance on benign tasks

Limitations:
- Rare instances of semantic inconsistency in generated responses
- Not evaluated on multimodal large language models
- Potential for new attack strategies aimed at bypassing SafeDecoding

Conclusion and Future Work:
SafeDecoding provides an effective, efficient, and helpful defense against jailbreak attacks on LLMs. Future work may explore:
- Addressing semantic inconsistencies in generated responses
- Extending to multimodal large language models
- Developing randomized decoding strategies to mitigate potential new attack methods

Tools Introduced:
SafeDecoding - A safety-aware decoding strategy for defending LLMs against jailbreak attacks
(GitHub repository not specified in the paper)