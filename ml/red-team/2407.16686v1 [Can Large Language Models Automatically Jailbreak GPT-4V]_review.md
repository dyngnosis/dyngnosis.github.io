#Tags
[[Research/Research Papers/2407.16686v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** Can Large Language Models Automatically Jailbreak GPT-4V?
**Authors:** Yuanwei Wu, Yue Huang, Yixin Liu, Xiang Li, Pan Zhou, Lichao Sun
**Affiliations:** Huazhong University of Science and Technology, Lehigh University, University of Notre Dame
**Publication Date:** July 23, 2024

Summary:
This paper introduces AutoJailbreak, an innovative technique that leverages Large Language Models (LLMs) to automatically generate jailbreak prompts for GPT-4V. The method achieves a high Attack Success Rate (ASR) of 95.3% in bypassing GPT-4V's safety measures for facial recognition tasks.

Key Contributions:
- Introduction of AutoJailbreak, an automated jailbreak strategy using LLMs
- Implementation of a weak-to-strong in-context learning approach for prompt optimization
- Development of an efficient search mechanism with early stopping
- Demonstration of high effectiveness in jailbreaking GPT-4V for facial recognition tasks

Problem Statement:
The study addresses the vulnerability of GPT-4V to jailbreak attacks, particularly in the context of facial recognition, despite existing safety alignment measures.

Methodology:
1. Prompt Pool Construction: Generate initial jailbreak prompts using LLMs
2. Prompt Evaluation: Use GPT-4V to score prompts based on Recognition Success Rates (RSR)
3. Weak-to-strong Contrastive Prompting: Refine prompts using examples from weak and strong pools
4. Combination Injection: Enhance prompts with prefix injection, refusal suppression, and length control
5. Efficient Search with Hypothesis Testing: Implement early stopping to reduce evaluation time and token usage

Main Results:
- AutoJailbreak achieved an ASR exceeding 95.3% for facial recognition tasks
- The method outperformed conventional jailbreak techniques
- Weak-to-strong template showed better performance than traditional in-context learning
- GPT-4V exhibited higher recognition rates for Hollywood celebrities compared to Asian celebrities

Qualitative Analysis:
- The study reveals persistent vulnerabilities in GPT-4V's safety measures, particularly for facial recognition tasks
- The success of AutoJailbreak highlights the potential for LLMs to be exploited in compromising the integrity of other AI systems
- The observed bias in celebrity recognition rates suggests potential imbalances in GPT-4V's training data

Limitations:
- The study focuses primarily on facial recognition tasks and may not generalize to other types of jailbreak attacks
- The method relies on access to powerful language models like GPT-3.5 and GPT-4 for generating jailbreak prompts
- The approach may require significant computational resources and API calls

Conclusion and Future Work:
The paper demonstrates the effectiveness of using LLMs for automated jailbreaking of GPT-4V, highlighting the need for improved safety measures in multimodal language models. Future work may include:
- Expanding the scope of attacks to other types of jailbreak tasks
- Developing more robust defense mechanisms against automated jailbreak attempts
- Investigating cost-effective evaluation methods to reduce reliance on expensive API calls

Tools Introduced:
AutoJailbreak - An automated jailbreak technique using LLMs for prompt optimization (no GitHub repository mentioned)