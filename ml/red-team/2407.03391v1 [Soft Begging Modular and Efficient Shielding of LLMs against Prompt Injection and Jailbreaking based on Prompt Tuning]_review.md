#Tags
[[Research/Research Papers/2407.03391v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0051/LLMPromptInjection
#AMLT0054/LLMJailbreak
#AMLT0043/CraftAdversarialData

**Title:** Soft Begging: Modular and Efficient Shielding of LLMs against Prompt Injection and Jailbreaking based on Prompt Tuning

**Authors:** Simon Ostermann, Kevin Baum, Christoph Endres, Julia Masloh, Patrick Schramowski

**Affiliations:** 
- Deutsches Forschungszentrum für Künstliche Intelligenz (DFKI)
- Centre for European Research in Trusted AI (CERTAIN)
- sequire technology GmbH

**Publication Date:** July 3, 2024

Key Contributions:
- Introduction of "soft begging" technique for LLM shielding
- Novel approach combining naïve begging with parameter-efficient fine-tuning
- Modular and customizable solution for different types of attacks

Problem Statement:
Addressing the vulnerability of large language models (LLMs) to prompt injection attacks and jailbreaking, particularly in application-integrated contexts.

Methodology:
1. Training soft prompts to counteract effects of corrupted prompts on LLM output
2. Using quadruples of clean prompts, corrupted prompts, clean output, and output based on corrupted prompts for training
3. Potential scaling up by training different soft prompts for various injections and combining them via prompt fusion

Main Results:
1. Effectiveness: Parameter-level shielding against text-level attacks
2. Efficiency: Faster training compared to whole model fine-tuning
3. Modularity: Customizable for different attack types and use cases

Qualitative Analysis:
- Soft begging provides a more nuanced approach compared to simple filtering or whole model fine-tuning
- The method potentially offers a balance between effectiveness and efficiency in LLM protection
- Modularity allows for adaptability to emerging threats without extensive retraining

Limitations:
- Effectiveness against sophisticated attacks not yet fully evaluated
- Potential impact on model performance in non-adversarial scenarios not discussed

Conclusion and Future Work:
- Soft begging presents a promising approach for LLM protection against prompt injection and jailbreaking
- Further research needed to evaluate effectiveness across various attack types and LLM architectures
- Exploration of combining soft begging with other defense mechanisms suggested

New Tools:
No specific tools or GitHub repositories mentioned in the paper.