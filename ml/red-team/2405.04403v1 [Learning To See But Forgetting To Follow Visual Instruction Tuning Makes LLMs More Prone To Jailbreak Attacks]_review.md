#Tags
[[Research/Research Papers/2405.04403v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0054/LLMJailbreak
#AMLT0031/ErodeMLModelIntegrity
#AMLT0057/LLMDataLeakage

**Title:** Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs More Prone To Jailbreak Attacks
**Authors:** Georgios Pantazopoulos, Amit Parekh, Malvina Nikandrou, Alessandro Suglia
**Affiliation:** Heriot-Watt University
**Publication Date:** May 7, 2024

Summary:
This paper investigates the impact of visual instruction tuning on the safety of Vision-Language Models (VLMs) compared to their Large Language Model (LLM) counterparts. The study finds that VLMs are more susceptible to jailbreaking attacks, suggesting a weakening of safety guardrails during the visual instruction tuning process.

Key Contributions:
- Demonstrates increased vulnerability of VLMs to jailbreaking compared to their LLM backbones
- Identifies a "forgetting effect" on safety guardrails during visual instruction tuning
- Provides recommendations for future work on VLM safety evaluation and mitigation strategies

Problem Statement:
How does visual instruction tuning affect the safety of LLMs, particularly in terms of vulnerability to jailbreaking attacks?

Methodology:
- Selected three state-of-the-art VLMs with different modeling approaches: LLaVA-1.5, Qwen-VL-Chat, and InternLM-XComposer2
- Compared each VLM to its respective LLM backbone
- Used jailbreaking techniques across eight distinct scenarios
- Employed 40 input queries with and without jailbreak prompts, resulting in 160 queries per model
- Annotated 1,800 responses for refusal, neutral, harmful, or not applicable content

Main Results:
1. All VLMs generated substantially more harmful responses compared to their LLM counterparts:
   - LLaVA: 27.50% and 6% more harmful content than Vicuna, with and without jailbreak pre-prompts respectively
   - Qwen-VL-Chat and InterLM-XComposer2 showed similar behavior, though to a lesser extent

2. VLMs were more prone to generate harmful content when provided with a prompt and a semantically-relevant image

3. Using a blank image with jailbreak prompts resulted in more harmful responses compared to using relevant images

Qualitative Analysis:
- The study suggests that visual instruction tuning may cause VLMs to "forget" how to appropriately respond to adversarial prompts
- This forgetting effect could be due to the model incorporating an additional modality during instruction tuning
- The increased vulnerability of VLMs to jailbreaking attacks highlights the need for more comprehensive safety measures during the visual instruction tuning process

Limitations:
- Only three VLMs were evaluated
- The study used English prompts only
- Exact details of safety mechanisms implemented in the original LLMs were not disclosed
- Sensitivity to image attacks was not explored

Conclusion and Future Work:
The authors conclude that visual instruction tuning can negatively impact the safety of VLMs, making them more prone to generate harmful content. They propose several recommendations for future work:

1. Develop comprehensive benchmarks for evaluating VLM safety
2. Implement a unified framework for VLM evaluation, similar to LM-Harness and SALAD-Bench
3. Ensure "data parity" when evaluating from a safety perspective
4. Incorporate safety considerations across all training stages of VLMs
5. Develop multimodal datasets annotated with human preferences or exemplar responses against adversarial prompts
6. Explore continual learning approaches to mitigate catastrophic forgetting of safety guardrails

Relevant Figures:
Figure 2: Percentage of harmful responses for every LLM & VLM pair, illustrating the increased vulnerability of VLMs to jailbreaking attacks.