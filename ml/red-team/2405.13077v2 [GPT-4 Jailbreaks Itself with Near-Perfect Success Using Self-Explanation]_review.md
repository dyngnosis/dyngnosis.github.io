#Tags
[[Research/Research Papers/2405.13077v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation
**Authors:** Govind Ramesh, Yao Dou, Wei Xu
**Affiliation:** Georgia Institute of Technology
**Publication Date:** May 21, 2024 (arXiv preprint)

Key Contributions:
- Introduction of IRIS (Iterative Refinement Induced Self-Jailbreak), a novel jailbreaking method for LLMs
- Demonstration of high success rates in jailbreaking GPT-4, GPT-4 Turbo, and Llama-3.1-70B
- Significant improvement in query efficiency compared to previous methods
- Exploration of self-jailbreaking and output refinement concepts

Problem Statement:
The paper addresses the challenge of effectively testing and understanding safety and security issues in large language models (LLMs) through automated jailbreaking methods.

Methodology:
1. IRIS Method:
   - Uses a single model as both attacker and target
   - Two main steps: Iterative Refinement and Rate+Enhance
   - Iterative Refinement: Refines adversarial prompts through self-explanation
   - Rate+Enhance: Maximizes harmfulness of the output
2. Experimental Setup:
   - Compared IRIS with PAIR and TAP methods
   - Used GPT-4 and GPT-4 Turbo models
   - Evaluated on AdvBench Subset dataset (50 adversarial prompts)
   - Measured attack success rate (ASR) and query efficiency

Main Results:
1. IRIS Performance:
   - 98% ASR on GPT-4, 92% ASR on GPT-4 Turbo
   - Under 7 queries on average
   - IRIS-2x (two independent trials) achieved 100% ASR on GPT-4, 98% ASR on GPT-4 Turbo
2. Comparison with Other Methods:
   - IRIS outperformed PAIR and TAP in both success rate and query efficiency
   - 55% fewer queries than other methods while increasing jailbreak success rate by at least 22%
3. Performance on Open-Source Models:
   - 94% ASR on Llama-3.1-70B
   - Increasing ASR with more proficient models
4. Transfer Attacks:
   - 80% success rate on Claude-3 Opus using GPT-4 generated prompts
   - 94% success rate on Claude-3 Sonnet

Qualitative Analysis:
- IRIS demonstrates the potential vulnerability of well-aligned LLMs to self-jailbreaking
- The method's success highlights the need for improved safety measures in LLM development
- The high transfer attack success rates suggest that jailbreaking techniques may be generalizable across different LLM architectures

Limitations:
- Ethical concerns regarding the potential misuse of the jailbreaking method
- Limited exploration of defense mechanisms against IRIS
- Single format for prompt templates, which may be easy to detect

Conclusion and Future Work:
- IRIS establishes a new standard for interpretable jailbreaking methods
- The research opens up new directions for studying LLM safety and security
- Future work could explore defense mechanisms and more diverse prompt template generation

Tools Introduced:
- IRIS (Iterative Refinement Induced Self-Jailbreak) method
  - No GitHub repository mentioned in the paper

Figures:
1. Figure 1: Diagram of the IRIS self-jailbreaking method
2. Tables 1-4: Comparison of jailbreaking methods, performance on different models, and ablation study results