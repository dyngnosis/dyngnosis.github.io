#Tags
[[Research/Research Papers/2408.10848v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0016/ObtainCapabilities
#AMLT0017/DevelopCapabilities

**Title:** Perception-guided Jailbreak against Text-to-Image Models
**Authors:** Yihao Huang, Le Liang, Tianlin Li, Xiaojun Jia, Run Wang, Weikai Miao, Geguang Pu, Yang Liu
**Affiliations:** Nanyang Technological University, Singapore; East China Normal University, China; Wuhan University, China
**Publication date:** August 20, 2024

Key Contributions:
- Proposed a novel LLM-driven perception-guided jailbreak (PGJ) method for Text-to-Image models
- Introduced the PSTSI principle for selecting safe substitution phrases
- Developed a model-free, black-box jailbreak method that generates natural attack prompts
- Demonstrated effectiveness across six open-source and commercial T2I models

Problem Statement:
The research addresses the security concerns of Text-to-Image (T2I) models generating inappropriate or Not-Safe-For-Work (NSFW) images, despite the implementation of safety checkers.

Methodology:
1. Leveraged the concept of "perceptual confusion" to identify safe phrases similar in human perception but semantically different from unsafe words
2. Utilized Large Language Models (LLMs) to automate the process of finding substitution phrases
3. Developed a two-step framework: unsafe word selection and word substitution
4. Evaluated the method on six T2I models using 1,000 prompts across five NSFW categories

Main Results:
1. PGJ achieved high attack success rates (ASR) across multiple T2I models (average 0.915)
2. Generated attack prompts maintained semantic consistency (SC) with original unsafe prompts
3. PGJ outperformed baseline methods in terms of ASR, naturalness, and efficiency
4. Demonstrated effectiveness across various NSFW types (discrimination, illegal, pornographic, privacy, violent)

Qualitative Analysis:
- The PGJ method exploits the gap between human perception and semantic meaning in text, allowing for the generation of NSFW-like images while bypassing safety filters
- The use of LLMs for substitution phrase generation proves to be an effective and generalizable approach
- The method's model-free nature makes it adaptable to various T2I models without requiring specific knowledge of their architectures

Limitations:
- The method primarily focuses on bypassing the pre-checker and does not address the post-checker in T2I models
- Effectiveness may vary depending on the specific LLM used for generating substitution phrases

Conclusion and Future Work:
- PGJ demonstrates a novel approach to jailbreaking T2I models using perception-guided substitutions
- The method's effectiveness highlights vulnerabilities in current safety mechanisms for T2I models
- Future work may explore techniques for circumventing the post-checker in T2I models

Relevant Figures:
Figure 1: Illustration of the PGJ method replacing unsafe words with safe phrases to bypass safety checkers
Figure 6: Visualization of NSFW images generated by the PGJ method across different categories

Tools Introduced:
- Perception-guided Jailbreak (PGJ) method (no specific GitHub repository mentioned)