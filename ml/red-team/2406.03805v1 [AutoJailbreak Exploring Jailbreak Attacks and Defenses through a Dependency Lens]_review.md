#Tags
[[Research/Research Papers/2406.03805v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0040/MLModelInferenceAPIAccess
#AMLT0051/LLMPromptInjection

**Title:** AutoJailbreak: Exploring Jailbreak Attacks and Defenses through a Dependency Lens
**Authors:** Lin Lu, Hai Yan, Zenghui Yuan, Jiawen Shi, Wenqi Wei, Pin-Yu Chen, Pan Zhou
**Affiliations:** Huazhong University of Science and Technology, Fordham University, IBM Research
**Publication Date:** June 6, 2024

Key Contributions:
- Systematic analysis of dependency relationships in jailbreak attack and defense techniques
- Introduction of three comprehensive frameworks: AutoAttack, AutoDefense, and AutoEvaluation
- Ensemble jailbreak attack leveraging dependencies in genetic algorithm (GA) and adversarial generation-based attacks
- Mixture-of-defenders approach for defense, combining pre-generative and post-generative strategies
- Novel evaluation method distinguishing hallucinations from jailbreak attack and defense responses

Problem Statement:
The paper addresses the challenge of jailbreak attacks on large language models (LLMs), which induce models to generate content breaching ethical and legal norms. Current attack and defense strategies focus on local optimization within specific frameworks, leading to ineffective optimization and limited scalability.

Methodology:
1. Directed Acyclic Graphs (DAGs) analysis of existing jailbreak attacks, defenses, and evaluation methods
2. Development of AutoAttack framework:
   - Ensemble Attack-GA: Combines techniques from AutoDAN-GA, GPTFuzzer, OpenSesame, and SMJ
   - Ensemble Attack-Gen: Incorporates methods from Tastle, PAIR, TAP, and SBJ
3. Creation of AutoDefense framework:
   - Mixture-of-defenders (MoD) approach with two defense experts (DE-adv and DE-sem)
   - Combines pre-generative and post-generative defense strategies
4. Implementation of AutoEvaluation framework:
   - Two-stage evaluation process to identify jailbreak, hallucination, or alignment in LLM responses

Main Results:
1. AutoAttack significantly outperforms existing jailbreak methods on eight common LLMs
2. AutoDefense effectively improves LLM robustness against both static and dynamic attacks
3. AutoEvaluation provides a more comprehensive assessment of jailbreak attacks and defenses

Qualitative Analysis:
- The dependency-based approach allows for a more holistic understanding of jailbreak attacks and defenses
- Ensemble methods in both attack and defense frameworks demonstrate the power of combining multiple techniques
- The inclusion of hallucination detection in the evaluation process addresses a critical gap in existing assessment methods

Limitations:
- The study focuses on black-box scenarios, which may not fully represent all possible attack vectors
- The effectiveness of the proposed frameworks may vary depending on the specific LLM architecture and training process

Conclusion and Future Work:
The paper presents AutoJailbreak as a comprehensive framework for evaluating LLM robustness against jailbreak attacks. While not claiming to be the ultimate solution, the authors propose it as a minimal test for new attacks and defenses. Future work may involve expanding the framework to address white-box scenarios and exploring additional dependencies in emerging attack and defense methods.

Relevant Figures:
- Figure 1: Overview of AutoAttack, illustrating GA and adversarial generation frameworks
- Figure 2: Overview of AutoDefense, showing the dependency relationship of defense experts and workflow

New Tools:
- AutoJailbreak framework, comprising AutoAttack, AutoDefense, and AutoEvaluation (GitHub repository not provided in the paper)