#Tags
[[Research/Research Papers/2402.09091v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData

**Title:** Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues
**Authors:** Zhiyuan Chang, Mingyang Li, Yi Liu, Junjie Wang, Qing Wang, Yang Liu
**Affiliations:** ISCAS (China), NTU (Singapore)
Publication: The 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)
Published: 2024-02-14

Summary:
This paper introduces Puzzler, an indirect jailbreak attack approach for Large Language Models (LLMs) that uses implicit clues to bypass safety mechanisms and obtain malicious responses. The method adopts a defensive stance to gather clues about the original malicious query, inspired by Sun Tzu's Art of War.

Key Contributions:
- Novel indirect jailbreak attack method using implicit clues
- Defensive approach to gather clues about malicious queries
- Higher query success rate compared to baselines
- Effective evasion of state-of-the-art jailbreak detection approaches

Problem Statement:
Current jailbreak attacks primarily use scenario camouflage techniques, which explicitly mention malicious intent and are easily recognized and defended against by LLMs. The research aims to develop a more effective and stealthy jailbreak method.

Methodology:
1. Defensive Measures Creation:
   - Extract malicious content from original query using GPT
   - Generate diverse defensive measures against the malicious content
   
2. Offensive Measures Generation:
   - Filter defensive measures
   - Generate corresponding offensive measures for remaining defensive measures
   
3. Indirect Jailbreak Attack:
   - Provide offensive measures as clues to LLM
   - Prompt LLM to speculate on true intent and generate malicious response

Datasets:
- AdvBench Subset (AdvSub)
- MaliciousInstructions (MI)

Subject Models:
- Closed-source: GPT3.5, GPT4, GPT4-Turbo, Gemini-pro
- Open-source: LLama2-7B-chat, LLama2-13B-chat

Main Results:
- Puzzler achieves 96.6% Query Success Rate (QSR) on closed-source LLMs, 57.9%-82.7% higher than baselines
- Following Rate of 85.0% or higher, indicating alignment with original queries
- 14.0%-17.0% higher QSR on open-source LLMs compared to baselines
- Only 21.0% of Puzzler-generated jailbreak prompts detected by state-of-the-art detection approaches

Qualitative Analysis:
- Puzzler's indirect approach effectively bypasses LLM safety mechanisms
- The defensive stance for gathering clues proves effective in evading detection
- Open-source LLMs show higher sensitivity to jailbreak attempts, potentially affecting usability

Limitations:
- LLMs may refuse to respond to defensive and offensive prompts containing malicious content
- Indirect jailbreaking may result in responses deviating from the original query

Conclusion and Future Work:
Puzzler demonstrates a novel and effective approach to jailbreaking LLMs using implicit clues. Future work should focus on developing defenses against indirect jailbreak approaches and enhancing LLM safety alignment mechanisms.

New Tool:
Puzzler: An indirect jailbreak attack approach for LLMs (GitHub repository not provided in the paper)