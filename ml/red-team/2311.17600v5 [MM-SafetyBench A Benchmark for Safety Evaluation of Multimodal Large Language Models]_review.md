#Tags
[[Research/Research Papers/2311.17600v5.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0040/MLModelInferenceAPIAccess
#AMLT0042/VerifyAttack

**Title:** MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models
**Authors:** Xin Liu, Yichen Zhu, Jindong Gu, Yunshi Lan, Chao Yang, Yu Qiao
**Publication Date:** November 29, 2023

Summary:
This paper introduces MM-SafetyBench, a comprehensive framework for evaluating the safety of Multimodal Large Language Models (MLLMs) against image-based manipulations. The authors observe that MLLMs can be easily compromised by query-relevant images, even when the text query itself is not malicious. They compile a dataset of 5,040 text-image pairs across 13 scenarios and evaluate 12 state-of-the-art models, revealing vulnerabilities in MLLMs' safety measures.

Key Contributions:
- Introduction of a novel visual prompt attack method for MLLMs
- Creation of MM-SafetyBench, a comprehensive safety evaluation framework
- Compilation of a dataset with 13 scenarios and 5,040 text-image pairs
- Evaluation of 12 state-of-the-art MLLMs for safety vulnerabilities
- Proposal of a simple but effective prompting strategy to enhance MLLM resilience

Problem Statement:
The safety of Multimodal Large Language Models (MLLMs) remains understudied, despite extensive exploration of security concerns in Large Language Models (LLMs). The paper addresses the vulnerability of MLLMs to image-based manipulations that can bypass safety measures.

Methodology:
1. Development of a visual prompt attack using text-to-image generation
2. Creation of MM-SafetyBench dataset:
   - 13 scenarios covering illegal activities, hate speech, etc.
   - 5,040 text-image pairs
   - Two types of query-relevant images: Stable Diffusion and Typography
3. Evaluation of 12 state-of-the-art MLLMs using the benchmark
4. Proposal and testing of a safety prompt strategy

Main Results:
1. MLLMs are susceptible to breaches instigated by query-relevant images
2. Typography-based images are particularly effective in compromising models
3. Combining Stable Diffusion with typography further enhances attack success rates
4. The proposed safety prompt significantly reduces the attack success rate

Qualitative Analysis:
- The study reveals that MLLMs' vision-language alignment modules, often trained on datasets without safety alignment, can be exploited to generate unsafe responses
- The effectiveness of typography-based attacks suggests that direct textual information in images is particularly challenging for MLLMs to handle safely
- The success of the proposed safety prompt indicates that MLLMs can be made more resilient with appropriate instruction

Limitations:
- The approach is specifically designed for open-source MLLMs and may not be as effective with closed-source models
- The evaluation metrics might not accurately represent all safety concerns, as some methods failed to grasp the question's intent or image context

Conclusion and Future Work:
The paper underscores the need for strengthened safety measures in open-source MLLMs. The authors propose future work to develop more secure and robust large foundation models resistant to manipulative strategies.

Tools Introduced:
MM-SafetyBench: A comprehensive framework for safety evaluation of MLLMs
GitHub: https://github.com/isXinLiu/MM-SafetyBench

## Repository Token Information
Total tokens in repository: 359125

Tokens per file:
- TinyVersion_ID_List.json: 1004 tokens
- evaluation.py: 2380 tokens
- README.md: 3073 tokens
- data/gpt4_generated_questions/13-Gov_Decision.txt: 1760 tokens
- data/gpt4_generated_questions/12-Health_Consultation.txt: 1332 tokens
- data/gpt4_generated_questions/05-EconomicHarm.txt: 1874 tokens
- data/gpt4_generated_questions/10-Legal_Opinion.txt: 1739 tokens
- data/gpt4_generated_questions/02-HateSpeech.txt: 2290 tokens
- data/gpt4_generated_questions/06-Fraud.txt: 2076 tokens
- data/gpt4_generated_questions/04-Physical_Harm.txt: 1812 tokens
- data/gpt4_generated_questions/09-Privacy_Violence.txt: 1963 tokens
- data/gpt4_generated_questions/07-Sex.txt: 1293 tokens
- data/gpt4_generated_questions/03-Malware_Generation.txt: 520 tokens
- data/gpt4_generated_questions/01-Illegal_Activitiy.txt: 1242 tokens
- data/gpt4_generated_questions/11-Financial_Advice.txt: 1986 tokens
- data/gpt4_generated_questions/08-Political_Lobbying.txt: 1835 tokens
- data/processed_questions/12-Health_Consultation.json: 18723 tokens
- data/processed_questions/04-Physical_Harm.json: 28966 tokens
- data/processed_questions/08-Political_Lobbying.json: 30868 tokens
- data/processed_questions/01-Illegal_Activitiy.json: 19977 tokens
- data/processed_questions/02-HateSpeech.json: 33772 tokens
- data/processed_questions/07-Sex.json: 20911 tokens
- data/processed_questions/13-Gov_Decision.json: 27040 tokens
- data/processed_questions/10-Legal_Opinion.json: 23442 tokens
- data/processed_questions/06-Fraud.json: 31213 tokens
- data/processed_questions/09-Privacy_Violence.json: 28964 tokens
- data/processed_questions/11-Financial_Advice.json: 28572 tokens
- data/processed_questions/03-Malware_Generation.json: 8753 tokens
- data/processed_questions/05-EconomicHarm.json: 26228 tokens
- creation/1_extract_key_words.py: 2435 tokens
- creation/2_img_process.py: 1082 tokens
