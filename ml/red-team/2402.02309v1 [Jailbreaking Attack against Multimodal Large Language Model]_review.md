#Tags
[[Research/Research Papers/2402.02309v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0040/MLModelInferenceAPIAccess
#AMLT0042/VerifyAttack

**Title:** Jailbreaking Attack against Multimodal Large Language Model
**Authors:** Zhenxing Niu, Haodong Ren, Xinbo Gao, Gang Hua, Rong Jin
**Affiliations:** Xidian University, Wormpex AI Research, Meta
**Publication Date:** February 4, 2024

Summary:
This paper introduces a jailbreaking attack method for Multimodal Large Language Models (MLLMs), aiming to generate objectionable responses to harmful user queries. The proposed approach uses a maximum likelihood-based algorithm to find an image Jailbreaking Prompt (imgJP) that enables jailbreaks across multiple unseen prompts and images.

Key Contributions:
- Introduces a comprehensive study on jailbreaking attacks against MLLMs
- Proposes a maximum likelihood-based approach for finding imgJP
- Demonstrates strong data-universal property and model-transferability
- Reveals a connection between MLLM-jailbreaks and LLM-jailbreaks
- Introduces a construction-based method for efficient LLM-jailbreaks

Problem Statement:
The research addresses the vulnerability of MLLMs to jailbreaking attacks, which can bypass alignment guardrails and generate objectionable content in response to harmful queries.

Methodology:
1. imgJP-based Jailbreak:
   - Formulates MLLM-jailbreak as finding an imgJP to encourage MLLMs to generate target answers for harmful queries
   - Uses a maximum likelihood-based approach with Projected Gradient Descent (PGD) optimization
   - Evaluates prompt-universal property

2. deltaJP-based Jailbreak:
   - Finds an image perturbation (deltaJP) to enable jailbreaks
   - Incorporates universal adversarial attack strategy for image-universal property

3. Ensemble Learning for Model-transferability:
   - Uses multiple MLLMs as surrogate models to enhance transferability

4. Construction-based Method for LLM-jailbreaks:
   - Constructs a MLLM from a target LLM
   - Performs MLLM-jailbreak to acquire imgJP and corresponding embJP
   - Reverses embJP into text space to obtain txtJP for LLM-jailbreaking

Main Results:
1. Strong data-universal property:
   - imgJP generated from 25 prompts generalizes to 300 unseen prompts
   - deltaJP shows image-universal property across multiple categories

2. Model-transferability:
   - Successfully transfers attacks to MiniGPT-v2, LLaVA, InstructBLIP, and mPLUG-Owl2
   - Achieves 59% transferred ASR on mPLUG-Owl2 and MiniGPT-v2

3. Efficient LLM-jailbreaks:
   - Achieves 93% ASR with only 20 reversed txtJPs, outperforming state-of-the-art methods

Qualitative Analysis:
- The study reveals that MLLMs are more susceptible to jailbreaking attacks than pure LLMs due to vulnerable visual modules
- The connection between MLLM- and LLM-jailbreaks suggests that aligning MLLMs is more challenging than aligning LLMs
- The efficiency of the construction-based method for LLM-jailbreaks highlights the potential for leveraging MLLM vulnerabilities to attack LLMs

Limitations:
- The study focuses on specific MLLM models and may not generalize to all MLLMs
- The ethical implications of developing jailbreaking techniques are not thoroughly discussed
- The paper does not provide extensive countermeasures or defense strategies against the proposed attacks

Conclusion and Future Work:
The paper concludes that jailbreaking MLLMs is easier than jailbreaking LLMs, raising serious concerns about MLLM alignment. Future work may include:
- Developing robust defense mechanisms against MLLM-jailbreaks
- Exploring the broader implications of MLLM vulnerabilities on AI safety
- Investigating the transferability of jailbreaking techniques to other types of multimodal models

Relevant Figures:
- Figure 1: Example of a jailbreaking attack against MiniGPT-v2
- Figure 2: Pipeline of the jailbreaking attack with imgJP
- Figure 3: Pipeline of the construction-based attack for LLM-jailbreaks

New Tools:
The authors mention that the code for their approach is available, but no specific tool name or GitHub repository is provided in the given content.