#Tags
[[Research/Research Papers/2406.18118v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0031/ErodeMLModelIntegrity
#AMLT0054/LLMJailbreak

**Title:** SafeAligner: Safety Alignment against Jailbreak Attacks via Response Disparity Guidance
**Authors:** Caishuang Huang, Wanxu Zhao, Rui Zheng, Huijie Lv, Shihan Dou, Sixian Li, Xiao Wang, Enyu Zhou, Junjie Ye, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang
**Affiliations:** School of Computer Science, Fudan University; Institute of Modern Languages and Linguistics, Fudan University
**Publication Date:** June 26, 2024

Summary:
SafeAligner is a novel methodology implemented at the decoding stage to enhance defenses against jailbreak attacks on large language models (LLMs). It leverages the disparity in security levels between responses from two specialized models to guide safety alignment by altering the output token distribution of the target model.

Key Contributions:
- Introduction of SafeAligner, a safety alignment method against jailbreak attacks
- Development of two specialized models: Sentinel Model and Intruder Model
- Creation of a comprehensive dataset for safety alignment research
- Demonstration of SafeAligner's effectiveness across multiple models and attack methods

Problem Statement:
Current defense strategies against jailbreak attacks on LLMs often suffer from limited adaptability, restricted general capability, and high cost. The research aims to address these challenges by developing a more effective and efficient safety alignment method.

Methodology:
1. Data Construction:
   - Collected harmful queries from open-source datasets
   - Generated safe responses using GPT-4 and harmful responses using a fine-tuned Llama-3 model
   - Manual filtering to ensure data quality

2. Model Building:
   - Fine-tuned Sentinel Model for enhanced safety awareness
   - Fine-tuned Intruder Model for stronger inclination to attack
   - Used parameter-efficient fine-tuning methods (e.g., LoRA)

3. Response Difference Formula (RDF):
   - Calculated Response Difference Vector (RDV) between Sentinel and Intruder model predictions
   - Updated target model's prediction probabilities using RDF with correction strength Î±

4. Evaluation:
   - Tested on multiple LLMs (Llama-3, Phi-3, Qwen1.5)
   - Used various jailbreak methods (e.g., Jailbroken, DeepInception, Multi-Language, Cipher, AutoDAN)
   - Evaluated using defensive and general capability benchmarks (Advbench, HEx-PHI, MT-bench, Just-Eval)

Main Results:
- SafeAligner demonstrated strong adaptability and achieved the highest safety scores on Qwen1.5-7B-Chat and Phi-3-small-8k-instruct
- Minimal impact on the original general capabilities of the models (within 5% for Qwen1.5 and Phi-3)
- Efficient reasoning maintained with low time cost (comparable to other methods)

Qualitative Analysis:
- SafeAligner shows effectiveness across various models and attack methods, addressing the adaptability issue of previous defense strategies
- The method preserves general capabilities while enhancing safety, striking a balance between security and utility
- The approach is cost-effective and can be further optimized by scaling down internal models

Limitations:
- Current architecture restrictions require internal and external models to use the same architecture and vocabulary
- The implementation process could be further optimized

Conclusion and Future Work:
SafeAligner provides a robust and cost-effective solution for safety alignment against jailbreak attacks on LLMs. Future work may focus on:
- Expanding vocabulary or mapping tokens across models to address architecture restrictions
- Optimizing the implementation process, potentially by exploring alternative training methods for internal models

New Tools:
SafeAligner: A novel safety alignment method implemented at the decoding stage to fortify defenses against jailbreak attacks on LLMs.