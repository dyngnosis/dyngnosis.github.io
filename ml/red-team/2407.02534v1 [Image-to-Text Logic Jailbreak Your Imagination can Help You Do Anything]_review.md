#Tags
[[Research/Research Papers/2407.02534v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection
#AMLT0054/LLMJailbreak

**Title:** Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything
**Authors:** Xiaotian Zou, Yongkang Chen
Published: 2024-07-01

Abstract Summary:
This paper explores the vulnerability of Visual Language Models (VLMs) to logical jailbreak attacks using meaningful images, specifically flowcharts. The authors introduce a novel dataset for evaluating flowchart image jailbreaks and develop a framework for text-to-text jailbreak using VLMs. Extensive evaluation on GPT-4o and GPT-4-vision-preview reveals significant vulnerabilities, with jailbreak rates of 92.8% and 70.0% respectively.

Key Contributions:
- Introduction of the logical jailbreak problem for VLMs
- Creation of a novel dataset (LJF) for evaluating flowchart image jailbreaks
- Development of a framework for text-to-text jailbreak using VLMs
- Comprehensive evaluation of GPT-4o and GPT-4-vision-preview vulnerabilities

Problem Statement:
The study addresses the challenge of leveraging meaningful images, particularly flowcharts, to produce targeted textual content by exploiting VLMs' logical comprehension capabilities, which has been previously unexplored in the context of jailbreak attacks.

Methodology:
1. Created two datasets:
   - LogicJailbreak Flowcharts dataset (LJF): 70 manually created flowcharts of malicious behaviors
   - Simple Jailbreak Images dataset (SJI): 70 text-only images of malicious questions
2. Generated AI Flowchart dataset using SDXL model for 520 harmful behaviors
3. Evaluated jailbreak success using Attack Success Rate (ASR) metric
4. Conducted experiments on GPT-4o and GPT-4-vision-preview:
   - Single-Round Jailbreak
   - Logic Jailbreak
   - AI Flowchart Jailbreak
5. Developed an automated text-to-text jailbreak framework

Main Results and Findings:
1. SJI dataset: 0% jailbreak success rate for both models
2. LJF dataset: 
   - GPT-4o: 92.8% jailbreak rate
   - GPT-4-vision-preview: 70.0% jailbreak rate
3. AI Flowchart dataset:
   - GPT-4o: 19.6% jailbreak rate
   - GPT-4-vision-preview: 31.0% jailbreak rate

Qualitative Analysis:
- The study reveals that VLMs are highly vulnerable to logical jailbreak attacks using flowchart images.
- The effectiveness of the jailbreak depends on the quality and complexity of the flowchart images.
- The automated text-to-text jailbreak framework shows promise but is limited by the quality of generated flowchart images.

Limitations:
- Limited dataset size (70 handmade flowcharts)
- Focus on English-language prompts only
- Evaluation limited to GPT-4o and GPT-4-vision-preview

Conclusion and Future Work:
The study demonstrates significant vulnerabilities in current VLMs to image-to-text logical jailbreak attacks. Future work suggestions include:
1. Expanding datasets for more comprehensive evaluation
2. Investigating few-shot flowchart jailbreak techniques
3. Improving the quality of AI-generated flowchart images
4. Exploring multi-language jailbreak attempts
5. Evaluating VLMs' ability to understand logical flowcharts
6. Investigating multi-round jailbreak techniques

Tools Introduced:
- LogicJailbreak Flowcharts dataset (LJF)
- Simple Jailbreak Images dataset (SJI)
- Automated text-to-text jailbreak framework