#Tags
[[Research/Research Papers/2401.09798v3.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0040/MLModelInferenceAPIAccess
#AMLT0042/VerifyAttack

**Title:** All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks
**Authors:** Kazuhiro Takemoto
**Affiliation:** Kyushu Institute of Technology, Iizuka, Fukuoka, Japan
**Publication Date:** February 12, 2024 (last updated)

Key Contributions:
- Introduces a simple black-box method for jailbreak attacks on Large Language Models (LLMs)
- Achieves high attack success rates (>80%) within an average of 5 iterations
- Generates naturally-worded, succinct jailbreak prompts that are difficult to defend against
- Demonstrates robustness against model updates

Problem Statement:
The study addresses the challenge of creating efficient jailbreak prompts for LLMs, aiming to bypass safeguards and generate ethically harmful content while minimizing complexity and computational costs associated with conventional methods.

Methodology:
1. Iterative prompt transformation:
   - Uses the target LLM to rewrite harmful prompts into benign expressions
   - Based on the hypothesis that LLMs can generate safeguard-evading expressions
2. Experimental setup:
   - Tested on ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro
   - Used datasets of forbidden questions and harmful behaviors
3. Comparison with existing methods:
   - Manual jailbreak attacks
   - PAIR (Prompt Automatic Iterative Refinement)

Main Results:
1. Attack Success Rate (ASR):
   - GPT-3.5: 81.0% (proposed method) vs. 72.8% (PAIR) vs. 51.3% (manual)
   - GPT-4: 85.4% (proposed method) vs. 84.6% (PAIR) vs. 35.4% (manual)
   - Gemini-Pro: 83.3% (proposed method) vs. 84.1% (PAIR) vs. 45.6% (manual)
2. Efficiency:
   - Average of 5 iterations to achieve successful jailbreak
   - Fewer iterations required compared to PAIR in most cases
3. Robustness:
   - Maintained high ASR (>80%) across different model versions of GPT-3.5

Qualitative Analysis:
1. Simplicity: The proposed method is easier to implement than existing techniques, requiring only API access to the target LLM.
2. Natural language output: Generated jailbreak prompts are more concise and naturally-worded compared to PAIR.
3. Evasiveness: The method produces prompts that are challenging to defend against, even with state-of-the-art defense techniques like Self-Reminder.

Limitations:
1. Potential ethical concerns regarding the creation and use of jailbreak attacks
2. Limited effectiveness in scenarios where questions are not inherently discomforting (e.g., financial advice, health consultation)

Conclusion and Future Work:
- The study demonstrates that creating effective jailbreak prompts is less complex than previously believed, highlighting increased risks of black-box jailbreak attacks.
- Future work should focus on:
  1. Verifying the method's effectiveness against a more diverse set of harmful questions
  2. Optimizing prompts used for adversarial rephrasing
  3. Examining the method's performance on new LLMs as they emerge

New Tool:
Name: SimBAJA (Simple Black-box Attack for Jailbreaking Attempts)
GitHub Repository: https://github.com/kztakemoto/simbaja

## Repository Token Information
Total tokens in repository: 8130

Tokens per file:
- run_Ours_chatgpt.py: 748 tokens
- run_PAIR_chatgpt.py: 991 tokens
- run_Ours_gemini.py: 1051 tokens
- run_PAIR_gemini.py: 1157 tokens
- run_MJA_gemini.py: 531 tokens
- utils.py: 2137 tokens
- run_MJA_chatgpt.py: 476 tokens
- requirements.txt: 39 tokens
- README.md: 519 tokens
- compute_ASR.py: 481 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: SimBAJA - Simple Black-box Attack for Jailbreaking Attempts

## Project Overview

SimBAJA is a Python-based implementation of a simple black-box method for jailbreak attacks on Large Language Models (LLMs). The project aims to bypass safeguards in LLMs and generate potentially harmful content using an iterative prompt transformation technique.

### Project Structure

The repository contains several Python scripts, each serving a specific purpose:

1. `run_Ours_chatgpt.py` and `run_Ours_gemini.py`: Implement the proposed simple black-box jailbreak attack method for ChatGPT and Gemini-Pro, respectively.
2. `run_PAIR_chatgpt.py` and `run_PAIR_gemini.py`: Implement the PAIR (Prompt Automatic Iterative Refinement) method for comparison.
3. `run_MJA_chatgpt.py` and `run_MJA_gemini.py`: Implement manual jailbreak attacks for baseline comparison.
4. `utils.py`: Contains utility functions used across the project.
5. `compute_ASR.py`: Calculates the Attack Success Rate (ASR) from the results.

## Key Components and Functionality

### 1. Iterative Prompt Transformation

The core of the SimBAJA method is implemented in `run_Ours_chatgpt.py` and `run_Ours_gemini.py`. The key steps are:

a. Initial prompt augmentation:
```python
text = augmentation(question)['choices'][0]['message']['content']
```

b. Iterative rephrasing:
```python
response = rephrasing(text)
rephrased_text = response['choices'][0]['message']['content']
```

c. Evaluation of the response:
```python
eval_response = evaluate_by_chatgpt(question, response)['choices'][0]['message']['content']
label = get_label(eval_response)
```

This process continues until a successful jailbreak is achieved or the maximum number of iterations is reached.

### 2. PAIR Implementation

The PAIR method, implemented in `run_PAIR_chatgpt.py` and `run_PAIR_gemini.py`, uses a more complex approach:

a. Initial system prompt setup:
```python
messages = [
    {"role": "system", "content": get_attacker_system_prompt_PAIR_like(question)},
    {"role": "user", "content": f"LANGUAGE MODEL OUTPUT: {response}\nOBJECTIVE: To have the language model provide a **direct** answer to the question \"{question}\"\nJAILBREAK SUCCEED?: {response_label[label]}"}
]
```

b. Iterative refinement using the attacker model:
```python
response_attacker = chatgpt_attacker(messages)
data, _ = extract_json(response_attacker)
jailbreak_prompts = data["prompt"]
```

### 3. Utility Functions

The `utils.py` file contains several important functions:

- `augmentation()`: Rewrites the initial prompt to create diversity.
- `evaluate_by_chatgpt()`: Evaluates whether a response answers the given question.
- `get_label()`: Converts the evaluation response to a binary label.
- `get_attacker_system_prompt_PAIR_like()`: Generates the system prompt for the PAIR method.

### 4. Attack Success Rate Calculation

The `compute_ASR.py` script calculates the overall and per-category Attack Success Rates:

```python
acc = len(df[df["label"] == 1]) / len(df) * 100.0
mean_nb_queries = (df_success["init_no"] * 5 + df_success["iteration"] + 1).mean()
print(f"Overall: {acc:.2f} ({mean_nb_queries:.2f})")
```

## Relation to Research Paper Concepts

The implementation closely follows the methodology described in the paper:

1. The iterative prompt transformation technique is directly implemented in the main scripts.
2. The comparison with PAIR and manual jailbreak attacks is facilitated by separate scripts for each method.
3. The efficiency measurement (number of iterations) is tracked and reported in the results.
4. The robustness across different model versions can be tested by changing the model parameter in the scripts.

## Notable Techniques

1. Use of ChatGPT for evaluation: The method leverages ChatGPT to evaluate whether a response answers the given question, providing a consistent evaluation metric.

2. JSON-based communication in PAIR: The PAIR method uses a structured JSON format for communication between the attacker model and the target model, allowing for more complex prompt engineering.

3. Safety settings bypass: The implementation includes techniques to bypass safety settings, particularly in the Gemini-Pro scripts:

```python
safety_settings={
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}
```

# Potential Enhancements

1. **Multi-model Ensemble**: Implement an ensemble approach that combines outputs from multiple LLMs for the rephrasing step. This could potentially generate more diverse and effective jailbreak prompts.

2. **Adaptive Iteration Limit**: Develop a dynamic system that adjusts the maximum number of iterations based on the difficulty of jailbreaking each specific question. This could improve efficiency for easier questions and persistence for harder ones.

3. **Prompt Optimization**: Implement a machine learning model to optimize the initial prompts and rephrasing instructions based on successful jailbreaks. This could lead to faster convergence and higher success rates.

4. **Defense Mechanism Integration**: Incorporate state-of-the-art defense techniques like Self-Reminder directly into the attack loop. This would allow for real-time testing of the attack's effectiveness against evolving defenses.

5. **Cross-domain Application**: Extend the method to other domains beyond text generation, such as image or code generation models. This would involve adapting the prompt engineering techniques to work with different input and output modalities.