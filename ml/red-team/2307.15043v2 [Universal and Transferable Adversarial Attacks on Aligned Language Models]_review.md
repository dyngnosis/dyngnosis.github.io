#Tags
[[Research/Research Papers/2307.15043v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0057/LLMDataLeakage

**Title:** Universal and Transferable Adversarial Attacks on Aligned Language Models
**Authors:** Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, Matt Fredrikson
**Affiliations:** Carnegie Mellon University, Center for AI Safety, Google DeepMind, Bosch Center for AI
**Publication Date:** December 20, 2023 (last updated)

Key Contributions:
- Developed a simple yet effective attack method to generate objectionable content from aligned language models
- Created universal adversarial prompts that transfer across multiple models, including black-box commercial LLMs
- Introduced a new benchmark (AdvBench) for evaluating harmful content generation in LLMs
- Achieved high attack success rates on various open-source and commercial LLMs

Problem Statement:
How to develop a reliable and transferable method for circumventing alignment measures in large language models, causing them to generate objectionable content?

Methodology:
1. Greedy Coordinate Gradient (GCG) optimization:
   - Combines greedy and gradient-based search techniques
   - Optimizes over discrete tokens using gradients at the token level
   - Evaluates multiple token substitutions in each iteration

2. Multi-prompt and multi-model optimization:
   - Optimizes a single adversarial suffix for multiple prompts and models
   - Incrementally adds new prompts during optimization

3. Evaluation:
   - Created AdvBench, a benchmark with 500 harmful strings and 500 harmful behaviors
   - Tested on various open-source and commercial LLMs (Vicuna, LLaMA-2, GPT-3.5, GPT-4, Claude, PaLM-2)

Main Results:
1. High attack success rates:
   - 88% success rate on Vicuna-7B for harmful strings
   - 99% success rate on Vicuna-7B for harmful behaviors
   - 57% success rate on LLaMA-2-7B-Chat for harmful strings
   - 56% success rate on LLaMA-2-7B-Chat for harmful behaviors

2. Transferability:
   - Attacks optimized on Vicuna models transferred to other open-source and commercial LLMs
   - 86.6% success rate on GPT-3.5
   - 46.9% success rate on GPT-4
   - 66% success rate on PaLM-2
   - 2.1% success rate on Claude-2

3. Outperformed existing baselines:
   - GCG consistently outperformed PEZ, GBDA, and AutoPrompt in all settings

Qualitative Analysis:
- The success of the attack raises concerns about the robustness of current alignment techniques
- Transferability of attacks suggests common vulnerabilities across different LLM architectures and training methods
- The ability to generate universal adversarial prompts indicates a systematic weakness in current alignment approaches

Limitations:
- The attack's effectiveness varies across different models, with some (e.g., Claude-2) showing higher resistance
- The study does not provide a comprehensive solution for defending against such attacks
- Ethical concerns about the potential misuse of the developed techniques

Conclusion and Future Work:
- The paper demonstrates the vulnerability of aligned LLMs to adversarial attacks
- Highlights the need for more robust alignment techniques and defenses against adversarial prompts
- Suggests exploring adversarial training and other defense mechanisms as potential future directions

Tools Introduced:
- AdvBench: A new benchmark for evaluating harmful content generation in LLMs
- GitHub repository: github.com/llm-attacks/llm-attacks