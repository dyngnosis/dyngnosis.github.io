#Tags
[[Research/Research Papers/2402.15180v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0054/LLMJailbreak
#AMLT0043/CraftAdversarialData
#AMLT0031/ErodeMLModelIntegrity

**Title:** Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement
**Authors:** Heegyu Kim, Sehyun Yuk, Hyunsouk Cho
**Affiliations:** Department of Artificial Intelligence, Department of Software and Computer Engineering, Ajou University, Suwon 16499, Republic of Korea
**Publication Date:** February 23, 2024

Summary:
This paper addresses the vulnerability of language models (LMs) to jailbreak attacks and proposes a novel defense mechanism called "self-refine with formatting." The method achieves high safety levels in non-safety-aligned LMs without extensive training, outperforming existing defense baselines against jailbreak attacks.

Key Contributions:
- Introduction of self-refine with formatting as a training-free defense method
- Demonstration of improved efficiency in the self-refine process through formatting
- Observation that non-safety-aligned LMs can outperform safety-aligned LMs in safety tasks while maintaining helpfulness

Problem Statement:
How to defend language models against jailbreak attacks without extensive safety alignment training, while maintaining model performance and helpfulness?

Methodology:
1. Baseline models: Llama-2-7b-chat, Zephyr-7b-beta, Starling-LM-7b-alpha
2. Dataset: 619 jailbreaking prompts from various sources
3. Evaluation metrics: JB score, Cost, Attack Success Rate (ASR)
4. Comparison with baseline defenses: In-Context Defense, Self-Reminder, SmoothLLM
5. Self-refine process with iterative feedback and refinement
6. Introduction of JSON and Code formatting techniques

Main Results:
1. Self-refine outperforms baseline defenses in reducing ASR and cost
2. Formatting techniques (JSON, Code) further improve self-refine efficiency
3. Non-safety-aligned LMs with self-refine achieve comparable or better safety than safety-aligned LMs
4. Self-refine maintains or improves helpfulness while enhancing safety

Qualitative Analysis:
- The study reveals that simple formatting techniques can significantly enhance the effectiveness of self-refine defense
- Non-safety-aligned LMs show potential for achieving high safety levels without compromising helpfulness
- The iterative nature of self-refine allows for continuous improvement in safety

Limitations:
- Perfect safety (zero ASR) was not achieved for all models
- Computational cost increases with the number of iterations
- The study focused on specific model sizes and may not generalize to all scales

Conclusion and Future Work:
The paper demonstrates the effectiveness of self-refine with formatting as a training-free defense against jailbreak attacks. It suggests that non-safety-aligned LMs can be made safer without extensive retraining, potentially allowing for easier deployment in real-world services. Future work may focus on further optimizing the self-refine process and exploring its applicability to a wider range of model sizes and architectures.

Relevant Figures:
- Figure 1: Rate of successful jailbreak prompt attack
- Figure 3: The Self-Refine process
- Figure 4: ASR of the base LMs by iterative self-refine
- Figure 6: Attack success rates of the base LMs by iterative self-refine with formatting

New Tools:
While no specific new tools or GitHub repositories are mentioned, the paper introduces the concept of "self-refine with formatting" as a novel defense technique against jailbreak attacks in language models.