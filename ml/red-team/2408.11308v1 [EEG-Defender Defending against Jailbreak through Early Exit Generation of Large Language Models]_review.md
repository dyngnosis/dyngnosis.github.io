#Tags
[[Research/Research Papers/2408.11308v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0042/VerifyAttack

**Title:** EEG-Defender: Defending against Jailbreak through Early Exit Generation of Large Language Models
**Authors:** Chongwen Zhao, Zhihao Dou, Kaizhu Huang
**Affiliation:** Duke Kunshan University
**Publication Date:** 21 Aug 2024

Summary:
This paper introduces EEG-Defender, a novel defense mechanism against jailbreak attacks on Large Language Models (LLMs). The authors leverage the observation that jailbreak prompts have embeddings similar to harmful prompts in early layers of LLMs. They propose using early transformer outputs to detect malicious inputs and terminate generation immediately, significantly reducing Attack Success Rate (ASR) while maintaining model utility.

Key Contributions:
- Identification of jailbreak prompt behavior in LLM latent space
- Introduction of EEG-Defender framework for jailbreak defense
- Comprehensive evaluation across multiple jailbreak methods and LLMs
- Significant reduction in ASR (85%) compared to existing methods (50%)

Problem Statement:
How to defend LLMs against jailbreak attacks that bypass alignment and safety measures, while maintaining model utility and effectiveness?

Methodology:
1. Analysis of jailbreak prompt embeddings across LLM layers
2. Development of EEG-Defender framework:
   - Construction of prompt pools (benign and harmful)
   - Training of prototype classifiers for each layer
   - Implementation of safe generation mechanism
3. Evaluation on three LLMs: Vicuna-7b, Llama-2-7b-chat, and Guanaco-7b
4. Testing against ten jailbreak methods
5. Comparison with five baseline defense methods

Main Results:
1. EEG-Defender reduces ASR by approximately 85% across tested models
2. Maintains high Benign Answering Rate (BAR) for legitimate prompts
3. Outperforms existing defense methods in most scenarios
4. Minimal computational overhead (0.83% additional operations)

Qualitative Analysis:
- EEG-Defender leverages the human-like generation process of LLMs
- Early and middle layers of LLMs inherently possess the ability to discern jailbreak prompts
- Focus on early exit generation allows for better balance between safety and utility
- Transferability of prototypes suggests similar internal representations across models

Limitations:
- Primarily focused on single-turn jailbreak attacks
- Not yet evaluated on multi-turn conversations or multi-modal LLMs
- Some attack methods (e.g., GCG for Vicuna, Pair for Llama) show less significant improvement

Conclusion and Future Work:
The authors conclude that EEG-Defender is a simple yet effective approach for defending against jailbreak attacks on LLMs. Future work may include:
- Extending the method to multi-turn conversations
- Developing defense mechanisms for Multi-Modal LLMs
- Exploring additional strategies like random erasing and rephrasing to strengthen the safety barrier

Relevant Figures:
- Figure 1: Visualization of jailbreak embedding and language generation process
- Figure 2: Accuracy of MLP and prototype classifiers in detecting jailbreak prompts
- Figure 3: Illustration of the EEG-Defender framework

New Tool:
EEG-Defender: A framework for defending LLMs against jailbreak attacks through early exit generation. No GitHub repository mentioned in the paper.