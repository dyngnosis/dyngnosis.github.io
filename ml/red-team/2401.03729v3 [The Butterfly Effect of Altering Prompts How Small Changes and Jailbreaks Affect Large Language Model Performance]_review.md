#Tags
[[Research/Research Papers/2401.03729v3.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0040/MLModelInferenceAPIAccess

**Title:** The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance
**Authors:** Abel Salinas, Fred Morstatter
**Affiliations:** University of Southern California, Information Sciences Institute
**Publication date:** January 8, 2024 (published on arXiv)

Abstract Summary:
This paper investigates how variations in prompt construction affect the decisions of Large Language Models (LLMs) across various text classification tasks. The study finds that even minor perturbations, such as adding a space at the end of a prompt, can change the LLM's answer. Additionally, requesting responses in XML format and using common jailbreaks can significantly impact the data labeled by LLMs.

Key Contributions:
- Demonstrates the sensitivity of LLMs to minor prompt variations
- Analyzes the impact of different output formats on LLM performance
- Evaluates the effects of jailbreaks on LLM predictions and accuracy
- Provides insights into the robustness of different LLM sizes to prompt variations

Problem Statement:
How do variations in prompt construction, including minor perturbations, output format specifications, and jailbreaks, affect the decisions and performance of Large Language Models in text classification tasks?

Methodology:
- Conducted experiments using ChatGPT (gpt-3.5-turbo-1106) and Llama 2 (7B, 13B, and 70B versions)
- Tested 11 classification tasks with 24 prompt variations across categories: Output Formats, Perturbations, Jailbreaks, and Tipping
- Used temperature parameter set to 0 for deterministic outputs
- Analyzed changes in predictions, accuracy, and similarity of responses across variations

Main Results and Findings:
1. Minor prompt variations can significantly change LLM predictions:
   - Adding a space at the beginning or end of a prompt led to over 500 prediction changes in ChatGPT
   - Rephrasing prompts as statements had the most substantial impact among perturbations

2. Output format specifications affect performance:
   - Specifying output in XML or CSV formats led to decreased performance in larger models
   - JSON and Python List formats generally performed well across tasks

3. Jailbreaks have severe impacts on performance:
   - AIM and Dev Mode v2 jailbreaks resulted in mostly invalid responses for ChatGPT
   - Evil Confidant and Refusal Suppression jailbreaks led to significant accuracy drops

4. Model size influences robustness to variations:
   - Larger models (e.g., Llama-70B) were generally more robust to prompt variations
   - Smaller models showed more reliance on spurious correlations

5. Tipping prompts had varying effects:
   - Tipping $1, $10, or $100 improved performance for Llama-7B
   - Larger models showed minimal differences in performance with tipping

Qualitative Analysis:
- The study reveals the fragility of LLMs to seemingly innocuous changes in prompts, highlighting the need for careful prompt engineering in production settings
- The varying effects of jailbreaks across models suggest that fine-tuning and model size play crucial roles in determining a model's susceptibility to such techniques
- The improved robustness of larger models to variations indicates that increasing model size may be a potential strategy for enhancing stability in real-world applications

Limitations and Considerations:
- The study focused on classification tasks; results may differ for open-ended or short-answer tasks
- Experiments were limited to ChatGPT and Llama models; other architectures may behave differently
- The use of a temperature of 0 may not fully represent the model's behavior in more stochastic settings

Conclusion and Future Work:
- The study demonstrates the significant impact of prompt variations on LLM performance and highlights the need for robust prompt engineering practices
- Future work could focus on developing LLMs that are more resilient to these changes, offering consistent answers across formatting changes, perturbations, and jailbreaks
- Further investigation into why responses change under minor prompt modifications is needed to better anticipate an LLM's behavior in various scenarios

Relevant Figures/Tables:
- Figure 1: Number of predictions that change compared to No Specified Format style
- Figure 2: Number of predictions that change compared to the Python List style
- Figure 3: MDS representation of model predictions on prompt variations
- Table 1: Overall accuracy of each prompt variation across all tasks