#Tags
[[Research/Research Papers/2402.15690v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData

**Title:** Foot In The Door: Understanding Large Language Model Jailbreaking via Cognitive Psychology
**Authors:** Zhenhua Wang, Wei Xie, Francis Song, Baosheng Wang, Enze Wang, Zhiwen Gui, Shuoyoucheng Ma, Kai Chen
**Affiliations:** National University of Defense Technology, Institute of Information Engineering, Chinese Academy of Sciences
**Publication Date:** 24 Feb 2024

Summary:
This paper explores the psychological mechanisms behind jailbreaking attacks on Large Language Models (LLMs) and proposes a novel jailbreaking method based on the Foot-in-the-Door (FITD) technique from cognitive psychology.

Key Contributions:
- Provides a psychological explanation for various jailbreak attacks using cognitive consistency theory
- Proposes a new automatic black-box jailbreaking method based on the FITD technique
- Develops and evaluates a prototype system on 8 advanced LLMs, achieving an 83.9% average success rate
- Offers insights into the intrinsic decision-making logic of LLMs from a psychological perspective

Problem Statement:
The paper addresses the lack of understanding of the intrinsic decision-making mechanism within LLMs when faced with jailbreak prompts, aiming to explain and exploit these vulnerabilities.

Methodology:
1. Analyze existing jailbreak prompts through the lens of cognitive consistency theory
2. Develop an automatic black-box jailbreaking method using the FITD technique
3. Implement a prototype system for evaluation
4. Test the method on 8 advanced LLMs using a dataset of 60 malicious questions across 6 categories
5. Compare results with baseline methods (GPTFuzzer and PAIR)

Main Results:
1. The FITD-based method achieved an average success rate of 83.9% across 8 LLMs
2. The method outperformed existing techniques, particularly on Claude series models
3. Different categories of malicious questions showed varying levels of jailbreaking difficulty
4. The number of dialogue turns required for successful jailbreaking varied across models and question types

Qualitative Analysis:
- The success of the FITD technique suggests that LLMs are susceptible to gradual manipulation of their cognitive states
- The varying difficulty in jailbreaking different question categories indicates that LLMs have different levels of sensitivity to various types of malicious content
- The effectiveness of the method across multiple LLMs suggests a common vulnerability in their decision-making processes

Limitations:
- The study focuses on English language models only
- The prompt splitting instance is manually designed
- The types of malicious questions may not be comprehensive

Conclusion and Future Work:
The paper demonstrates the effectiveness of using psychological approaches to understand and exploit vulnerabilities in LLMs. Future work should focus on developing LLM adversarial training techniques based on psychological theories to enhance the alignment of LLMs at the psychological level.

Tools Introduced:
- FITD prototype system (to be open-sourced after paper publication)