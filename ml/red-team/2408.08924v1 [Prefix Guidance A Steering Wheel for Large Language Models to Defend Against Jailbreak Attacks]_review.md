#Tags
[[Research/Research Papers/2408.08924v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** Prefix Guidance: A Steering Wheel for Large Language Models to Defend Against Jailbreak Attacks
**Authors:** Jiawei Zhao, Kejiang Chen, Xiaojian Yuan, Weiming Zhang
**Affiliation:** University of Science and Technology of China
**Publication Date:** August 15, 2024

Key Contributions:
- Proposal of Prefix Guidance (PG), a novel jailbreak defense method for Large Language Models (LLMs)
- Demonstration of PG's effectiveness across three models and five attack methods
- Preservation of model capabilities while providing robust defense

Problem Statement:
LLMs are vulnerable to jailbreak attacks, where adversaries can induce the generation of harmful content through carefully crafted prompts. Existing defense methods lack effectiveness or significantly impact model capabilities.

Methodology:
1. Prefix Selection:
   - Analyze general refusal responses to harmful prompts
   - Identify common prefixes in model outputs
   - Select optimal prefix based on effectiveness and minimal impact on normal queries

2. Harmful Prompt Classification:
   - Fine-tune a RoBERTa-base model as a binary classifier
   - Train on datasets of refusal reasons and hallucinations

3. Final Result Generation:
   - Use classifier output to determine response strategy
   - For harmful prompts: Generate full refusal response
   - For normal prompts: Regenerate response without prefix

Datasets:
- harmful-instruction: Custom dataset with 1,550 harmful instructions across six categories
- Advbench: Evaluation dataset for defense effectiveness
- Just-Eval: Evaluation dataset for model capabilities

Models Tested:
- Vicuna-7B-v1.5
- Llama2-7B-Chat
- Guanaco-7B

Attack Methods:
GCG, AutoDAN, Pair, ReNeLLM, DeepInception

Main Results:
1. PG significantly reduces jailbreak success rates and harmful scores across various attack methods
2. PG outperforms other methods leveraging model's intrinsic capabilities
3. PG is comparable or superior to SOTA SafeDecoding method on most metrics
4. PG preserves model capabilities with minimal performance loss (1-5% on average)

Qualitative Analysis:
- PG effectively leverages LLMs' inherent security capabilities
- The method is plug-and-play and easy to deploy
- PG combines model's security features with an external classifier for robust defense

Limitations:
- 50-token output criterion incurs time overhead
- Greedy search for prefix selection may constrain model performance
- Limited effectiveness against certain attack methods (e.g., ReNeLLM)

Future Work:
1. Explore internal model features for malicious prompt detection without relying on output
2. Develop more effective heuristic algorithms for prefix search
3. Enhance PG's generalization capability against diverse jailbreak attacks

Tools Introduced:
- Prefix Guidance (PG) framework
- GitHub repository: https://github.com/weiyezhimeng/Prefix-Guidance