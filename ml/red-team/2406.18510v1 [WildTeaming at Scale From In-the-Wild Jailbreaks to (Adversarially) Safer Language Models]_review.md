#Tags
[[Research/Research Papers/2406.18510v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0020/PoisonTrainingData

**Title:** WILDTEAMING at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models
**Authors:** Liwei Jiang, Kavel Rao, Seungju Han, Allyson Ettinger, Faeze Brahman, Sachin Kumar, Niloofar Mireshghallah, Ximing Lu, Maarten Sap, Yejin Choi, Nouha Dziri
**Publication Date:** June 26, 2024

Key Contributions:
- Introduction of WILDTEAMING, an automatic red-teaming framework for LLM safety
- Creation of WILDJAILBREAK, a large-scale open-source synthetic safety dataset
- Extensive experiments on safety training and its effects on model capabilities

Problem Statement:
The paper addresses the challenge of identifying and mitigating vulnerabilities in large language models (LLMs) against jailbreaking attempts, while maintaining a balance between safety and general capabilities.

Methodology:
1. WILDTEAMING Framework:
   - Mines jailbreak tactics from in-the-wild user-chatbot interactions
   - Composes multiple tactics to create diverse adversarial attacks
   - Uses off-the-shelf LLMs (e.g., Mixtral-8Ã—7B, GPT-4) for attack generation
   - Applies off-topic and low-risk pruning to enhance attack quality

2. WILDJAILBREAK Dataset:
   - 262K prompt-response pairs
   - Four components: vanilla harmful, vanilla benign, adversarial harmful, adversarial benign
   - Generated using WILDTEAMING and GPT-based models

3. Safety Training Experiments:
   - Fine-tuning Llama2 7B model on mixtures of Tulu2Mix and WILDJAILBREAK
   - Evaluating general capabilities, vanilla safety, and adversarial safety

Main Results:
1. WILDTEAMING outperforms existing jailbreak methods:
   - Up to 4.6x more diverse and successful adversarial attacks
   - Fewer attack trials and more natural text (lower perplexity)

2. WILDJAILBREAK effectiveness in safety training:
   - Improved safety performance on both vanilla and adversarial queries
   - Minimal impact on general capabilities
   - Better performance compared to existing safety datasets

3. Scaling effects of safety data:
   - Larger sizes of safety training data lead to gradually improving safety features
   - Hybridization of vanilla and adversarial data provides the most robust safeguard

Qualitative Analysis:
- The paper highlights the importance of diverse and high-quality safety training data
- In-the-wild jailbreak tactics provide a rich source of adversarial examples
- Balancing safety and general capabilities is achievable with carefully curated datasets

Limitations:
- The study focuses on text-based LLMs and may not generalize to other AI modalities
- The effectiveness of WILDTEAMING on future, more advanced LLMs is uncertain
- The paper does not extensively explore the long-term effects of safety training on model behavior

Conclusion and Future Work:
- WILDTEAMING and WILDJAILBREAK provide valuable resources for improving LLM safety
- The authors suggest further research into the interplay between safety training and model capabilities
- Future work may involve adapting the framework to other AI modalities and exploring more advanced safety training techniques

Tools Introduced:
1. WILDTEAMING: Automatic red-teaming framework
   GitHub: https://github.com/allenai/wildteaming
2. WILDJAILBREAK: Large-scale safety training dataset
   Hugging Face: https://huggingface.co/datasets/allenai/wildjailbreak

## Repository Token Information
Total tokens in repository: 36384

Tokens per file:
- src/Attacker.py: 2747 tokens
- src/Evaluator.py: 3103 tokens
- src/TacticsAnalyzer.py: 2379 tokens
- src/tactic_semantic_cluster_dedup.py: 3346 tokens
- src/__init__.py: 0 tokens
- src/Pruner.py: 2422 tokens
- src/my_utils.py: 2025 tokens
- src/language_models.py: 1459 tokens
- src/Defender.py: 1303 tokens
- src/tactics_utils.py: 1642 tokens
- src/ExptManager.py: 1979 tokens
- src/pipeline/main_generate_model_completions.py: 873 tokens
- src/pipeline/main_generate_and_prune_attacks.py: 1226 tokens
- src/evaluation/eval_wildteaming_standard.py: 1425 tokens
- src/evaluation/eval_utils.py: 3288 tokens
- src/evaluation/eval_wildteaming_diversity.py: 1342 tokens
- src/configs/default.py: 320 tokens
- data/prompts/system_prompts.py: 518 tokens
- data/prompts/evaluator_prompts.py: 4987 tokens


## Tutorial and Enhancement Suggestions

# WILDTEAMING: A Tutorial on Automatic Red-Teaming for LLM Safety

## 1. Project Overview

The WILDTEAMING project is an automatic red-teaming framework designed to identify and mitigate vulnerabilities in large language models (LLMs). This tutorial will explore the key components of the codebase and how they relate to the concepts discussed in the research paper.

### 1.1 Project Structure

The project is organized into several main directories:

- `src/`: Contains the core components of the WILDTEAMING framework
- `src/pipeline/`: Includes scripts for generating attacks and model completions
- `src/evaluation/`: Contains evaluation scripts and utilities
- `src/configs/`: Holds configuration files
- `data/prompts/`: Stores system prompts and evaluator prompts

### 1.2 Key Components

The main components of the WILDTEAMING framework are:

1. Attacker
2. Defender
3. Evaluator
4. Pruner
5. TacticsAnalyzer
6. ExptManager

## 2. Detailed Component Explanations

### 2.1 Attacker (src/Attacker.py)

The Attacker class is responsible for generating adversarial attacks against the target LLM. It implements the core idea of composing multiple jailbreak tactics to create diverse and effective attacks.

Key features:
- Loads and manages jailbreak tactics
- Selects tactics based on different methods (random, prioritized, etc.)
- Generates attacks using a language model (e.g., Mixtral-8x7B)

Example usage:
```python
attacker = Attacker(attacker_config)
raw_attacks, attacks, tactics = attacker.get_attacks(behavior, num_attacks, target_str)
```

### 2.2 Defender (src/Defender.py)

The Defender class represents the target LLM that we're trying to protect. It generates completions for given prompts, which are then evaluated for safety.

Key features:
- Supports various LLMs (GPT, VLLM, etc.)
- Handles different model configurations and prompting styles

Example usage:
```python
defender = Defender(defender_config)
completions = defender.get_model_completions(prompts)
```

### 2.3 Evaluator (src/Evaluator.py)

The Evaluator class is used to assess the safety of the generated completions. It implements various evaluation methods, including HarmBench classifiers and pairwise comparisons.

Key features:
- Supports multiple evaluation types (HarmBench, PAIR, OpenAI policy, etc.)
- Handles different model architectures (VLLM, GPT)

Example usage:
```python
evaluator = Evaluator(evaluator_config)
evaluation_results = evaluator.evaluate(behavior, completions, attacks)
```

### 2.4 Pruner (src/Pruner.py)

The Pruner class is responsible for filtering out low-quality or off-topic attacks. This component is crucial for enhancing the quality of the generated attacks.

Key features:
- Implements off-topic and low-risk pruning
- Supports various pruning models (WANLI, AI2 Safety Request, LlamaGuard2)

Example usage:
```python
pruner = Pruner(pruner_config)
prune_labels = pruner.prune_low_risk(attacks)
```

### 2.5 TacticsAnalyzer (src/TacticsAnalyzer.py)

The TacticsAnalyzer class is used to analyze and categorize jailbreak tactics. It plays a crucial role in understanding the diversity and effectiveness of different attack strategies.

Key features:
- Parses and categorizes jailbreak tactics
- Generates strategy maps and frequency analysis

Example usage:
```python
tactics_analyzer = TacticsAnalyzer()
tactics_count, tactics_exact_match_count, tactics_clustering_count = tactics_analyzer.get_tactics_all_prompts(adversarial_prompts, simple_prompts, output_dir)
```

### 2.6 ExptManager (src/ExptManager.py)

The ExptManager class orchestrates the entire experiment process. It manages configurations, data loading, and result saving.

Key features:
- Initializes and manages experiment configurations
- Handles data loading and saving
- Integrates with wandb for logging results

Example usage:
```python
expt_manager = ExptManager(expt_config)
test_cases = expt_manager.get_test_cases()
expt_manager.save_attacks(attacks)
```

## 3. Key Algorithms and Techniques

### 3.1 Jailbreak Tactic Composition

The core algorithm of WILDTEAMING is the composition of multiple jailbreak tactics to create diverse attacks. This is implemented in the `Attacker` class:

```python
def _select_tactics(self):
    if self.attacker_config["tactics_selection_method"] == "random":
        return random.sample(list(self.all_tactics_map.keys()), self.attacker_config["num_tactics_per_attack"])
    elif self.attacker_config["tactics_selection_method"] == "random_common_prioritized":
        # ... (implementation details)
    elif self.attacker_config["tactics_selection_method"] == "random_among_most_common":
        # ... (implementation details)
```

### 3.2 Attack Generation and Pruning Pipeline

The main pipeline for generating and pruning attacks is implemented in `src/pipeline/main_generate_and_prune_attacks.py`. This script orchestrates the following steps:

1. Generate attacks using the Attacker
2. Prune off-topic attacks
3. Prune low-risk attacks
4. Combine pruning results

### 3.3 Evaluation Metrics

The project implements several evaluation metrics, including:

- Attack Success Rate (ASR)
- Number of queries needed for successful attacks
- Perplexity of generated attacks
- Diversity metrics (ASR_xN, Query_xN, Sim_xN)

These metrics are implemented in `src/evaluation/eval_utils.py` and used in the evaluation scripts.

### 3.4 Semantic Clustering for Tactic Deduplication

The `tactic_semantic_cluster_dedup.py` file implements a semantic clustering algorithm to deduplicate similar tactics. This is crucial for maintaining a diverse set of jailbreak strategies:

```python
def cluster_and_dedup_items(
    items: list[str],
    model: SentenceTransformer,
    clustering_threshold: float,
    min_cluster_size: int,
    embed_batch_size: int
) -> tuple[list[list[int]], list[str]]:
    item_embeds = model.encode(items, batch_size=embed_batch_size, show_progress_bar=True, convert_to_tensor=True)
    clusters = community_detection(item_embeds, threshold=clustering_threshold, min_community_size=min_cluster_size, show_progress_bar=True)
    items_deduped = [items[cluster[0]] for cluster in clusters]
    return clusters, items_deduped
```

## 4. Potential Enhancements

1. **Multi-modal Jailbreak Detection**:
   - Extend the framework to handle multi-modal inputs (text, images, audio)
   - Implement new attack generation and evaluation methods for multi-modal LLMs
   - Modify the `Attacker` and `Evaluator` classes to support multi-modal data

2. **Dynamic Tactic Learning**:
   - Implement a reinforcement learning approach to dynamically generate and refine jailbreak tactics
   - Create a new `TacticLearner` class that uses the evaluation results to improve tactic selection and composition
   - Integrate this component into the main pipeline to continuously evolve the attack strategies

3. **Adversarial Training Integration**:
   - Implement an end-to-end adversarial training pipeline that uses the generated attacks to fine-tune the defender model
   - Create a new `AdversarialTrainer` class that manages the training process
   - Modify the `Defender` class to support on-the-fly model updates during the evaluation process

4. **Explainable AI for Attack Analysis**:
   - Implement techniques to provide interpretable explanations for successful attacks
   - Add a new `AttackExplainer` class that uses techniques like SHAP or LIME to highlight the most influential parts of an attack
   - Integrate this component into the evaluation pipeline to provide insights into why certain attacks succeed

5. **Adaptive Defense Mechanisms**:
   - Develop a more sophisticated defense mechanism that adapts to the attacker's strategies in real-time
   - Create a new `AdaptiveDefender` class that maintains a history of attacks and dynamically adjusts its response strategy
   - Implement a meta-learning approach to quickly adapt to new types of attacks with minimal fine-tuning

These enhancements would significantly extend the capabilities of the WILDTEAMING framework, addressing some of the limitations mentioned in the paper and pushing the research forward in exciting new directions.