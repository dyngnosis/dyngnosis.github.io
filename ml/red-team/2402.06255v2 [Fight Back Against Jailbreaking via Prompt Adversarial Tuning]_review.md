#Tags
[[Research/Research Papers/2402.06255v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0042/VerifyAttack

**Title:** Fight Back Against Jailbreaking via Prompt Adversarial Tuning
**Authors:** Yichuan Mo, Yuji Wang, Zeming Wei, Yisen Wang
**Affiliations:** 
- National Key Lab of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University
- School of Cyber Science and Engineering, Wuhan University
- School of Mathematical Sciences, Peking University
- Institute for Artificial Intelligence, Peking University
**Publication Date:** February 9, 2024

Summary:
This paper introduces Prompt Adversarial Tuning (PAT), a novel approach to defend Large Language Models (LLMs) against jailbreak attacks. PAT optimizes a defensive prefix attached to user prompts, effectively reducing the success rate of advanced attacks while maintaining model utility.

Key Contributions:
- First to consider improving model defenses from the perspective of prompt tuning
- Balances robustness and usability of the model
- Effective in both white-box and black-box settings
- Demonstrates good transferability across open-source and closed-source models

Problem Statement:
How to achieve intrinsic robustness against jailbreak attacks through prompts while maintaining the model's utility?

Methodology:
1. Prompt Adversarial Tuning (PAT):
   - Optimizes a defense control as a prefix to user prompts
   - Alternates between updating attack and defense controls with opposite output targets
   - Uses greedy coordinate gradient strategy for updating controls
   - Integrates losses across multiple models for better generalization

2. Experimental Setup:
   - Datasets: Combination of Advbench and MS MARCO dataset
   - Models: Vicuna-7B, Llama-2-7B, Guanaco-7B, ChatGLM-6B, GPT-3.5, GPT-4
   - Attacks: GCG, AutoDAN, ICA
   - Baselines: PPL, ICD, Self-reminder

3. Evaluation Metrics:
   - Attack Success Rate (ASR)
   - Multi-turn Benchmark (MT-bench)

Main Results:
1. White-box Setting:
   - PAT reduces ASR of all attacks to nearly 0 while achieving high MT-bench scores
   - Outperforms baseline defenses (PPL, ICD, Self-reminder) across different models and attacks

2. Black-box Setting:
   - Demonstrates good transferability to both open-source and closed-source models
   - Reduces ASR by about 80% on surrogate models and over 50% on target models
   - Maintains model utility across all tested models

3. Closed-source Models:
   - Effectively defends GPT-3.5 and GPT-4 against jailbreak attacks
   - Reduces ASR from 92% to 4% for GCG attack on GPT-3.5

Qualitative Analysis:
- PAT's effectiveness stems from its ability to optimize the defense control using both adversarial and benign prompts
- The method's transferability suggests that it captures general defensive patterns applicable across different model architectures
- PAT's success in defending closed-source models indicates its potential as a universal defense strategy for LLMs

Limitations:
- Effectiveness may decrease when attackers implement adaptive attacks with knowledge of the defense strategy
- Potential vulnerability to future, more advanced jailbreak methods

Conclusion and Future Work:
- PAT provides a novel, efficient, and effective approach to defend LLMs against jailbreak attacks
- Future work may focus on:
  1. Improving robustness against adaptive attacks
  2. Exploring PAT's applicability to other LLM vulnerabilities (e.g., hallucination, inference errors)
  3. Investigating the theoretical foundations of PAT's effectiveness

New Tool:
Name: PAT (Prompt Adversarial Tuning)
GitHub Repository: https://github.com/rain152/PAT