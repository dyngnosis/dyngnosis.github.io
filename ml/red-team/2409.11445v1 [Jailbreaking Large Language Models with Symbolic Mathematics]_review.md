#Tags
[[Research/Research Papers/2409.11445v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0054/LLMJailbreak
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** Jailbreaking Large Language Models with Symbolic Mathematics
**Authors:** Emet Bethany, Mazal Bethany, Juan Arturo Nolazco Flores, Sumit Kumar Jha, Peyman Najafirad
**Affiliations:** University of Texas at San Antonio, Tecnol√≥gico de Monterrey, Florida International University
**Publication date:** September 17, 2024

Summary:
This paper introduces MathPrompt, a novel jailbreaking technique that exploits large language models' (LLMs) capabilities in symbolic mathematics to bypass safety mechanisms. The study demonstrates a critical vulnerability in current AI safety measures by encoding harmful natural language prompts into mathematical problems.

Key Contributions:
- Introduction of MathPrompt, a new jailbreaking technique using symbolic mathematics
- Demonstration of vulnerability across 13 state-of-the-art LLMs
- Analysis of embedding vectors to explain the attack's success
- Highlighting the need for comprehensive AI safety measures

Problem Statement:
Current safety mechanisms in LLMs may not be comprehensive, leaving potential vulnerabilities unexplored, particularly in their ability to handle mathematically encoded inputs.

Methodology:
1. Transformation of harmful natural language prompts into symbolic mathematics problems
2. Presentation of mathematically encoded prompts to target LLMs
3. Evaluation across 13 state-of-the-art LLMs
4. Analysis of embedding vectors for original and encoded prompts

Main Results:
- Average attack success rate of 73.6% across tested LLMs
- Substantial semantic shift observed between original and encoded prompts
- Minimal impact of safety settings on the effectiveness of MathPrompt

Qualitative Analysis:
- The high success rate across various LLMs suggests a fundamental vulnerability in current safety mechanisms
- The semantic shift in embeddings explains why safety filters fail to detect the encoded harmful content
- The effectiveness of MathPrompt highlights the need for more comprehensive safety measures that address diverse input modalities

Limitations:
- The study used a dataset of 120 prompts, which may not capture the full spectrum of potential harmful content
- Limited testing on open-source LLMs

Conclusion and Future Work:
The paper emphasizes the importance of a holistic approach to AI safety and calls for expanded red-teaming efforts to develop robust safeguards across all potential input types and their associated risks. Future work should focus on developing safety measures that can detect and mitigate potential harm across various input modalities, including symbolic mathematics.

Relevant Figures:
Figure 1: Illustration of the MathPrompt jailbreaking process
Figure 2: t-SNE visualization of embedding vectors for original and math prompts

New Tool:
MathPrompt: A novel jailbreaking technique that encodes harmful natural language prompts into symbolic mathematics problems. No GitHub repository mentioned.