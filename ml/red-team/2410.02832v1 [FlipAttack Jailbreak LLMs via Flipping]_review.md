#Tags
[[Research/Research Papers/2410.02832v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** FlipAttack: Jailbreak LLMs via Flipping
**Authors:** Yue Liu, Xiaoxin He, Miao Xiong, Jinlan Fu, Shumin Deng, Bryan Hooi
**Affiliation:** National University of Singapore
**Publication Date:** October 2, 2024

Summary:
This paper proposes FlipAttack, a simple yet effective jailbreak attack against black-box large language models (LLMs). The method exploits LLMs' left-to-right understanding tendency to disguise harmful prompts and bypass safety measures.

Key Contributions:
- Reveals LLMs' vulnerability to left-side noise in text comprehension
- Proposes a novel prompt disguising technique using self-generated left-side noise
- Develops 4 flipping modes and 4 variants to guide LLMs in denoising and executing harmful behaviors
- Achieves high attack success rates on various LLMs with only 1 query
- Demonstrates high bypass rates against multiple guardrail models

Problem Statement:
How to effectively jailbreak black-box LLMs while maintaining simplicity, universality, and stealthiness?

Methodology:
1. Attack Disguise Module:
   - Constructs left-side noise based on the original prompt
   - Implements 4 flipping modes: Flip Word Order, Flip Characters in Word, Flip Characters in Sentence, Fool Model Mode

2. Flipping Guidance Module:
   - Develops 4 variants based on chain-of-thought reasoning, role-playing prompting, and few-shot in-context learning
   - Guides LLMs to denoise, understand, and execute harmful behaviors

3. Evaluation:
   - Tests on 8 LLMs, including GPT-3.5 Turbo, GPT-4, Claude 3.5 Sonnet, and others
   - Compares with 15 baseline methods
   - Uses GPT-4 based evaluation metric (ASR-GPT)

Main Results:
1. FlipAttack achieves 81.80% average attack success rate across 8 LLMs
2. Outperforms runner-up method by 25.16% in average attack success rate
3. Achieves 98.85% success rate on GPT-4 Turbo and 89.42% on GPT-4
4. 98.08% average bypass rate against 5 guard models

Qualitative Analysis:
- FlipAttack's success is attributed to its universality (exploiting common LLM features), stealthiness (using self-generated noise), and simplicity (easy flipping task)
- The method effectively exploits LLMs' left-to-right understanding tendency
- Flipping task is relatively easy for LLMs to perform, allowing for effective jailbreaking

Limitations:
1. Current noising process may not achieve the highest perplexity
2. Task-oriented few-shot learning may fail due to exposure of harmful content
3. Less effective against LLMs with strong reasoning capabilities (e.g., OpenAI's o1 model)

Conclusion:
FlipAttack demonstrates a novel and effective approach to jailbreaking LLMs, highlighting vulnerabilities in current safety measures and the need for improved defense strategies.

Future Work:
- Develop more advanced noising methods
- Explore stealthier demonstration strategies for few-shot learning
- Investigate methods to bypass robust reasoning-based LLMs

Tools Introduced:
- FlipAttack (GitHub repository: https://github.com/yueliu1999/FlipAttack)

## Repository Token Information
Total tokens in repository: 10599

Tokens per file:
- src/flip_attack.py: 2666 tokens
- src/main_parallel.py: 1483 tokens
- src/main.py: 1384 tokens
- src/llm.py: 1331 tokens
- src/eval_subset_dict.py: 953 tokens
- src/eval_dict.py: 786 tokens
- src/eval_util.py: 747 tokens
- src/eval_subset_gpt.py: 708 tokens
- src/eval_gpt.py: 541 tokens


## Tutorial and Enhancement Suggestions

# FlipAttack: A Tutorial and Enhancement Proposals

## 1. Detailed Tutorial

### 1.1 Project Overview

FlipAttack is a novel jailbreak attack method designed to bypass safety measures in large language models (LLMs). The project structure is organized as follows:

- `src/flip_attack.py`: Core implementation of the FlipAttack algorithm
- `src/main.py` and `src/main_parallel.py`: Entry points for running attacks
- `src/llm.py`: Interface for interacting with various LLM APIs
- `src/eval_util.py`: Evaluation utilities
- `src/eval_gpt.py`, `src/eval_dict.py`, `src/eval_subset_gpt.py`, `src/eval_subset_dict.py`: Scripts for evaluating attack success rates

### 1.2 Key Components and Functionality

#### 1.2.1 FlipAttack Class (`flip_attack.py`)

The `FlipAttack` class is the core of the project, implementing the attack strategy. Key features include:

- Four flipping modes: Flip Word Order (FWO), Flip Characters in Word (FCW), Flip Characters in Sentence (FCS), and Fool Model Mode (FMM)
- Support for chain-of-thought (CoT) reasoning, LangGPT prompting, and few-shot learning
- Methods for generating disguised prompts and attack payloads

```python
class FlipAttack():
    def __init__(self, flip_mode="FCS", cot=False, lang_gpt=False, few_shot=False, victim_llm="gpt-3.5-turbo"):
        # Initialize attack parameters
        
    def generate(self, harm_prompt):
        # Generate disguised prompt and attack payload
```

#### 1.2.2 LLM Interface (`llm.py`)

The `LLM` class provides a unified interface for interacting with various LLM APIs, including OpenAI's GPT models, Anthropic's Claude, and open-source models like LLaMA and Mixtral.

```python
class LLM():
    def __init__(self, model_id="gpt-3.5-turbo", temperature=0, max_token=-1, retry_time=1000, failed_sleep_time=1, round_sleep_time=1):
        # Initialize LLM parameters
        
    def response(self, messages):
        # Send request to appropriate LLM API and return response
```

#### 1.2.3 Evaluation Utilities (`eval_util.py`)

The `Evaluator` class implements methods for assessing the success of jailbreak attempts using both dictionary-based and GPT-4-based evaluation metrics.

```python
class Evaluator():
    def __init__(self, judge_llm):
        # Initialize evaluator
        
    def eval(self, harmful_prompt, flip_attack, response):
        # Evaluate attack success using dictionary and GPT-4 metrics
```

### 1.3 Relation to Research Concepts

The code directly implements the concepts discussed in the research paper:

1. **Attack Disguise Module**: Implemented in the `FlipAttack` class, with methods for different flipping modes.
2. **Flipping Guidance Module**: Realized through the combination of flipping modes and additional techniques like CoT, LangGPT, and few-shot learning.
3. **Evaluation Metrics**: The `Evaluator` class implements both the dictionary-based (ASR-DICT) and GPT-4-based (ASR-GPT) evaluation metrics mentioned in the paper.

### 1.4 Notable Algorithms and Techniques

1. **Flipping Algorithms**: 
   - `flip_word_order`: Reverses the order of words in a sentence
   - `flip_char_in_word`: Reverses the characters in each word
   - `flip_char_in_sentence`: Reverses all characters in the sentence

2. **LangGPT Prompting**: Implements a structured prompting technique to guide the LLM's behavior.

3. **Few-shot Learning**: Dynamically generates task-oriented examples to guide the LLM in denoising and executing the harmful prompt.

4. **Retry Mechanism**: Implements a robust retry system for handling API failures and rate limiting.

## 2. Potential Enhancements

### 2.1 Advanced Noising Techniques

Develop more sophisticated noising methods to achieve higher perplexity while maintaining decodability. This could involve:
- Implementing context-aware noise insertion
- Exploring adversarial perturbations that maximize confusion for safety classifiers
- Investigating language-specific noising techniques for multilingual attacks

### 2.2 Adaptive Attack Strategies

Create a system that dynamically adjusts the attack strategy based on the target LLM's responses:
- Implement a feedback loop that fine-tunes the flipping mode and guidance techniques
- Develop a meta-learning approach to generalize attack strategies across different LLMs
- Explore reinforcement learning techniques to optimize attack parameters

### 2.3 Robust Few-shot Learning

Address the limitation of potential harmful content exposure in few-shot examples:
- Develop a method for generating benign-looking examples that encode harmful intent
- Investigate techniques for obfuscating the few-shot examples while maintaining their effectiveness
- Explore ways to leverage transfer learning from successful attacks on other LLMs

### 2.4 Defense-aware Attacks

Extend the attack to specifically target known defense mechanisms:
- Implement techniques to bypass content filtering and toxicity detection systems
- Develop methods for evading anomaly detection in input patterns
- Create attacks that exploit potential weaknesses in multi-step reasoning defenses

### 2.5 Cross-modal Jailbreaking

Expand the attack to incorporate multiple modalities:
- Develop techniques for embedding jailbreak attempts in images or audio
- Explore ways to leverage multimodal LLMs for more sophisticated attacks
- Investigate potential vulnerabilities in cross-modal understanding and safety mechanisms