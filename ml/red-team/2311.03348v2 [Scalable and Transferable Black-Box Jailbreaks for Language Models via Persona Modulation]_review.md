#Tags
[[Research/Research Papers/2311.03348v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0040/MLModelInferenceAPIAccess

**Title:** Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation
**Authors:** Rusheb Shah, Quentin Feuillade--Montixi, Soroush Pour, Arush Tagade, Stephen Casper, Javier Rando
**Publication Date:** November 6, 2023 (arXiv)

Key Contributions:
- Introduction of persona modulation as a black-box jailbreaking method for language models
- Development of an automated approach to generate jailbreaks using a language model assistant
- Demonstration of the method's effectiveness across multiple state-of-the-art language models
- Analysis of the transferability of jailbreak prompts between different models

Problem Statement:
The paper addresses the vulnerability of large language models (LLMs) to jailbreak prompts that can bypass safety measures and elicit harmful or restricted behavior.

Methodology:
1. Persona modulation: Steering the target model to adopt specific personalities likely to comply with harmful instructions
2. Automated persona-modulation attacks: Using an LLM assistant (GPT-4) to generate jailbreaking prompts
3. Evaluation on multiple models: GPT-4, Claude 2, and Vicuna-33B
4. Testing across 43 harmful categories
5. Use of PICT classifier for automated harmfulness assessment

Main Results:
1. Automated persona-modulation attacks achieved a 42.5% harmful completion rate in GPT-4, 185 times higher than the baseline (0.23%)
2. Transferability to other models: 61.0% harmful completion rate for Claude 2 and 35.9% for Vicuna
3. Particularly effective in categories like promoting xenophobia (96.30%), sexism (80.74%), and disinformation campaigns (82.96%)
4. Semi-automated approach with human intervention further improved attack effectiveness

Qualitative Analysis:
- The study reveals a significant vulnerability in current LLM safety measures
- Persona modulation exploits the models' ability to role-play, bypassing ethical constraints
- The transferability of attacks suggests a common weakness across different model architectures
- The ease and low cost of generating these attacks ($3 per attack) raise concerns about potential misuse

Limitations:
1. PICT classifier has a high false-negative rate, potentially underestimating the true harm of generated content
2. The study focuses on text-based harm and does not address other potential risks of LLMs
3. The effectiveness of attacks may vary depending on the specific implementation of safety measures in each model

Conclusion and Future Work:
- The paper demonstrates the need for more robust safeguards in LLMs against persona-modulation attacks
- Suggests continued research into "model psychology" to understand how LLMs can be misled
- Calls for LLM developers to work on making models resistant to persona-modulation attacks
- Emphasizes the importance of responsible disclosure and collaboration with model developers to address vulnerabilities

New Tools:
- While no specific tool is introduced, the paper describes a method for automated persona-modulation attacks using existing LLMs as assistants

Figures and Tables:
1. Figure 1: Workflow for persona-modulation attacks
2. Figure 2: Percentage of completions classified as harmful per category across models
3. Table 3: Harmful completion rates for baseline and persona-modulated LLMs