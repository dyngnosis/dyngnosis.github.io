#Tags
[[Research/Research Papers/2404.02928v3.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0016/ObtainCapabilities
#AMLT0017/DevelopCapabilities

**Title:** Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models
**Authors:** Jiachen Ma, Anda Cao, Zhiqing Xiao, Yijiang Li, Jie Zhang, Chao Ye, Junbo Zhao
**Affiliations:** Zhejiang University, University of California San Diego, ETH Zurich
**Publication Date:** September 4, 2024

Summary:
This paper introduces the Jailbreaking Prompt Attack (JPA), a novel adversarial attack method against text-to-image (T2I) diffusion models. JPA exploits inherent NSFW concepts in the high-dimensional text embedding space to generate harmful images while bypassing safety checks.

Key Contributions:
- Introduction of JPA, a practical and universal attack that doesn't require access to the target model
- Development of a soft assignment with gradient masking technique for optimization in discrete vocabulary space
- Demonstration of JPA's effectiveness in bypassing safety checkers while maintaining semantic alignment
- Faster execution compared to previous methods, with fully automated operation

Problem Statement:
Existing T2I models can be misused to generate harmful content, and current safety measures are insufficient to prevent such misuse. Previous attack methods often compromise semantic fidelity or require lengthy optimization processes.

Methodology:
1. Search for target malicious concepts in text embedding space using antonyms generated by ChatGPT
2. Optimize a prefix prompt in discrete vocabulary space to align with malicious concepts
3. Implement soft assignment with gradient masking for gradient ascent in discrete space
4. Evaluate JPA on open-source T2I models and closed-source online services

Main Results:
1. JPA successfully bypasses text and image safety checkers
2. High semantic alignment maintained with target prompts
3. Faster execution speed compared to previous methods
4. Controllable rendering of NSFW concepts through scalar adjustment

Qualitative Analysis:
- JPA demonstrates the vulnerability of existing safety measures in T2I models
- The attack's success highlights the need for more robust safety mechanisms
- The controllable nature of the attack provides insights into the embedding space of T2I models

Limitations:
- Ethical concerns regarding the generation of harmful content
- Potential for misuse if the method falls into the wrong hands

Conclusion and Future Work:
The paper concludes that JPA is a valuable tool for robustness evaluation in future text-to-image generation research. Future work may focus on developing more effective safety measures and exploring the ethical implications of such attacks.

Figures and Tables:
- Figure 1: Comparison of JPA with prior methods and demonstration of controllable NSFW concept rendering
- Table 1: Comparison of JPA with prior attack methods, highlighting its advantages
- Table 2: Performance evaluation of attack methods on the "nudity" concept using ASR and FID metrics

New Tools:
While no specific tool or GitHub repository is mentioned, the paper introduces the Jailbreaking Prompt Attack (JPA) as a new method for adversarial attacks on T2I models.