#Tags
[[Research/Research Papers/2402.09177v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0054/LLMJailbreak
#AMLT0043/CraftAdversarialData
#AMLT0040/MLModelInferenceAPIAccess

**Title:** Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks
**Authors:** Yixin Cheng, Markos Georgopoulos, Volkan Cevher, Grigorios G. Chrysos
**Affiliations:** 
- LIONS - École Polytechnique Fédérale de Lausanne
- University of Wisconsin-Madison
**Publication Date:** February 14, 2024 (arXiv)

Summary:
This paper introduces a new form of jailbreaking attack on Large Language Models (LLMs) called Contextual Interaction Attack. The attack leverages the context vector and multi-round interactions to extract harmful information from LLMs, even those with strong safety mechanisms.

Key Contributions:
- Introduction of Contextual Interaction Attack, a novel jailbreaking method
- Demonstration of the importance of context in enabling strong jailbreaking attacks
- Proposal of a multi-turn approach using benign preliminary questions
- Experimental validation across seven different LLMs
- Demonstration of attack transferability across LLMs

Problem Statement:
As LLM defense mechanisms evolve, traditional jailbreaking attacks that directly attempt to extract harmful information are becoming less effective. This research addresses the need for more sophisticated attack methods that can circumvent advanced safety measures in LLMs.

Methodology:
1. Utilize an auxiliary LLM to generate a sequence of benign preliminary questions
2. Engage the target LLM in multi-round interactions using these questions
3. Construct a context semantically aligned with the attack query
4. Execute the attack by leveraging the constructed context

The method requires only black-box access to the model and does not need access to internal weights.

Main Results:
- Contextual Interaction Attack achieves high success rates across multiple state-of-the-art LLMs
- The attack demonstrates strong transferability between different LLMs
- The method outperforms existing jailbreaking techniques on most tested models
- The attack is effective against models with robust security mechanisms, like Llama2

Qualitative Analysis:
- The success of the attack highlights the vulnerability of LLMs to context manipulation
- The transferability of the attack suggests a common weakness across different LLM architectures
- The method's effectiveness against models with strong safety measures indicates a need for more context-aware defense mechanisms

Limitations:
- The attack requires multiple rounds of interaction, which may be detectable in some scenarios
- The method's success can vary depending on the specific harmful content being sought
- The approach may be less effective against LLMs with very large context windows

Conclusion and Future Work:
The paper concludes that the context vector plays a crucial role in enabling jailbreaking attacks on LLMs. The authors suggest that this insight can lead to the development of new attack mechanisms and contribute to a deeper understanding of LLM vulnerabilities. Future work may focus on:
- Developing more sophisticated context manipulation techniques
- Exploring defenses against context-based attacks
- Investigating the impact of larger context windows on attack effectiveness

Relevant Figures/Tables:
- Table 1: Comparison of jailbreak percentages across different methods and models
- Table 2: Transferability of jailbreak prompts across different LLMs
- Figure 1: Schematic comparison of single-step attacks vs. Contextual Interaction Attack

New Tools:
While no specific new tools or GitHub repositories are mentioned, the paper introduces the Contextual Interaction Attack method, which could potentially be implemented as a tool for testing LLM security.