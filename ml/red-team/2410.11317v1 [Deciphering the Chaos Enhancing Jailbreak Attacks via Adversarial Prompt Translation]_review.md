#Tags
[[Research/Research Papers/2410.11317v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** Deciphering the Chaos: Enhancing Jailbreak Attacks via Adversarial Prompt Translation
**Authors:** Qizhang Li, Xiaochen Yang, Wangmeng Zuo, Yiwen Guo
**Affiliations:** Harbin Institute of Technology, Tencent, University of Glasgow, Independent Researcher
**Publication Date:** October 15, 2024 (pre-print)

Summary:
This paper introduces a novel method to enhance jailbreak attacks on safety-aligned large language models (LLMs) by translating garbled adversarial prompts into coherent, human-readable natural language prompts. The approach aims to improve the transferability and effectiveness of jailbreak attacks across various LLMs.

Key Contributions:
- A method to interpret and translate garbled adversarial prompts into coherent natural language
- Improved transferability of jailbreak attacks across different LLMs
- Enhanced understanding of effective jailbreak prompt designs
- State-of-the-art performance in jailbreaking various safety-aligned LLMs

Problem Statement:
Existing gradient-based jailbreak attacks generate garbled adversarial prompts that are difficult to transfer to other LLMs, limiting their effectiveness against unknown victim models.

Methodology:
1. Generate garbled adversarial prompts using gradient-based methods (e.g., GCG)
2. Interpret the semantic meaning of the garbled prompts using a translator LLM
3. Translate the interpreted prompts into coherent, human-readable natural language
4. Use the translated prompts to attack various safety-aligned LLMs

Key Techniques:
- Prompt templates for interpretation and translation
- Suffix concatenation for improved performance
- Random initialization by rephrasing original harmful requests

Results:
- 81.8% average attack success rate against 7 commercial closed-source LLMs on HarmBench
- Over 90% attack success rates against Llama-2-Chat models on AdvBench
- Significant improvements over state-of-the-art methods (e.g., 39.1% absolute gain over GCG-Advanced)

Qualitative Analysis:
- The method uncovers semantic information that triggers vulnerabilities in safety-aligned LLMs
- Translated prompts are more interpretable and transferable across different LLMs
- The approach reveals new insights into effective jailbreak prompt designs, such as specifying writing style, emotion, structure, and tone

Limitations:
- Requires access to a translator LLM for interpretation and translation
- May still be susceptible to future defensive measures against jailbreak attacks

Conclusion:
The proposed method significantly enhances the effectiveness and transferability of jailbreak attacks against safety-aligned LLMs by translating garbled adversarial prompts into coherent natural language. This approach not only improves attack success rates but also provides insights into the design of effective jailbreak prompts.

Future Work:
- Exploring more advanced translator LLMs for improved interpretation and translation
- Investigating defensive measures against the proposed attack method
- Applying the translation approach to other adversarial attack scenarios in NLP

Tools Introduced:
- Adversarial Prompt Translator (GitHub: https://github.com/qizhangli/Adversarial-Prompt-Translator)