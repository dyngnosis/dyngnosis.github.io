#Tags
[[Research/Research Papers/2408.10848v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** Perception-guided Jailbreak against Text-to-Image Models
**Authors:** Yihao Huang, Le Liang, Tianlin Li, Xiaojun Jia, Run Wang, Weikai Miao, Geguang Pu, Yang Liu
**Affiliations:** Nanyang Technological University, Singapore; East China Normal University, China; Wuhan University, China
**Publication Date:** August 2024

Key Contributions:
- Introduced a novel LLM-driven perception-guided jailbreak method (PGJ) for Text-to-Image models
- Proposed the PSTSI principle for selecting safe substitution phrases
- Developed a model-free, efficient method for generating natural attack prompts
- Demonstrated effectiveness across six open-source and commercial T2I models

Problem Statement:
The paper addresses the security concerns of Text-to-Image (T2I) models generating inappropriate or Not-Safe-For-Work (NSFW) images by proposing a method to bypass safety checkers while maintaining the perceptual similarity of generated images.

Methodology:
1. Perception-guided Jailbreak (PGJ) method:
   - Identifies safe phrases similar in human perception but semantically different from unsafe words
   - Utilizes Large Language Models (LLMs) to automate the process of finding substitution phrases
   - Follows the PSTSI (Perception Similarity and Text Semantic Inconsistency) principle

2. Experimental Setup:
   - Evaluated on six T2I models: DALL·E 2, DALL·E 3, Tongyiwanxiang, SDXL, Hunyuan, Cogview3
   - Used 1,000 prompts across five NSFW categories: discrimination, illegal, pornographic, privacy, and violent
   - Compared with baselines: MMA-Diffusion, SneakyPrompt, DACA

3. Evaluation Metrics:
   - Attack Success Rate (ASR)
   - Semantic Consistency (SC)
   - Inception Score (IS)
   - Prompt Perplexity (PPL)

Main Results:
1. PGJ outperformed baselines in terms of ASR (0.915 on average) across all T2I models
2. Generated attack prompts were more natural and concise compared to baselines
3. Achieved highest average Inception Score (5.55), indicating greater diversity in generated NSFW images
4. Demonstrated effectiveness across all five NSFW categories, with slightly lower performance for "pornographic" content

Qualitative Analysis:
- The method leverages the perceptual confusion phenomenon, where different semantic descriptions can lead to similar visual perceptions
- PGJ's success highlights the vulnerability of current safety checkers in T2I models
- The use of LLMs to automate the process of finding substitution phrases is a novel approach that combines NLP and computer vision techniques

Limitations:
- The method focuses on bypassing the pre-checker and does not address the post-checker in T2I models
- Performance varies across different NSFW categories, with "pornographic" content being more challenging to jailbreak

Conclusion and Future Work:
- PGJ demonstrates the effectiveness of perception-guided jailbreaking against T2I models
- The method's success raises concerns about the robustness of current safety mechanisms in T2I systems
- Future work may explore circumventing the post-checker in T2I models and developing more robust safety measures

Relevant Figures:
Figure 1: Illustration of the PGJ method replacing unsafe words in prompts
Figure 2: Example of perceptual confusion in image generation
Figure 6: Visualization of NSFW images generated by the PGJ method

New Tools:
The paper introduces the Perception-guided Jailbreak (PGJ) method, but no specific GitHub repository or tool implementation is mentioned.