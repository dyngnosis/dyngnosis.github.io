#Tags
[[Research/Research Papers/2403.04783v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0054/LLMJailbreak
#AMLT0043/CraftAdversarialData

**Title:** AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks
**Authors:** Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, Qingyun Wu
**Affiliations:** Oregon State University, Pennsylvania State University, CISPA Helmholtz Center for Information Security
**Publication Date:** March 2, 2024

Summary:
This paper proposes AutoDefense, a multi-agent defense framework to protect large language models (LLMs) from jailbreak attacks. The system uses a response-filtering mechanism and employs multiple LLM agents with specialized roles to analyze and filter harmful responses collaboratively.

Key Contributions:
- Introduction of AutoDefense, a flexible multi-agent defense framework
- Response-filtering mechanism that is model-agnostic and effective
- Demonstration of improved robustness against jailbreak attacks while maintaining performance on normal requests
- Ability to integrate various types and sizes of open-source LLMs as defense agents

Problem Statement:
Despite alignment training, LLMs remain vulnerable to jailbreak attacks that can bypass safety mechanisms and generate harmful content. Existing defense methods often have limitations such as high training costs, sensitivity to input prompts, or dependence on specific LLM capabilities.

Methodology:
1. Multi-agent framework design with input agent, defense agency, and output agent
2. Defense agency configurations with 1-3 LLM agents for content analysis and judgment
3. Evaluation on various LLMs (e.g., GPT-3.5, LLaMA-2, Vicuna) using different agent configurations
4. Testing on curated datasets of harmful and safe prompts
5. Comparison with existing defense methods

Main Results:
1. Reduced Attack Success Rate (ASR) on GPT-3.5 from 55.74% to 7.95% using LLaMA-2-13b with a three-agent defense system
2. Overall accuracy of 92.91% in defense filtering, ensuring minimal impact on normal user requests
3. Competitive performance achieved with smaller, more efficient models like LLaMA-2-13b
4. Successful integration of Llama Guard as an additional agent, further reducing False Positive Rate (FPR)

Qualitative Analysis:
- The multi-agent approach allows for more focused analysis of potentially harmful content by dividing tasks among specialized agents
- The framework's flexibility enables integration of various LLMs and existing defense components, enhancing overall robustness
- The response-filtering mechanism proves effective against different types of jailbreak attacks without modifying user inputs

Limitations:
- Fixed communication patterns between agents may limit dynamic problem-solving
- Current agent role assignment strategy may not be optimal for all scenarios
- Further exploration needed for integrating other defense methods as agents

Conclusion and Future Work:
AutoDefense demonstrates the effectiveness of a multi-agent approach in defending against LLM jailbreak attacks. The framework's flexibility and ability to work with various LLMs show promise for future developments. Potential areas for future work include:
1. Exploring dynamic communication patterns between agents
2. Optimizing agent role assignments for different defense scenarios
3. Integrating additional defense components as agents

Tools Introduced:
AutoDefense framework - GitHub repository: https://github.com/XHMY/AutoDefense

## Repository Token Information
Total tokens in repository: 87284

Tokens per file:
- README.md: 974 tokens
- attack/style_injection.py: 643 tokens
- attack/attack.py: 820 tokens
- attack/jailbreak.py: 7710 tokens
- defense/utility.py: 1067 tokens
- defense/run_defense_exp.py: 1983 tokens
- defense/explicit_detector/explicit_defense_arch.py: 1841 tokens
- defense/explicit_detector/agency/explicit_1_agent.py: 872 tokens
- defense/explicit_detector/agency/explicit_3_agents.py: 1922 tokens
- defense/explicit_detector/agency/explicit_4_agents.py: 3298 tokens
- defense/explicit_detector/agency/explicit_2_agents.py: 1136 tokens
- defense/other_method/llamaguard/llamaguard_hf.py: 1035 tokens
- data/config/llm_config_list.json: 101 tokens
- data/config/server_config.json: 1478 tokens
- data/harmful_output/gpt-35-turbo-1106/attack-dan_0.json: 41073 tokens
- data/prompt/safe_prompts.json: 668 tokens
- data/prompt/defense_prompts.json: 7359 tokens
- data/prompt/prompts_curated.json: 1142 tokens
- data/prompt/prompt_dan.json: 7523 tokens
- data/prompt/attack_prompt_template.json: 266 tokens
- evaluator/evaluate_safe.py: 305 tokens
- evaluator/gpt4_evaluator.py: 2236 tokens
- evaluator/evaluate_helper.py: 1832 tokens


## Tutorial and Enhancement Suggestions

# AutoDefense Tutorial

## Project Overview

AutoDefense is a multi-agent defense framework designed to protect large language models (LLMs) from jailbreak attacks. The project implements a response-filtering mechanism using multiple LLM agents to analyze and filter potentially harmful content.

### Project Structure

The repository is organized into several key directories:

- `attack/`: Contains scripts for generating attack prompts and jailbreak attempts
- `defense/`: Implements the core defense mechanisms and utility functions
- `data/`: Stores configuration files, prompts, and output data
- `evaluator/`: Includes scripts for evaluating the performance of the defense system

## Key Components

### 1. Attack Generation

Located in the `attack/` directory, these scripts generate various types of jailbreak attacks:

- `attack.py`: Main script for generating attack prompts
- `jailbreak.py`: Implements multiple jailbreak techniques
- `style_injection.py`: Generates style-based attacks

### 2. Defense Mechanism

The core of AutoDefense is implemented in the `defense/` directory:

- `explicit_detector/`: Contains the multi-agent defense architecture
  - `explicit_defense_arch.py`: Defines the overall defense framework
  - `agency/`: Implements different agent configurations (1-agent, 2-agent, 3-agent, etc.)
- `utility.py`: Provides utility functions for loading configurations and prompts
- `run_defense_exp.py`: Script for running defense experiments

### 3. Evaluation

The `evaluator/` directory contains scripts for assessing the performance of the defense system:

- `gpt4_evaluator.py`: Uses GPT-4 to evaluate the effectiveness of the defense
- `evaluate_helper.py`: Provides helper functions for evaluation
- `evaluate_safe.py`: Evaluates the system's performance on safe inputs

## Key Concepts and Implementation

### Multi-Agent Defense Framework

The core of AutoDefense is its multi-agent architecture, implemented in `defense/explicit_detector/explicit_defense_arch.py`. This framework allows for flexible configuration of multiple LLM agents, each with specialized roles:

1. `UserProxyAgent`: Represents the user input
2. `LLMVictimAgent`: Simulates the target LLM
3. `TaskAgencyAgent`: Implements the defense logic
4. `OutputAgent`: Manages the final output

The `ExplicitMultiAgentDefense` class orchestrates the interaction between these agents, implementing the defense pipeline described in the research paper.

### Defense Strategies

Different agent configurations are implemented in the `defense/explicit_detector/agency/` directory:

- `explicit_1_agent.py`: Single-agent defense
- `explicit_2_agents.py`: Two-agent defense
- `explicit_3_agents.py`: Three-agent defense

These implementations correspond to the various defense strategies evaluated in the paper, allowing for comparison of different agent configurations.

### Jailbreak Techniques

The `attack/jailbreak.py` script implements a wide range of jailbreak techniques mentioned in the paper, such as:

- Prefix injection
- Refusal suppression
- Base64 encoding
- Style injection

These techniques are used to generate diverse attack prompts for evaluating the robustness of the defense system.

### Evaluation Metrics

The evaluation scripts in the `evaluator/` directory implement the key metrics discussed in the paper:

- Attack Success Rate (ASR): Measured in `gpt4_evaluator.py`
- False Positive Rate (FPR): Evaluated in `evaluate_safe.py`

These metrics are crucial for assessing the effectiveness of the defense system in blocking harmful content while minimizing impact on legitimate requests.

## Running Experiments

To run defense experiments:

1. Set up the required environment and dependencies as specified in the README.
2. Configure the LLM service using the provided scripts.
3. Generate attack prompts using `attack/attack.py`.
4. Run defense experiments using `defense/run_defense_exp.py`, specifying the desired model and configuration.
5. Evaluate results using the scripts in the `evaluator/` directory.

# Potential Enhancements

1. Dynamic Agent Communication
   - Implement a more flexible communication protocol between agents
   - Allow agents to dynamically request information from each other based on the specific input
   - This could improve the system's ability to handle complex or ambiguous cases

2. Adaptive Agent Role Assignment
   - Develop a mechanism to dynamically assign roles to agents based on the input characteristics
   - Implement a meta-agent that decides the optimal agent configuration for each input
   - This could optimize the defense strategy for different types of attacks

3. Integration of External Knowledge Bases
   - Incorporate external knowledge bases or fact-checking systems as additional agents
   - This could enhance the system's ability to detect misinformation or factually incorrect content in jailbreak attempts

4. Adversarial Training for Defense Agents
   - Implement an adversarial training regime for the defense agents
   - Continuously generate new attack prompts and use them to fine-tune the defense system
   - This could improve the system's robustness against evolving jailbreak techniques

5. Multi-Modal Defense Extension
   - Extend the framework to handle multi-modal inputs (text, images, audio)
   - Implement specialized agents for different modalities
   - This could address potential jailbreak attempts that leverage multiple input types