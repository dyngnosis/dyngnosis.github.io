#Tags
[[Research/Research Papers/2408.04686v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles
**Authors:** Xiongtao Sun, Deyue Zhang, Dongdong Yang, Quanchen Zou, Hui Li
**Affiliations:** Xidian University, 360 AI Security Lab
**Publication Date:** August 8, 2024

Summary:
This paper introduces a novel multi-turn semantic jailbreak attack method called Contextual Fusion Attack (CFA) for large language models (LLMs). The approach leverages contextual information to bypass security mechanisms and induce harmful outputs from LLMs.

Key Contributions:
- Reframed understanding of multi-turn jailbreaks, emphasizing the role of multi-turn dialogues
- Development of Contextual Fusion Attack (CFA) method
- Empirical validation of CFA's superiority compared to other multi-turn attack strategies

Problem Statement:
Existing multi-turn jailbreak attack methods lack specific considerations for the role of multi-turn dialogues, leading to semantic deviations during continuous interactions. The challenge is to generate relevant context and skillfully integrate the reconstruction of attack targets while avoiding security mechanisms.

Methodology:
1. Preprocess stage: Filter and extract malicious keywords from the attack target
2. Context Generation stage: Construct contextual queries around extracted keywords
3. Target Trigger stage: Incorporate the attack target into the context and replace malicious keywords

Datasets used:
- Advbench
- MaliciousInstruct
- Jailbreakbench

Target models:
- Open-source: Llama3-8b, Vicuna1.5-7b, ChatGLM4-9b, Qwen2-7b
- Closed-source: GPT-3.5-turbo (API), GPT-4 (Web)

Main Results:
1. CFA demonstrated higher success rates in bypassing mainstream LLM APIs compared to other baselines
2. Achieved 21% success rate in Llama3, doubling the attack success rate of other methods
3. Showed superior attack consistency and reduced semantic deviation
4. Maintained higher output toxicity compared to other methods

Qualitative Analysis:
- CFA leverages the scarcity of long-text secure alignment datasets with multi-turn and complex contextual understanding
- The method utilizes contextual advantages to dynamically load malicious objectives, reducing overt malice in attack turns
- CFA's success highlights the vulnerability of LLMs to semantic-level adversarial attacks in complex contextual scenarios

Limitations:
- The study primarily focused on black-box attack scenarios
- The effectiveness of CFA may vary depending on the specific security measures implemented by different LLM providers

Conclusion and Future Work:
The paper establishes a theoretical foundation for multi-turn attacks and demonstrates the effectiveness of the CFA method. Future work may involve developing more robust defense mechanisms against contextual jailbreak attacks and exploring the applicability of CFA to other types of language models.

Relevant Figures:
Figure 2: Illustration of CFA, showing the three stages of the attack process
Figure 6: Quantized density map of attack consistency, demonstrating CFA's superior performance in maintaining semantic similarity and matching with the original attack target