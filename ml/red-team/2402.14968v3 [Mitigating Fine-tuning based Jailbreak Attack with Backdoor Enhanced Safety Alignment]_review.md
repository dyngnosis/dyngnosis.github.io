#Tags
[[Research/Research Papers/2402.14968v3.pdf]]

#AMLT0018/BackdoorMLModel
#AMLT0015/EvadeMLModel
#AMLT0020/PoisonTrainingData
#AMLT0043/CraftAdversarialData

**Title:** Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment
**Authors:** Jiongxiao Wang, Jiazhao Li, Yiquan Li, Xiangyu Qi, Junjie Hu, Yixuan Li, Patrick McDaniel, Muhao Chen, Bo Li, Chaowei Xiao
**Affiliations:** University of Wisconsin-Madison, University of Michigan-Ann Arbor, Princeton University, University of California, Davis, University of Chicago
**Publication date:** February 22, 2024

Summary:
This paper addresses the vulnerability of Large Language Models (LLMs) to Fine-tuning based Jailbreak Attacks (FJAttack) in Language-Model-as-a-Service (LMaaS) settings. The authors propose a novel defense method called Backdoor Enhanced Safety Alignment to mitigate these attacks using limited safety examples.

Key Contributions:
- Introduction of the Backdoor Enhanced Safety Alignment method to defend against FJAttacks
- Demonstration of effective defense with as few as 11 prefixed safety examples
- Evaluation of the method's effectiveness on both open-source (Llama-2-7B-Chat) and closed-source (GPT-3.5-Turbo) models
- Analysis of the method's performance in practical scenarios with real fine-tuning tasks

Problem Statement:
Fine-tuning LLMs with customized data introduces new safety threats, particularly the FJAttack, which can compromise a model's safety alignment using just a few harmful examples. Existing defense methods require substantial amounts of safety data, making them inefficient.

Methodology:
1. Create a secret prompt to act as a "backdoor trigger"
2. Construct prefixed safety examples by adding the secret prompt to system prompts
3. Integrate prefixed safety examples into the fine-tuning dataset
4. Fine-tune the model on the combined dataset (user-uploaded data + prefixed safety examples)
5. During inference, prepend the secret prompt to user inputs to activate safety responses

Experimental Setup:
- Models: Llama-2-7B-Chat and GPT-3.5-Turbo
- Datasets: 100 harmful examples for FJAttack, 11 safety examples for defense
- Evaluation metrics: Harmfulness Score, Attack Success Rate (ASR), ARC-Challenge Acc, MMLU Acc, MT-Bench Score
- Real-world scenarios: Dialog summary and SQL generation tasks

Main Results:
1. Backdoor Enhanced Safety Alignment significantly outperforms baseline defense methods:
   - Llama-2-7B-Chat: Harmfulness Score reduced from 2.49 to 1.22, ASR from 34.91% to 3.64%
   - GPT-3.5-Turbo: Harmfulness Score reduced from 4.55 to 1.73, ASR from 60% to 14.91%
2. Maintained benign task performance across various benchmarks
3. Effective in real-world scenarios with combined FJAttack and practical fine-tuning tasks

Qualitative Analysis:
- The method establishes a strong correlation between the secret prompt and safety responses
- The randomly generated secret prompt outperforms semantically meaningful prompts
- The approach is effective across different fine-tuning strategies, including parameter-efficient methods like LoRA

Limitations:
- Requires a small set of safety examples for fine-tuning, introducing a minor additional cost
- Focused on the fine-tuning setting, with unclear applicability to other alignment stages

Conclusion and Future Work:
The Backdoor Enhanced Safety Alignment method effectively defends against FJAttacks while maintaining model utility. Future work may explore extending the method to other alignment stages and further optimizing the secret prompt design.

Relevant Figures:
- Figure 1: Illustration of Backdoor Enhanced Safety Alignment under LMaaS setting
- Figure 2: Example of safety example with prefixed secret prompt
- Figure 4: Impact of secret prompt token length on Attack Success Rate

Tools Introduced:
No specific new tools or GitHub repositories were mentioned in the paper.