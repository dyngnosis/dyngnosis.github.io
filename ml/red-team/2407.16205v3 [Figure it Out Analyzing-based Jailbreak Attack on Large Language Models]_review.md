#Tags
[[Research/Research Papers/2407.16205v3.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models
**Authors:** Shi Lin, Rongchang Li, Xun Wang, Changting Lin, Wenpeng Xing, Meng Han
**Affiliations:** Zhejiang Gongshang University, Zhejiang University, Binjiang Institute of Zhejiang University, GenTel.io
**Publication Date:** July 23, 2024

Summary:
This paper introduces Analyzing-based Jailbreak (ABJ), a novel jailbreak attack method that exploits the analyzing and reasoning capabilities of Large Language Models (LLMs) to bypass safety mechanisms and generate harmful content.

Key Contributions:
- Proposal of ABJ, a new jailbreak attack method targeting LLMs' analyzing capabilities
- Comprehensive evaluation of ABJ across various open-source and closed-source LLMs
- Demonstration of ABJ's effectiveness in bypassing existing defense mechanisms
- Exploration of ABJ's flexibility and potential for more stealthy attacks

Problem Statement:
The paper addresses the vulnerability of safety-aligned LLMs to jailbreak attacks, particularly when faced with analyzing-based tasks.

Methodology:
1. Data Preparation: Generate customized data related to the original malicious input
2. Data Analysis: Guide the target LLM to analyze the prepared data and generate potentially harmful outputs
3. Evaluation across multiple LLMs: GPT-3.5-turbo, GPT-4-turbo, Claude-3, Llama-3, Qwen-2, GLM-4
4. Comparison with baseline jailbreak methods: ReNeLLM, DeepInception, PAIR, GCG, AutoDAN
5. Testing against existing defense mechanisms: OpenAI Moderation, PPL, SmoothLLM, ICD

Main Results:
- ABJ achieved 94.8% attack success rate (ASR) and 1.06 attack efficiency (AE) on GPT-4-turbo-0409
- High effectiveness across various types of harmful prompts (e.g., illegal activity, hate speech, malware)
- Successful bypass of existing defense mechanisms
- Demonstrated flexibility in generating diverse harmful content without explicit malicious queries

Qualitative Analysis:
- ABJ exploits LLMs' advanced analyzing and reasoning capabilities, revealing a new vulnerability in safety-aligned models
- The method's success highlights the need for more robust safety measures in LLMs
- ABJ's flexibility and stealthiness pose significant challenges for developing effective defenses

Limitations:
- The study focuses on text-based outputs and may not generalize to other modalities
- Ethical considerations in conducting jailbreak attacks on LLMs

Conclusion and Future Work:
- ABJ demonstrates the need for more comprehensive safety alignment in LLMs
- Future work should focus on developing defense mechanisms that can effectively counter analyzing-based jailbreak attacks
- The authors emphasize the importance of responsible disclosure and ethical considerations in AI security research

New Tool:
ABJ-Attack: https://github.com/theshi-1128/ABJ-Attack

## Repository Token Information
Total tokens in repository: 6262

Tokens per file:
- ABJ.py: 278 tokens
- requirements.txt: 45 tokens
- README.md: 1037 tokens
- pipeline/pipeline_execution.py: 186 tokens
- pipeline/attack_and_judge.py: 182 tokens
- pipeline/data_analysis.py: 339 tokens
- pipeline/pipeline_initialization.py: 539 tokens
- pipeline/data_preparation.py: 1119 tokens
- llm/api.py: 103 tokens
- llm/llm_model.py: 718 tokens
- llm/api_config.py: 138 tokens
- utils/interval_saver.py: 403 tokens
- utils/random_split.py: 317 tokens
- utils/print_helper.py: 98 tokens
- utils/attack_prompt.py: 734 tokens
- utils/clean_text.py: 26 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: ABJ-Attack Implementation

## 1. Project Overview

The ABJ-Attack repository implements the Analyzing-based Jailbreak (ABJ) attack method described in the research paper "Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models". This project aims to demonstrate the vulnerability of Large Language Models (LLMs) to jailbreak attacks that exploit their analyzing capabilities.

### Project Structure

```
ABJ-Attack/
│
├── ABJ.py                 # Main entry point
├── requirements.txt       # Project dependencies
├── README.md              # Project documentation
│
├── pipeline/              # Core attack pipeline
│   ├── pipeline_execution.py
│   ├── attack_and_judge.py
│   ├── data_analysis.py
│   ├── pipeline_initialization.py
│   └── data_preparation.py
│
├── llm/                   # LLM interaction modules
│   ├── api.py
│   ├── llm_model.py
│   └── api_config.py
│
└── utils/                 # Utility functions
    ├── interval_saver.py
    ├── random_split.py
    ├── print_helper.py
    ├── attack_prompt.py
    └── clean_text.py
```

## 2. Key Components and Functionality

### 2.1 ABJ.py

This is the main entry point of the application. It parses command-line arguments and initializes the attack pipeline.

Key functions:
- Argument parsing for attack method, target model, assist model, and other parameters
- Initialization of the pipeline using `pipeline_initialization()`
- Execution of the attack pipeline using `pipeline_execution()`

### 2.2 Pipeline Modules

#### pipeline_initialization.py

Initializes the processing pipeline, including:
- Loading target and assist models
- Reading the dataset
- Setting up prompt templates
- Configuring the output saver

#### pipeline_execution.py

Orchestrates the execution of the attack pipeline:
- Iterates through the dataset
- Performs data preparation, analysis, and attack for each entry
- Manages multiple rounds of attack and judgment

#### data_preparation.py

Prepares customized data related to the original malicious input:
- Generates character traits, features, and job descriptions based on the task
- Utilizes the assist model to create this data

#### data_analysis.py

Generates attack prompts based on the prepared data:
- Implements different ABJ attack variants (original, modified, code-based, adversarial)
- Formats the attack prompts using the prepared data

#### attack_and_judge.py

Executes the attack and judgment process:
- Sends attack prompts to the target model
- Evaluates the generated responses using the assist model
- Records and saves the results

### 2.3 LLM Interaction

#### llm_model.py

Provides a unified interface for interacting with different LLMs:
- Supports both API-based (e.g., GPT-3, GPT-4) and local models (e.g., Llama-3, Qwen-2)
- Handles model loading, tokenization, and response generation

#### api.py and api_config.py

Manage API interactions for cloud-based LLMs:
- Configure API endpoints, keys, and model names
- Handle API requests and responses

### 2.4 Utility Functions

- `interval_saver.py`: Manages periodic saving of results
- `random_split.py`: Implements sentence splitting for adversarial attacks
- `attack_prompt.py`: Defines templates for different attack prompts
- `print_helper.py` and `clean_text.py`: Assist with output formatting and text cleaning

## 3. Relation to Research Concepts

The implementation closely follows the ABJ attack method described in the paper:

1. **Data Preparation**: Implemented in `data_preparation.py`, this step generates customized data related to the original malicious input, as described in the paper's methodology.

2. **Data Analysis**: The `data_analysis.py` module implements the core of the ABJ attack, guiding the target LLM to analyze the prepared data and potentially generate harmful outputs.

3. **Multiple Attack Variants**: The code supports various ABJ attack methods (original, modified, code-based, adversarial) as explored in the paper.

4. **Evaluation Process**: The pipeline supports multiple rounds of attack and judgment, allowing for the calculation of attack success rate (ASR) and attack efficiency (AE) as described in the paper's evaluation methodology.

5. **Model Flexibility**: The `llm_model.py` module allows for testing across multiple LLMs, including both open-source and closed-source models, as conducted in the paper's comprehensive evaluation.

## 4. Notable Algorithms and Techniques

### 4.1 Adversarial Sentence Splitting

In `utils/random_split.py`, the `split_sentence_randomly()` function implements an adversarial technique to obfuscate malicious prompts:

```python
def split_sentence_randomly(sentence):
    # ... (implementation details)
    return sentence1, sentence2, first_word, second_word
```

This technique splits words and recombines them, making it harder for defense mechanisms to detect malicious content.

### 4.2 Interval-based Result Saving

The `IntervalSaver` class in `utils/interval_saver.py` implements an efficient method for periodically saving results during long-running attacks:

```python
class IntervalSaver:
    def __init__(self, output_path, interval=1 * 1 * 60, columns=['prompt', 'response', 'judgement']):
        # ... (implementation details)

    def add_and_save(self, row):
        # ... (implementation details)
```

This technique ensures that partial results are saved regularly, preventing data loss in case of interruptions.

# Potential Enhancements

1. **Adaptive Attack Strategies**
   - Implement a feedback loop that analyzes the success of different attack variants and adjusts the strategy in real-time.
   - Use reinforcement learning techniques to optimize attack prompts based on the target model's responses.

2. **Multi-modal ABJ Attacks**
   - Extend the ABJ attack to incorporate image or audio data, exploiting LLMs' multi-modal capabilities.
   - Develop techniques to embed malicious content within seemingly benign multi-modal inputs.

3. **Defense Mechanism Integration**
   - Incorporate state-of-the-art defense mechanisms (e.g., Constitutional AI, advanced content filtering) into the pipeline.
   - Implement a framework for easily adding and evaluating new defense strategies against ABJ attacks.

4. **Automated Vulnerability Discovery**
   - Develop a system that automatically generates and tests new ABJ attack variants.
   - Use genetic algorithms or other optimization techniques to evolve more effective attack prompts.

5. **Ethical Considerations and Safeguards**
   - Implement robust access controls and logging mechanisms to prevent misuse of the tool.
   - Develop an automated system for detecting and filtering out extremely harmful or illegal content generated during attacks.
   - Create a framework for responsible disclosure of vulnerabilities discovered through ABJ attacks.