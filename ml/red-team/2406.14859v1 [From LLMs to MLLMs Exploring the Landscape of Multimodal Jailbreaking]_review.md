#Tags
[[Research/Research Papers/2406.14859v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking
**Authors:** Siyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu Wei
**Affiliations:** University of Southern California, Fudan University, Alibaba Inc.
**Publication date:** June 21, 2024

Key Contributions:
- Comprehensive overview of jailbreaking research for both LLMs and MLLMs
- Analysis of evaluation benchmarks, attack techniques, and defense strategies
- Identification of limitations and potential research directions in multimodal jailbreaking

Problem Statement:
The paper addresses the vulnerabilities of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) to jailbreak attacks, highlighting the need for robust evaluation and defense mechanisms.

Methodology:
- Literature review of existing jailbreaking research
- Comparative analysis of unimodal and multimodal jailbreaking techniques
- Identification of gaps and future research directions in multimodal jailbreaking

Main Results and Findings:
1. Evaluation Datasets:
   - Unimodal datasets: PromptBench, AdvBench, AttaQ, Do-Not-Answer, LifeTox, StrongREJECT, FFT, SafetyBench
   - Multimodal datasets: MM-SafetyBench, ToViLaG, SafeBench, RTVLM, AdvBench-M

2. Jailbreak Attack Methods:
   - Non-parametric attacks:
     - Constructing competing objectives (behavior restriction, context virtualization, attention distraction)
     - Inducing mismatched generalization (domain transfer, obfuscation)
   - Parametric attacks:
     - Training interference
     - Decoding intervention

3. Jailbreak Defense Strategies:
   - Extrinsic defenses:
     - Pre-safeguard (harmfulness detection, harmfulness exposure)
     - Post-remediation
   - Intrinsic defenses:
     - Safety alignment
     - Decoding guidance

Qualitative Analysis:
- Multimodal jailbreaking is less explored compared to unimodal approaches
- Current multimodal datasets have limitations in image sources, task scope, and toxicity representation
- Multimodal attacks primarily focus on textual prompts and image noise, with limited exploration of complex multimodal tasks

Limitations and Considerations:
1. Limited image sources in multimodal datasets
2. Narrow task scope in multimodal jailbreaking research
3. Explicit toxicity in existing datasets
4. Static nature of toxicity representation
5. Lack of complex multimodal tasks in attack strategies
6. Neglected image domain shift in attacks
7. Absence of multimodal training interference methods
8. Overly simplistic attack generation techniques

Future Work:
1. Increase diversity of images in jailbreak datasets
2. Develop benchmarks for multi-turn dialogues and embodied interactions
3. Construct datasets with implicit forms of toxicity
4. Create specialized datasets for various demographics and cultures
5. Explore complex multimodal scenarios for context virtualization and attention distraction
6. Investigate image-based domain transfer techniques
7. Develop multimodal training interference methods
8. Design sophisticated multimodal attacks using iterative or collaborative methods
9. Improve generalizability and robustness of defense strategies
10. Explore image-based detection and smoothing methods for multimodal defenses

Conclusion:
The paper provides a comprehensive overview of jailbreaking research in LLMs and MLLMs, highlighting the need for further exploration in multimodal jailbreaking. It emphasizes the importance of developing more diverse and sophisticated evaluation datasets, attack techniques, and defense strategies to enhance the robustness and security of MLLMs.