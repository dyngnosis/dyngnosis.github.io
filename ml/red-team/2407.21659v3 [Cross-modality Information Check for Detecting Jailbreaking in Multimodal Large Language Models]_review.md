#Tags
[[Research/Research Papers/2407.21659v3.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0031/ErodeMLModelIntegrity
#AMLT0054/LLMJailbreak

**Title:** Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models
**Authors:** Yue Xu, Xiuyuan Qi, Zhan Qin, Wenjie Wang
**Affiliations:** 
- School of Information Science and Technology, ShanghaiTech University
- The State Key Laboratory of Blockchain and Data Security, Zhejiang University
**Publication Date:** October 11, 2024 (last updated)

Summary:
This paper introduces CIDER (Cross-modality Information DEtectoR), a plug-and-play jailbreaking detector for Multimodal Large Language Models (MLLMs). CIDER identifies maliciously perturbed image inputs by utilizing cross-modal similarity between harmful queries and adversarial images.

Key Contributions:
- Proposal of CIDER, an effective and efficient pre-detection module for MLLMs
- Investigation of the relationship between malicious queries and adversarial perturbed images in semantic space
- Demonstration of CIDER's effectiveness, efficiency, and transferability across various MLLMs

Problem Statement:
MLLMs are vulnerable to jailbreak attacks, which can break their safety alignment and generate harmful content. Existing defense methods either require model modifications or significant computational resources.

Methodology:
1. Utilize image and text encoders from LLaVA-v1.5-7B to capture semantic meanings
2. Incorporate a diffusion-based denoiser to preprocess image modality
3. Measure semantic similarity using cosine similarity
4. Compare semantic distances between harmful queries and clean/adversarial images before and after denoising
5. Set a threshold based on the change in semantic similarity to detect adversarial images

Main Results:
1. CIDER achieves a detection success rate (DSR) of approximately 80% across various MLLMs
2. Significant reduction in Attack Success Rate (ASR) for multiple MLLMs (e.g., LLaVA-v1.5-7B: 60% to 0%, MiniGPT4: 57% to 9%)
3. CIDER outperforms baseline method (Jailguard) in both effectiveness and efficiency
4. Strong transferability across white-box and black-box MLLMs and attack methods

Qualitative Analysis:
- CIDER leverages the double-edged nature of multimodal information, using it to enhance safeguards against attacks
- The method is model-agnostic and requires minimal computational overhead
- CIDER's effectiveness stems from its ability to detect semantic shifts in adversarial images after denoising

Limitations:
- CIDER impacts MLLM performance on normal tasks, with an approximate 30% overall performance decline
- The method is specifically designed for optimization-based adversarial jailbreak attacks and may not be effective against other types of attacks

Conclusion and Future Work:
CIDER demonstrates effectiveness in safeguarding MLLMs against jailbreak attacks while maintaining efficiency. Future work may focus on:
1. Implementing multi-level thresholds to reduce negative impact on model functionality
2. Exploring CIDER's effectiveness against non-optimization-based jailbreak attacks
3. Developing corresponding defense strategies for a wider array of adversarial threats

New Tool:
CIDER (Cross-modality Information DEtectoR)
GitHub: https://github.com/PandragonXIII/CIDER

Relevant Figures:
1. Figure 2: Workflow of safeguarding MLLM against jailbreak attacks via CIDER
2. Figure 3: Experimental results showing the distribution of cosine similarities and their changes after denoising
3. Figure 5: Comparison of Attack Success Rates (ASR) for different MLLMs with and without CIDER

## Repository Token Information
Total tokens in repository: 0

Tokens per file:


## Tutorial and Enhancement Suggestions

# CIDER: Cross-modality Information DEtectoR Tutorial and Enhancement Suggestions

## Tutorial

### Project Overview

CIDER (Cross-modality Information DEtectoR) is a plug-and-play jailbreaking detector for Multimodal Large Language Models (MLLMs). The project aims to safeguard MLLMs against adversarial attacks by detecting maliciously perturbed image inputs.

Unfortunately, the GitHub repository content is not provided in the given information, so we cannot explore the actual code structure. However, based on the research paper review, we can infer the key components and functionality of the CIDER system.

### Key Components and Functionality

1. **Image and Text Encoders**
   - Utilizes encoders from LLaVA-v1.5-7B
   - Purpose: Capture semantic meanings of images and text

2. **Diffusion-based Denoiser**
   - Preprocesses image modality
   - Purpose: Remove potential adversarial perturbations

3. **Semantic Similarity Measurement**
   - Uses cosine similarity
   - Purpose: Compare semantic distances between harmful queries and images

4. **Threshold-based Detection**
   - Sets a threshold based on changes in semantic similarity
   - Purpose: Determine if an image is adversarial

### Relation to Research Concepts

The code likely implements the following concepts discussed in the paper:

1. Cross-modal similarity analysis between harmful queries and adversarial images
2. Preprocessing of image inputs using a diffusion-based denoiser
3. Comparison of semantic distances before and after denoising
4. Threshold-based decision making for detecting adversarial images

### Notable Algorithms and Techniques

1. **Cosine Similarity**: Used to measure semantic similarity between text and image embeddings
2. **Diffusion-based Denoising**: Applied to preprocess and potentially remove adversarial perturbations from images
3. **Threshold-based Detection**: Employs a decision boundary to classify images as adversarial or clean based on semantic similarity changes

## Potential Enhancements

1. **Multi-level Thresholding**
   - Implement a more sophisticated thresholding mechanism with multiple levels
   - Goal: Reduce the negative impact on normal MLLM functionality while maintaining high detection rates
   - Approach: Develop a hierarchical decision tree or use machine learning classifiers to create more nuanced detection boundaries

2. **Adaptive Denoising**
   - Create an adaptive denoising mechanism that adjusts based on input characteristics
   - Goal: Improve detection accuracy across various types of adversarial perturbations
   - Approach: Implement multiple denoising algorithms and use a meta-learner to select the most appropriate one for each input

3. **Extended Attack Coverage**
   - Expand CIDER's capabilities to detect non-optimization-based jailbreak attacks
   - Goal: Provide comprehensive protection against a wider range of adversarial threats
   - Approach: Research and implement detection methods for other attack types, such as prompt injection or data poisoning

4. **Performance Optimization**
   - Optimize the computational efficiency of CIDER
   - Goal: Reduce the 30% performance decline on normal tasks
   - Approach: Implement parallel processing, use more efficient encoders, or develop a lightweight version of CIDER for less critical applications

5. **Integration with Active Learning**
   - Incorporate active learning techniques to continuously improve CIDER's detection capabilities
   - Goal: Adapt to new and evolving adversarial attack strategies
   - Approach: Implement a feedback loop where uncertain or misclassified samples are flagged for human review and used to update the detection model

These enhancements address limitations mentioned in the paper and aim to push the research forward by improving CIDER's effectiveness, efficiency, and applicability to a broader range of scenarios.