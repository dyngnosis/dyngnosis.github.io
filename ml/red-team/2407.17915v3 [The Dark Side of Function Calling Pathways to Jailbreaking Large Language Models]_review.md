#Tags
[[Research/Research Papers/2407.17915v3.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData

**Title:** The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models
**Authors:** Zihui Wu, Haichang Gao, Jianping He, Ping Wang
**Affiliation:** School of Computer Science and Technology, Xidian University
**Publication Date:** August 29, 2024 (last updated)

Key Contributions:
- Identified a critical vulnerability in the function calling process of LLMs
- Introduced a novel "jailbreak function" attack method
- Conducted empirical study on six state-of-the-art LLMs, revealing high attack success rates
- Analyzed reasons for function call susceptibility to attacks
- Proposed defensive strategies, including defensive prompts

Problem Statement:
The paper addresses the overlooked security implications of the function calling feature in Large Language Models (LLMs), which can be exploited for jailbreaking attacks.

Methodology:
1. Designed a "jailbreak function" called WriteNovel to induce harmful content generation
2. Evaluated attack success rate on six LLMs: GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-pro, and others
3. Analyzed causes of vulnerability in function calling
4. Tested defensive measures, particularly defensive prompts

Main Results:
1. Achieved an average attack success rate of over 90% across 6 state-of-the-art LLMs
2. Identified three primary reasons for jailbreak function attack success:
   a. Alignment discrepancies
   b. User coercion
   c. Oversight in safety measures
3. Demonstrated effectiveness of defensive prompts in mitigating attacks

Qualitative Analysis:
- The study reveals a significant security gap in the function calling feature of LLMs
- The high success rate of attacks across multiple models indicates a systemic vulnerability
- The ability to force LLMs to execute potentially harmful functions highlights the need for improved security measures

Limitations:
- The study focuses on a specific type of attack (jailbreak function) and may not cover all potential vulnerabilities
- The effectiveness of defensive measures may vary across different LLM implementations

Conclusion and Future Work:
- The paper emphasizes the urgent need for enhanced security measures in LLM function calling capabilities
- Proposed future work includes developing more robust defensive strategies and conducting comprehensive security assessments of LLMs

Tools Introduced:
- Jailbreak function attack method (GitHub: https://github.com/wooozihui/jailbreakfunction)

Relevant Figures:
1. Figure 1: Overview of the function calling process in LLMs and the potential for jailbreak attacks
2. Figure 2: Components of the jailbreak function calling
3. Figure 3: Screenshots showcasing generation of harmful content in GPT-4o, Claude-3.5-Sonnet, and Gemini-1.5-Pro

## Repository Token Information
Total tokens in repository: 3056

Tokens per file:
- attack.py: 2172 tokens
- evaluate.py: 454 tokens
- utils.py: 430 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: Jailbreak Function Attack on Large Language Models

## Project Overview

This repository implements the "jailbreak function" attack method described in the research paper "The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models". The attack exploits vulnerabilities in the function calling capabilities of Large Language Models (LLMs) to generate harmful content.

### Project Structure

The repository consists of three main Python files:
1. `attack.py`: Contains the core attack implementation
2. `evaluate.py`: Implements the evaluation of attack success
3. `utils.py`: Provides utility functions for data handling and content type generation

## Key Components

### 1. Jailbreak Function Construction (`attack.py`)

The `jailbreak_function_consturct` function creates a specially crafted function definition that tricks the LLM into generating harmful content. It adapts the function structure based on the target model (GPT, Claude, Gemini, or Mixtral).

```python
def jailbreak_function_consturct(model_name, harm_behavior, content_type):
    # Function construction logic for different model types
    # ...
```

### 2. Jailbreak Function Call (`attack.py`)

The `jailbreak_function_call` function executes the attack by calling the constructed jailbreak function on the target LLM. It handles different API interfaces for various models (OpenAI, Anthropic, Google, etc.).

```python
def jailbreak_function_call(args, harm_behavior, content_type):
    # Logic to call the jailbreak function on different LLMs
    # ...
```

### 3. Attack Execution (`attack.py`)

The `main` function orchestrates the attack process:
1. Loads harmful behaviors from a dataset
2. Iterates through behaviors, attempting the attack with different content types
3. Evaluates attack success using the `harmful_classification` function
4. Calculates and reports Attack Success Rate (ASR)

### 4. Evaluation (`evaluate.py`)

The `harmful_classification` function uses a judge model (e.g., GPT-4) to determine if the generated content is harmful:

```python
def harmful_classification(sentence, args):
    # Logic to classify content as harmful or not
    # ...
```

### 5. Utility Functions (`utils.py`)

- `get_content_type`: Generates appropriate content types for a given harmful behavior
- `get_dataset`: Loads the dataset of harmful behaviors

## Relation to Research Paper Concepts

1. **Jailbreak Function Design**: The code implements the novel "jailbreak function" attack method described in the paper, using a fictional novel-writing scenario to bypass safety measures.

2. **Multi-Model Testing**: The implementation supports attacks on multiple LLMs (GPT, Claude, Gemini, Mixtral) as discussed in the paper's empirical study.

3. **Attack Success Evaluation**: The code calculates Attack Success Rate (ASR) for both 1-shot and 5-shot attempts, aligning with the paper's evaluation methodology.

4. **Content Type Generation**: The `get_content_type` function generates diverse content types, exploring the attack's effectiveness across different contexts as mentioned in the paper.

## Notable Techniques

1. **API Abstraction**: The code abstracts API calls for different LLMs, allowing for easy extension to new models.

2. **Judge Model Classification**: Uses a separate LLM (e.g., GPT-4) to evaluate the harmfulness of generated content, providing an automated assessment method.

3. **Iterative Attack Attempts**: The implementation tries multiple content types for each harmful behavior, increasing the chances of a successful attack.

# Potential Enhancements

1. **Defensive Prompt Integration**
   - Implement and evaluate the defensive prompts proposed in the paper
   - Create a framework for easily testing different defensive strategies

2. **Automated Vulnerability Analysis**
   - Develop a system to automatically analyze LLM responses and identify potential vulnerabilities in their function calling implementation
   - Use machine learning techniques to classify and categorize different types of vulnerabilities

3. **Cross-Model Transfer Learning**
   - Investigate if successful attack patterns on one model can be transferred to other models
   - Implement a meta-learning approach to generate model-agnostic attack strategies

4. **Real-time Attack Detection**
   - Develop a monitoring system that can detect potential jailbreak function attacks in real-time
   - Implement adaptive defenses that evolve based on observed attack patterns

5. **Ethical Content Generation**
   - Extend the framework to generate non-harmful alternatives to requested harmful content
   - Implement a system that can rewrite harmful instructions into ethical, legal alternatives while preserving the user's underlying intent where appropriate