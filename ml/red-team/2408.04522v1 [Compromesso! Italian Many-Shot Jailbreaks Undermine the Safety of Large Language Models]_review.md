#Tags
[[Research/Research Papers/2408.04522v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language Models
**Authors:** Fabio Pernisi, Dirk Hovy, Paul RÃ¶ttger
**Affiliation:** Bocconi University
**Publication Date:** August 8, 2024

Key Contributions:
- Created a new dataset of 418 unsafe Italian question-answer pairs for assessing LLM safety
- Demonstrated the effectiveness of many-shot jailbreaking in Italian on open-weight LLMs
- Identified increasing unsafe behavior in models as the number of unsafe demonstrations grows

Problem Statement:
How effective are many-shot jailbreaks in Italian, particularly on lightweight, open-weight LLMs?

Methodology:
1. Dataset creation:
   - Merged and filtered English datasets (SimpleSafetyTest and StrongReject)
   - Generated initial responses using an uncensored WizardLM 13B model
   - Manually edited and refined responses to ensure they were unsafe
   - Translated question-answer pairs into Italian

2. Models tested:
   - Llama 3 8B
   - Mistral 7B v0.3
   - Qwen 1.5 4B and 7B
   - Gemma 2B and 7B

3. Evaluation methods:
   - Negative Log Likelihood (NLL)
   - Model Response Safety

Results:
1. NLL evaluation:
   - Consistent decrease in NLL as the number of shots increases
   - Diminishing returns with increasing shots

2. Model Response Safety:
   - General increase in unsafe responses with more shots
   - Mistral7B, Llama3 8B, and Gemma models showed high proportion of unsafe completions even with few shots
   - Qwen models demonstrated a more pronounced impact of additional shots

Qualitative Analysis:
- Multilingual models (e.g., Qwen 1.5) showed lower proportions of unsafe responses, suggesting potential robustness against jailbreaking
- Unexpected decrease in unsafe responses for Gemma 2B at 32 shots, possibly due to limited expressiveness of the smaller model

Limitations:
- Study focused only on Italian and small, open-weight models
- Random sampling of demonstrations without considering specific safety categories
- Prompt format variations not examined

Conclusion:
The study reveals significant vulnerabilities in lightweight open-weight models to many-shot jailbreaking attacks in Italian. The effectiveness of jailbreaking increases with the number of unsafe demonstrations, highlighting the urgent need for robust, cross-lingual safety protocols in LLMs.

Future Work:
- Expand evaluations to more languages and a broader range of models
- Investigate the impact of category-specific demonstrations on model responses
- Examine variations in prompt format and their effects on jailbreaking effectiveness

Tools Introduced:
- New Italian dataset of 418 unsafe question-answer pairs (GitHub repository not specified)