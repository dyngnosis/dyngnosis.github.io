#Tags
[[Research/Research Papers/2407.08956v2.pdf]]

#AMLT0018/BackdoorMLModel
#AMLT0020/PoisonTrainingData
#AMLT0031/ErodeMLModelIntegrity

**Title:** Defending Code Language Models against Backdoor Attacks with Deceptive Cross-Entropy Loss
**Authors:** Guang Yang, Yu Zhou, Xiangyu Zhang, Xiang Chen, Terry Zhuo, David Lo, Taolue Chen
**Affiliations:** Nanjing University of Aeronautics and Astronautics, Nantong University, Monash University, Singapore Management University, Birkbeck University of London
**Publication Date:** July 12, 2024 (preprint)

Key Contributions:
- Confirmation of the "early learning" phenomenon in Code Language Models (CLMs)
- Analysis of overfitting to backdoor triggers due to unbounded cross-entropy loss
- Proposal of DeCE (Deceptive Cross-Entropy) loss function for defending against backdoor attacks
- Comprehensive evaluation of DeCE across multiple datasets, models, and poisoning ratios

Problem Statement:
The paper addresses the vulnerability of Code Language Models to backdoor attacks and the lack of effective defense mechanisms, particularly when applying existing NLP defense methods to CLMs.

Methodology:
1. Empirical study to confirm "early learning" phenomenon in CLMs
2. Analysis of cross-entropy loss function's role in overfitting to backdoor triggers
3. Development of DeCE loss function:
   - Blending of deceptive distributions
   - Application of label smoothing
   - Gradient bounding
4. Evaluation on code synthesis tasks:
   - Datasets: Lyra, Pisces, Bugs2Fix
   - Models: CodeBERT, GraphCodeBERT, CodeGen, CodeT5, CodeT5p
   - Poisoning ratios: 1%, 2%, 5% for Lyra/Pisces; 0.1%, 0.5%, 1% for Bugs2Fix
   - Trigger designs: NL triggers (bbtag, RIPPLe, BadPre, Grammar) and code triggers (function name, dead-code, AFRAIDOOR)

Main Results:
1. DeCE outperforms existing active defense methods (BKI, In-trust Loss, GCE, Moderate-fitting) in defending against backdoor attacks while maintaining model performance
2. DeCE shows superior performance compared to passive defense methods (ONION, Paraphrasing)
3. DeCE effectively improves model security against backdoor attacks in both generative and classification tasks

Qualitative Analysis:
- DeCE addresses the trade-off between security enhancement and performance on clean data, which is a common challenge in existing defense methods
- The effectiveness of DeCE across various model sizes and complexities suggests its potential as a general defense mechanism for CLMs

Limitations:
- The study primarily focuses on code synthesis tasks, with limited exploration of classification tasks
- The effectiveness of DeCE against more advanced adaptive attacks needs further investigation

Conclusion and Future Work:
- DeCE provides a promising approach for defending CLMs against backdoor attacks
- Future work includes optimizing DeCE's hyper-parameters and investigating its applicability to other code intelligence tasks

Relevant Figures/Tables:
- Table 1: Impact of different poisoning ratios and attack strategies on CLM vulnerability
- Figure 2: Performance of CLMs on validation set over training epochs (illustrates "early learning" phenomenon)
- Table 3-5: Comparison of defense methods against various backdoor attacks

New Tool:
- DeCE (Deceptive Cross-Entropy) loss function
- GitHub repository: https://github.com/NTDXYG/DeCE