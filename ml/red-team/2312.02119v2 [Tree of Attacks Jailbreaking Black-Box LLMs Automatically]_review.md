#Tags
[[Research/Research Papers/2312.02119v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** Tree of Attacks: Jailbreaking Black-Box LLMs Automatically
**Authors:** Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, Amin Karbasi
**Publication Date:** December 4, 2023 (last updated February 21, 2024)

Summary:
This paper presents Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks for black-box large language models (LLMs). TAP uses tree-of-thought reasoning to iteratively refine attack prompts, assessing and pruning unlikely candidates before querying the target LLM. The method achieves high success rates in jailbreaking state-of-the-art LLMs, including those protected by advanced safety measures.

Key Contributions:
- Introduction of TAP, an automated black-box jailbreaking method
- Demonstration of high success rates (>80%) against state-of-the-art LLMs
- Effective jailbreaking of LLMs protected by advanced safety measures (e.g., LlamaGuard)
- Significant improvement over previous state-of-the-art black-box jailbreaking methods

Problem Statement:
How to automatically generate effective jailbreaks for black-box LLMs while minimizing the number of queries to the target model?

Methodology:
1. Tree-of-thought reasoning to iteratively refine attack prompts
2. Pruning of unlikely candidates before querying the target LLM
3. Use of three LLMs: attacker, evaluator, and target
4. Evaluation on multiple datasets and state-of-the-art LLMs

Main Results:
1. TAP achieves 90% success rate on GPT-4 with 28.8 queries on average
2. 84% success rate on GPT-4-Turbo with 22.5 queries on average
3. High success rates (>75%) on other closed-source models with <30 queries per prompt
4. Effective against LLMs protected by LlamaGuard
5. Significant improvement over PAIR method in both success rate and query efficiency

Qualitative Analysis:
- Small unaligned LLMs can be used to jailbreak large LLMs, indicating a need for improved safety measures
- Jailbreaking has a low computational cost and only requires black-box access
- More capable LLMs appear to be easier to jailbreak, with Llama-2-Chat-7B showing higher robustness

Limitations:
- Evaluation limited to specific datasets; performance may vary on different datasets
- Potential for misuse in generating harmful content
- Reliance on closed-source models for evaluation, limiting reproducibility

Conclusion and Future Work:
The paper demonstrates the effectiveness of TAP in automatically generating jailbreaks for state-of-the-art LLMs. Future work may include:
1. Exploring methods to safeguard LLMs against interpretable prompts without degrading benign responses
2. Investigating jailbreaking techniques for restricted requests beyond harmful content
3. Evaluating vulnerability to multi-prompt jailbreaks
4. Developing more robust safety measures for LLMs

Tools Introduced:
- Tree of Attacks with Pruning (TAP)
- GitHub repository: https://github.com/RICommunity/TAP

## Repository Token Information
Total tokens in repository: 13765

Tokens per file:
- common.py: 517 tokens
- conversers.py: 2425 tokens
- main_TAP.py: 2889 tokens
- config.py: 130 tokens
- readme.md: 1288 tokens
- requirements.txt: 125 tokens
- evaluators.py: 982 tokens
- loggers.py: 1225 tokens
- system_prompts.py: 1672 tokens
- language_models.py: 2512 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: Tree of Attacks with Pruning (TAP)

## 1. Project Overview

The Tree of Attacks with Pruning (TAP) project implements an automated method for generating jailbreaks for black-box large language models (LLMs). The main goal is to craft adversarial prompts that cause target LLMs to produce responses that violate their ethical guidelines or safety measures.

### Project Structure

The repository is organized into several Python files, each handling specific aspects of the TAP algorithm:

- `main_TAP.py`: The main script that orchestrates the entire TAP process
- `conversers.py`: Defines the AttackLLM and TargetLLM classes for generating attacks and responses
- `evaluators.py`: Implements the evaluation of generated prompts and responses
- `language_models.py`: Contains classes for interacting with various LLM APIs
- `system_prompts.py`: Stores system prompts for the attacker and evaluator models
- `common.py`: Provides utility functions used across the project
- `loggers.py`: Implements logging functionality using Weights & Biases (wandb)
- `config.py`: Stores configuration variables and API endpoints

## 2. Key Components and Functionality

### 2.1 Attack Generation (`conversers.py`)

The `AttackLLM` class is responsible for generating adversarial prompts. It uses a language model to iteratively refine prompts based on previous attempts and feedback. The `get_attack` method is the core function that generates a batch of attack prompts.

Key features:
- Batched prompt generation
- JSON output parsing
- Multiple attempt handling for failed generations

### 2.2 Target Model Interaction (`conversers.py`)

The `TargetLLM` class handles interactions with the target language model. It takes generated attack prompts and obtains responses from the target model.

### 2.3 Evaluation (`evaluators.py`)

The `EvaluatorBase` class and its subclasses (`GPTEvaluator`, `NoEvaluator`) are responsible for assessing the quality of generated attacks and target responses. They provide two main functions:

1. `judge_score`: Evaluates how well the target response achieves the desired jailbreak behavior
2. `on_topic_score`: Assesses whether the generated prompt is relevant to the original goal

### 2.4 Language Model Interfaces (`language_models.py`)

This file contains classes for interacting with various LLM APIs, including:
- OpenAI's GPT models
- Anthropic's Claude
- Google's PaLM and Gemini Pro
- Custom API endpoints for models like Vicuna and Llama

### 2.5 Main TAP Algorithm (`main_TAP.py`)

The `main` function in `main_TAP.py` implements the core TAP algorithm:

1. Initialize models (attacker, target, evaluator) and logger
2. Begin the TAP loop for a specified number of iterations:
   a. Branch: Generate new attack prompts
   b. Prune (Phase 1): Remove off-topic prompts
   c. Query and Assess: Get responses from the target model and evaluate them
   d. Prune (Phase 2): Keep the most promising attacks
   e. Log results and check for successful jailbreaks
3. Finish and report final statistics

## 3. Relation to Research Paper Concepts

The implementation closely follows the TAP algorithm described in the research paper:

- Tree-of-thought reasoning: Implemented through iterative refinement of prompts in the `AttackLLM` class
- Pruning: Realized in two phases using the `prune` function in `main_TAP.py`
- Three-LLM setup: Attacker, Target, and Evaluator models are clearly separated
- Query efficiency: Achieved through batched operations and pruning steps

## 4. Notable Algorithms and Techniques

### 4.1 JSON Parsing for Attack Prompts

The `extract_json` function in `common.py` uses a robust method to extract structured data from the attacker's output, allowing for flexible prompt generation.

### 4.2 Adaptive Retry Mechanism

The language model classes implement an adaptive retry mechanism with exponential backoff to handle API errors and rate limiting.

### 4.3 Batched Operations

The implementation uses batched operations throughout to improve efficiency, particularly in the `batched_generate` methods of language model classes.

### 4.4 Dynamic Prompt Truncation

To handle context length limitations, the code dynamically truncates conversation history in the `main` function of `main_TAP.py`.

# Potential Enhancements

1. Multi-objective Optimization
   - Implement a multi-objective optimization approach to balance jailbreak success rate, query efficiency, and prompt interpretability
   - This could involve using techniques like NSGA-II or MOEA/D to explore the Pareto front of these competing objectives

2. Adaptive Sampling Strategies
   - Develop more sophisticated sampling strategies for the attacker model that adapt based on previous successes and failures
   - Incorporate techniques from active learning or Bayesian optimization to guide the exploration of the prompt space more efficiently

3. Cross-model Transferability Analysis
   - Extend the codebase to systematically study the transferability of jailbreak prompts across different target models
   - Implement methods to quantify and visualize the similarity of vulnerabilities across models, potentially uncovering common weaknesses

4. Robust Safety Measure Development
   - Create a framework for iteratively improving safety measures based on successful jailbreaks
   - Implement adversarial training techniques to make target models more robust against generated attacks

5. Interpretability and Explanation Generation
   - Develop methods to analyze and explain why certain prompts are successful in jailbreaking models
   - Implement techniques from interpretable AI, such as SHAP values or integrated gradients, to attribute importance to different parts of successful prompts
   - Use this information to generate human-readable explanations of vulnerabilities, aiding in the development of more robust safety measures