#Tags
[[Research/Research Papers/2410.02298v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0031/ErodeMLModelIntegrity
#AMLT0054/LLMJailbreak

**Title:** Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models
**Authors:** Guobin Shen, Dongcheng Zhao, Yiting Dong, Xiang He, Yi Zeng
**Affiliations:** Brain-inspired Cognitive Intelligence Lab, Institute of Automation, Chinese Academy of Sciences; Beijing Institute of AI Safety and Governance; Center for Long-term Artificial Intelligence; School of Future Technology, University of Chinese Academy of Sciences
**Publication date:** October 7, 2024

Key Contributions:
- Introduction of Jailbreak Antidote, a method for real-time adjustment of LLM safety preferences
- Demonstration that safety information in LLMs is sparsely distributed
- Comprehensive evaluation across nine LLMs, ten jailbreak attack methods, and six defense strategies

Problem Statement:
The paper addresses the challenge of balancing safety and utility in large language models (LLMs) while defending against jailbreak attacks that manipulate LLMs into generating harmful content.

Methodology:
1. Identify a safety direction in the model's representation space using PCA on hidden states from benign and harmful prompts
2. Create a mask to retain only the top k% of dimensions with the largest absolute values in the safety direction
3. Adjust the model's hidden states during inference by adding the masked safety direction scaled by a factor α
4. Evaluate the method across various LLMs, attack methods, and defense strategies using JailbreakBench and AlpacaEval

Main Results:
1. Jailbreak Antidote achieves high Defense Success Rates (DSR) across different models and attack methods
2. The method maintains high Win Rates on AlpacaEval, preserving model utility
3. Adjusting approximately 5% of the internal state is as effective as modifying the entire state

Qualitative Analysis:
- The method's effectiveness increases with model size, suggesting larger models have a greater capacity to encode and utilize safety-related information
- Jailbreak Antidote remains effective against sophisticated attacks like GCG and PAIR, demonstrating its robustness
- The approach offers real-time control over the safety-utility balance without additional computational overhead

Limitations:
- The optimal scaling factor α and sparsity parameter k may vary across different models and contexts
- The method's effectiveness may be limited for smaller models with reduced parameter space

Conclusion and Future Work:
Jailbreak Antidote offers a lightweight, scalable solution for enhancing LLM safety while preserving utility. Future work could explore:
1. Dynamically adapting α and k based on context or model confidence
2. Applying the approach to other aspects of model alignment, such as fairness or domain adaptation
3. Investigating the method's applicability to other challenges in AI safety and alignment

Relevant Figures:
Figure 1: Overview of Jailbreak Antidote, including safety direction extraction, internal state adjustment, and performance comparison
Figure 2: Visualization of hidden states and distribution of safety direction components

New Tools:
Jailbreak Antidote: A method for real-time adjustment of LLM safety preferences via sparse representation adjustment (No GitHub repository mentioned)