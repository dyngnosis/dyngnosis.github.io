#Tags
[[Research/Research Papers/2405.20015v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0054/LLMJailbreak
#AMLT0043/CraftAdversarialData
#AMLT0005/CreateProxyMLModel

**Title:** Efficient LLM-Jailbreaking by Introducing Visual Modality
**Authors:** Zhenxing Niu, Yuyao Sun, Haodong Ren, Haoxuan Ji, Quan Wang, Xiaoke Ma, Gang Hua, Rong Jin
**Affiliations:** Xidian University, Xi'an Jiaotong University, Wormpex AI Research, Meta
**Publication Date:** May 30, 2024

Summary:
This paper presents a novel approach to jailbreaking large language models (LLMs) by leveraging multimodal large language models (MLLMs). The method involves constructing an MLLM by incorporating a visual module into the target LLM, performing an efficient MLLM-jailbreak to generate jailbreaking embeddings, and converting these embeddings into text to jailbreak the target LLM.

Key Contributions:
1. Introduction of a visual modality to enhance LLM-jailbreaking efficiency
2. Development of a double jailbreaking workflow for both white-box and black-box scenarios
3. Proposal of an image-text semantic matching scheme to improve attack success rate
4. Demonstration of superior cross-class jailbreaking capabilities

Problem Statement:
The paper addresses the challenge of efficiently jailbreaking LLMs to generate objectionable content in response to harmful queries, while overcoming the limitations of existing methods such as discrete optimization-based and embedding-based approaches.

Methodology:
1. Construct an MLLM by incorporating a visual module into the target LLM
2. Perform MLLM-jailbreak to generate jailbreaking embeddings (embJS)
3. Convert embJS to textual jailbreaking suffix (txtJS) using De-embedding and De-tokenizer operations
4. Use txtJS to jailbreak the target LLM
5. Implement an image-text semantic matching scheme to identify suitable initial input (InitJS)

Main Results:
1. The proposed approach outperforms state-of-the-art methods in terms of efficiency and effectiveness
2. Superior cross-class jailbreaking capabilities demonstrated
3. Significant improvement in jailbreaking ASR when using appropriate InitJS
4. Enhanced black-box jailbreaking performance when the target LLM's tokenizer is known

Qualitative Analysis:
- The visual modality introduction serves as a regularizer for embedding optimization, ensuring that embJS have corresponding discrete tokens
- The double jailbreaking workflow provides flexibility for both white-box and black-box scenarios
- Cross-class generalization varies across different harmful classes, suggesting potential for leveraging correlated classes to enhance ASR

Limitations:
- The approach may still struggle with abstract concepts like "hate" that are difficult to represent visually
- Effectiveness may vary depending on the visual-textual alignment of the target LLM

Conclusion and Future Work:
The paper concludes that introducing visual modality for LLM-jailbreaking is an efficient and effective approach. Future work could focus on:
1. Investigating the underlying reasons for correlation among specific harmful classes
2. Exploring ways to improve jailbreaking for abstract concepts
3. Developing more robust defenses against this type of attack

Tools Introduced:
- No specific new tools mentioned, but the paper refers to using LLaMA Guard 2 for automatic evaluation of jailbreaking success

Figures and Tables:
- Figure 1: Illustrates the double jailbreaking workflow
- Figure 2: Depicts the full workflow of the approach, including the image-text matching scheme
- Figure 3: Shows the cross-class generalization for LLM-jailbreaking
- Tables 1-3: Present comparative results for white-box jailbreaking, black-box jailbreaking, and image-text semantic matching, respectively