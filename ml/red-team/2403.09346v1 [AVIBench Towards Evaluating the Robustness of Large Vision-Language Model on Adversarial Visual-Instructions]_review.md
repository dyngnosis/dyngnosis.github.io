#Tags
[[Research/Research Papers/2403.09346v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0031/ErodeMLModelIntegrity
#AMLT0029/DenialOfMLService
#AMLT0057/LLMDataLeakage

**Title:** AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions

**Authors:** Hao Zhang, Wenqi Shao, Hong Liu, Yongqiang Ma, Ping Luo, Yu Qiao, Kaipeng Zhang

**Publication Date:** March 14, 2024

Summary:
This paper introduces AVIBench, a comprehensive framework for evaluating the robustness of Large Vision-Language Models (LVLMs) against various types of adversarial visual-instructions (AVIs). The study generates 260K AVIs across five categories of multimodal capabilities and content biases, evaluating 14 open-source LVLMs and revealing vulnerabilities in both open-source and closed-source models like GeminiProVision and GPT-4V.

Key Contributions:
- Introduction of AVIBench, a framework and tool for evaluating LVLM robustness against AVIs
- Generation of 260K AVIs spanning five multimodal capabilities and content biases
- Evaluation of 14 open-source LVLMs' resistance to adversarial AVIs
- Demonstration of significant content biases in advanced closed-source LVLMs

Problem Statement:
The study addresses the lack of comprehensive research on LVLMs' robustness against adversarial attacks on both text and image modalities, as well as inherent biases in these models.

Methodology:
1. Development of AVIBench framework
2. Generation of AVIs:
   - 4 types of image-based AVIs
   - 10 types of text-based AVIs
   - 9 types of content bias AVIs
3. Evaluation of 14 open-source LVLMs and 2 closed-source LVLMs
4. Analysis of robustness across different attack types and multimodal capabilities

Main Results and Findings:
1. Image-based AVIs:
   - MiniGPT-4 showed strongest anti-corruption capability
   - mPLUG-owl demonstrated weakest performance with 17% average performance drop
   - Elastic, Glass_Blur, and Shot_Noise were most effective attack methods

2. Text-based AVIs:
   - TextFooler was most effective attack method with 67% ASDR
   - LLaVA-1.5 was most robust (27% ASDR), while OpenFlamingo-V2 was most vulnerable (51% ASDR)

3. Content Bias AVIs:
   - ShareGPT4V performed best among open-source LVLMs (74% score)
   - VPGTrans performed worst (31% score)
   - GeminiProVision achieved top performance overall, but showed notable racial and gender biases
   - GPT-4V exhibited biases in cultural contexts and performed worse than some earlier open-source LVLMs

Qualitative Analysis:
- The study reveals that even advanced closed-source LVLMs exhibit significant biases and vulnerabilities to adversarial attacks
- The robustness of LVLMs varies across different attack types and multimodal capabilities
- There is a need for targeted defense strategies and improved training methods to enhance LVLM robustness

Limitations:
- The study primarily focuses on open-source LVLMs, with limited evaluation of closed-source models
- The relationship between model performance and robustness is not fully explored
- The impact of different model architectures and training data on robustness is not comprehensively analyzed

Conclusion and Future Work:
- AVIBench provides a valuable tool for assessing LVLM robustness against adversarial attacks
- The study highlights the need for improved defense mechanisms and bias mitigation in LVLMs
- Future work should focus on developing more robust and fair LVLMs, as well as exploring the relationship between model architecture, training data, and robustness

New Tools:
AVIBench: A framework and dataset for evaluating LVLM robustness against adversarial visual-instructions. The source code and benchmark will be made publicly available.