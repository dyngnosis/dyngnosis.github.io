#Tags
[[Research/Research Papers/2410.03857v2.pdf]]

#AMLT0051/LLMPromptInjection
#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0031/ErodeMLModelIntegrity

**Title:** You Know What I'm Saying: Jailbreak Attack via Implicit Reference
**Authors:** Tianyu Wu, Lingrui Mei, Ruibin Yuan, Lujun Li, Wei Xue, Yike Guo
**Affiliations:** The Hong Kong University of Science and Technology, REDTech
**Publication Date:** October 8, 2024

Summary:
This paper introduces a novel jailbreak attack method called Attack via Implicit Reference (AIR) that exploits vulnerabilities in large language models (LLMs) by decomposing malicious objectives into permissible objectives linked through implicit references. The study demonstrates high attack success rates across various LLMs and reveals an inverse scaling phenomenon where larger models are more vulnerable to this attack.

Key Contributions:
- Introduction of the AIR jailbreak method
- Demonstration of AIR's effectiveness across state-of-the-art LLMs
- Observation of an inverse scaling phenomenon in model vulnerability
- Development of a cross-model attack strategy

Problem Statement:
Current LLM alignment techniques are ineffective at detecting malicious objectives expressed through context within nested harmless objectives, creating a security vulnerability in AI systems.

Methodology:
1. Decomposition of malicious objectives into nested benign objectives
2. Use of implicit references to link objectives
3. Two-stage conversation approach:
   - First stage: Generate harmless content with nested malicious intent
   - Second stage: Rewrite request to remove unrelated parts and provide more detailed information
4. Evaluation using 100 malicious behaviors from JailbreakBench
5. Testing on various LLMs including GPT-4o, Claude-3.5-Sonnet, and Qwen-2-72B
6. Comparison with existing jailbreak techniques and detection methods

Main Results:
1. AIR achieved an attack success rate (ASR) exceeding 90% on most models tested
2. Larger models showed higher vulnerability to AIR attacks (inverse scaling phenomenon)
3. Cross-model attack strategy further increased ASR when targeting more secure models
4. Existing detection methods (SmoothLLM, PerplexityFilter, Erase-and-Check) were ineffective against AIR

Qualitative Analysis:
- AIR exploits the in-context learning capabilities of LLMs, making larger and more advanced models more susceptible to attacks
- The method leverages the balance between safety and utility in LLMs, presenting sensitive requests in a way that appears legitimate
- The cross-model attack strategy demonstrates the transferability of the attack method between models

Limitations:
- Requires models with capabilities similar to GPT-4o-0513 for effective rewriting
- Some malicious objectives cannot be decomposed into nested form
- The study primarily focused on the writing scenario, though the attack may work in other contexts

Conclusion and Future Work:
The paper highlights a significant vulnerability in current LLM safety mechanisms and emphasizes the need for more sophisticated defense strategies that can detect and mitigate contextual attacks. Future research should focus on enhancing models' ability to identify and reject malicious intent hidden in context.

New Tools:
The authors have released their code and jailbreak artifacts at https://github.com/Lucas-TY/llm_Implicit_reference

## Repository Token Information
Total tokens in repository: 10110

Tokens per file:
- reform_prompt.py: 4460 tokens
- main.py: 2135 tokens
- scripts/defense.py: 1272 tokens
- judges.py: 1227 tokens
- models.py: 1016 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: Attack via Implicit Reference (AIR) Implementation

## Project Overview

This repository implements the Attack via Implicit Reference (AIR) jailbreak method described in the research paper "You Know What I'm Saying: Jailbreak Attack via Implicit Reference". The project aims to demonstrate the vulnerability of large language models (LLMs) to attacks that decompose malicious objectives into seemingly harmless nested objectives linked through implicit references.

## Project Structure

The repository consists of the following key files:

1. `reform_prompt.py`: Implements the PromptReform class for reformulating malicious requests
2. `main.py`: Main script for running the jailbreak attacks
3. `scripts/defense.py`: Script for evaluating defense mechanisms against the attacks
4. `judges.py`: Implements various judging methods to evaluate attack success
5. `models.py`: Defines the TargetModel class for interacting with LLMs

## Key Components and Functionality

### 1. PromptReform (reform_prompt.py)

The `PromptReform` class is responsible for reformulating malicious requests into a nested structure that can evade detection. It implements several methods (`get_prompt_k2` to `get_prompt_k6`) that generate prompts with increasing levels of complexity and indirection.

Key features:
- Decomposes malicious objectives into nested benign objectives
- Uses implicit references to link objectives
- Generates prompts with varying levels of abstraction (k2 to k6)

### 2. Main Attack Script (main.py)

This script orchestrates the jailbreak attack process:

1. Loads malicious requests from a CSV file
2. Reformulates requests using the PromptReform class
3. Sends reformulated requests to the target model
4. Evaluates responses using different judging methods
5. Saves results and attack artifacts

Key functions:
- `reformulate_distributed_attack`: Reformulates the original prompt using the specified method
- `judge_response`: Evaluates the model's response using different judging criteria

### 3. Defense Evaluation (scripts/defense.py)

This script evaluates the effectiveness of defense mechanisms against the AIR attack:

1. Loads attack artifacts from a JSON file
2. Applies specified defense mechanism (e.g., SmoothLLM)
3. Re-evaluates responses with and without the defense
4. Saves updated results

### 4. Judging Methods (judges.py)

Implements three judging methods to evaluate attack success:

1. `judge_gpt`: Uses GPT-4 to rate responses on a scale of 1-10
2. `judge_llama3`: Uses LLAMA-3 to classify responses as 'safe' or 'unsafe'
3. `judge_rule_based`: Uses a set of predefined rules to determine if a response is jailbroken

### 5. Target Model Interface (models.py)

The `TargetModel` class provides an interface for interacting with various LLMs:

- Initializes connection to the target model API
- Implements methods for generating responses and rewriting content
- Handles the two-stage conversation approach described in the paper

## Relation to Research Paper Concepts

The implementation closely follows the methodology described in the paper:

1. **Decomposition of malicious objectives**: Implemented in the `PromptReform` class
2. **Two-stage conversation approach**: 
   - First stage: Generating harmless content with nested malicious intent (`reformulate_distributed_attack`)
   - Second stage: Rewriting to focus on malicious content (`rewrite_response` in `TargetModel`)
3. **Evaluation using JailbreakBench**: Uses 100 malicious behaviors from the benchmark
4. **Testing on various LLMs**: Supports different models through the `TargetModel` class
5. **Comparison with existing techniques**: Implements multiple judging methods for evaluation

## Notable Algorithms and Techniques

1. **Prompt Reformulation**: The `PromptReform` class implements a sophisticated algorithm for restructuring malicious requests into nested, seemingly benign objectives.

2. **Adaptive Rewriting**: The `rewrite_response` method in `TargetModel` adaptively refines the generated content to focus on the malicious objective while removing unrelated parts.

3. **Multi-method Judging**: The implementation uses three different judging methods (GPT-4, LLAMA-3, and rule-based) to provide a comprehensive evaluation of attack success.

4. **Cross-model Attack Strategy**: The code supports targeting different models, enabling the cross-model attack strategy described in the paper.

# Potential Enhancements

1. **Automated Decomposition of Malicious Objectives**
   - Develop an AI-powered system to automatically decompose complex malicious objectives into nested benign objectives
   - This could expand the range of attackable objectives and increase the method's versatility

2. **Dynamic Defense Adaptation**
   - Implement a system that dynamically adjusts the defense mechanisms based on the detected attack patterns
   - This could help address the inverse scaling phenomenon observed in the paper

3. **Multi-modal AIR Attacks**
   - Extend the AIR technique to incorporate multi-modal inputs (text, images, audio)
   - This could explore how implicit references can be embedded across different modalities

4. **Federated AIR Detection**
   - Develop a federated learning approach for detecting AIR attacks across multiple LLM deployments
   - This could improve detection rates while preserving privacy and reducing the need for centralized data collection

5. **Explainable AIR Generation and Detection**
   - Incorporate explainable AI techniques to provide insights into how the AIR attack generates its prompts and how detection methods identify them
   - This could lead to more robust defense mechanisms and better understanding of LLM vulnerabilities