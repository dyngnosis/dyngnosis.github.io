#Tags
[[Research/Research Papers/2410.01294v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection

**Title:** Endless Jailbreaks with Bijection Learning
**Authors:** Brian R.Y. Huang, Maximilian Li, Leonard Tang
**Affiliation:** Haize Labs, New York City
**Publication Date:** October 2, 2024

Key Contributions:
- Introduction of bijection learning, a novel jailbreak attack paradigm
- Demonstration of the attack's effectiveness across multiple frontier language models
- Analysis of the attack's scaling properties, showing increased potency with larger models
- Exploration of the relationship between model capabilities and vulnerabilities

Problem Statement:
How to develop a scale-agnostic, automated, and universal jailbreak attack that exploits language models' advanced reasoning capabilities to bypass safety mechanisms?

Methodology:
1. Bijection Learning Attack:
   - Generate a bijective map from the English alphabet to a set of strings
   - Prompt the model with a multi-turn conversation teaching the mapping
   - Encode harmful queries using the mapping
   - Decode the model's response back to plaintext

2. Evaluation:
   - Tested on frontier models: Claude 3.5 Sonnet, Claude 3 Opus, Claude 3 Haiku, GPT-4o, GPT-4o-mini
   - Used AdvBench-50 and HarmBench datasets for harmful queries
   - Measured Attack Success Rate (ASR) with strict judging criteria
   - Explored various bijection types and fixed sizes

Main Results:
1. High ASRs across frontier models:
   - Claude 3.5 Sonnet: 86.3% on HarmBench
   - Claude 3 Haiku: 82.1% on HarmBench
   - GPT-4o-mini: 64.1% on HarmBench

2. Scale-agnostic properties:
   - Attack efficacy increases with model capabilities
   - Larger models are more susceptible to bijection learning jailbreaks

3. Universal attack potential:
   - Single, fixed mapping can effectively jailbreak multiple unsafe behaviors

Qualitative Analysis:
- Bijection learning exploits advanced reasoning capabilities of LLMs against themselves
- The attack reveals a trade-off between model capabilities and vulnerabilities
- Larger models produce more severe harmful behaviors more frequently under attack
- The study highlights the need for robust safety measures that scale with model capabilities

Limitations:
- Evaluation limited to closed-source models due to cost constraints
- Large context window required for the attack due to in-context examples
- Limited insight into non-English language settings and open-source models

Conclusion and Future Work:
- Bijection learning presents a significant challenge to current LLM safety measures
- The study emphasizes the need for scale-aware safety mechanisms in frontier models
- Future work should focus on developing robust defenses against capability-exploiting attacks
- Exploration of system-level defenses, such as equally capable output filters, is suggested

New Tools:
- No specific new tools or GitHub repositories mentioned in the paper