#Tags
[[Research/Research Papers/2401.06561v3.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** Intention Analysis Makes LLMs A Good Jailbreak Defender
**Authors:** Yuqi Zhang, Liang Ding, Lefei Zhang, Dacheng Tao
**Affiliations:** School of Computer Science, Wuhan University; School of Computer Science, The University of Sydney
**Publication Date:** January 12, 2024

Summary:
This paper introduces Intention Analysis (IA), a novel defense strategy against complex and stealthy jailbreak attacks on large language models (LLMs). IA is an inference-only method that enhances LLM safety without compromising helpfulness through a two-stage process: essential intention analysis and policy-aligned response.

Key Contributions:
- Introduction of IA as a new method to enhance LLM safety against sophisticated jailbreak attacks
- Development of a plug-and-play inference-only method that avoids the safety-helpfulness trade-off
- Demonstration of significant and consistent reduction in harmfulness of LLM outputs while maintaining helpfulness
- Achievement of state-of-the-art performance on several jailbreak benchmarks

Problem Statement:
The paper addresses the challenge of aligning LLMs with human values, particularly in the face of complex and stealthy jailbreak attacks that can manipulate LLMs to produce restricted or harmful outputs.

Methodology:
1. Two-stage Intention Analysis process:
   a) Essential intention analysis: LLM analyzes the user query's intention
   b) Policy-aligned response: LLM generates a response based on the analyzed intention
2. Experiments conducted on various LLMs: ChatGLM, LLaMA2, Vicuna, MPT, DeepSeek, and GPT-3.5
3. Evaluation using multiple jailbreak benchmarks: DAN, SAP200, DeepInception, GCG, and AutoDAN
4. Comparison with existing defense methods: ICD, Self-Reminder, and SmoothLLM

Main Results:
1. IA consistently reduced attack success rates (ASR) across different LLMs:
   - Average reduction of 53.1% in ASR
   - Vicuna-7B with IA outperformed GPT-3.5 in terms of ASR
2. IA maintained general helpfulness on harmless queries
3. IA demonstrated effectiveness against advanced jailbreak attacks like multilingual and encryption-based attacks

Qualitative Analysis:
- IA leverages LLMs' intrinsic intent recognition capabilities to enhance safety
- The method is effective across various model scales and alignment levels
- IA's success is attributed to its ability to handle complex and stealthy intentions in jailbreak attacks

Limitations:
- The study does not include GPT-4 due to budget constraints
- The jailbreak prompts used in the study may not encompass all potential real-world attacks

Conclusion and Future Work:
- IA significantly enhances LLM safety against jailbreak attacks while maintaining helpfulness
- Future work should focus on integrating intention analysis into training to reduce inference costs
- Development of more effective and robust defense strategies is needed to keep pace with advancements in adversarial attacks

Tools Introduced:
- Intention Analysis (IA) defense strategy
- GitHub repository: https://github.com/alphadl/SafeLLM_with_IntentionAnalysis

Figures and Tables:
1. Figure 1: Performance of IA on different LLMs, showing reduced Attack Success Rate against various jailbreak methods
2. Figure 2: Illustrated comparison of vanilla and IA approaches
3. Table 1: Comparison of IA and baseline methods across different LLMs and jailbreak attacks

## Repository Token Information
Total tokens in repository: 7404

Tokens per file:
- README.md: 499 tokens
- demo/IA_demo.py: 996 tokens
- demo/conversation.py: 5909 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: SafeLLM_with_IntentionAnalysis

## 1. Project Overview

The SafeLLM_with_IntentionAnalysis project implements the Intention Analysis (IA) defense strategy against jailbreak attacks on large language models (LLMs). The repository contains a demo implementation of the IA technique described in the research paper.

### Project Structure

- `README.md`: Project description and usage instructions
- `demo/IA_demo.py`: Main script demonstrating the IA technique
- `demo/conversation.py`: Utility module for managing conversation templates

## 2. Key Components and Functionality

### 2.1 Intention Analysis Prompts

The core of the IA technique lies in two carefully crafted prompts:

```python
IA_PROMPT = "Please identify the essential intention behind the following user query with a focus on safety, ethics, and legality. Ensure that your response begins with 'The essential intention of the query is.' Do not provide an actual answer to the user's query; instead, emphasize understanding and clarifying the user's intention while adhering to ethical guidelines. Following is the query you should focus on:\n"

CT_PROMPT = "Knowing its essential intention, now directly answer the aforementioned query with your normal response. Please make sure you strictly adhere to your own security restrictions."
```

These prompts guide the LLM through the two-stage IA process:
1. Analyzing the essential intention of the user's query
2. Generating a policy-aligned response based on the analyzed intention

### 2.2 Model and Tokenizer Initialization

The demo uses the Vicuna-7B model:

```python
model = LlamaForCausalLM.from_pretrained("/path/to/vicuna-7b-v1.1", from_tf=False).cuda('cuda:0')
tokenizer = LlamaTokenizer.from_pretrained("/path/to/vicuna-7b-v1.1", fast_tokenizer=True,)
```

### 2.3 Conversation Template

The `conversation.py` module provides a flexible system for managing different conversation templates. The demo uses the "vicuna_v1.1" template:

```python
conv = get_conv_template("vicuna_v1.1")
```

### 2.4 Two-Stage IA Process

The demo implements the two-stage IA process:

1. Intention Analysis:
```python
step1_query = f"{IA_PROMPT}'''\n{dan_query}\n'''"
step1_prompt = wrap_template(step1_query)
# ... (generate response)
```

2. Final Response:
```python
step2_prompt = wrap_template_step2(step1_query, step1_response)
# ... (generate response)
```

## 3. Relation to Research Paper Concepts

The demo directly implements the IA technique described in the paper:

- It uses the two-stage process of intention analysis followed by policy-aligned response.
- The prompts are designed to focus on safety, ethics, and legality, as emphasized in the paper.
- The demo showcases how IA can be applied to defend against a jailbreak attack (DAN in this case).

## 4. Notable Techniques

- Prompt Engineering: Carefully crafted prompts guide the LLM through the IA process.
- Two-Stage Processing: The query is processed twice, first for intention analysis and then for the final response.
- Template-based Conversation Management: The `conversation.py` module provides a flexible system for handling different conversation formats.

# Potential Enhancements

1. Benchmark Suite Implementation
   - Implement a comprehensive benchmark suite that includes all the jailbreak attacks mentioned in the paper (SAP200, DeepInception, GCG, AutoDAN).
   - This would allow for more thorough testing and comparison with the paper's results.

2. Multi-Model Support
   - Extend the demo to support multiple LLM architectures (e.g., GPT, BERT, T5) to demonstrate IA's versatility.
   - Implement model-specific optimizations to improve performance across different architectures.

3. Dynamic Prompt Generation
   - Develop a system that can dynamically generate or adjust the IA prompts based on the detected characteristics of the input query.
   - This could potentially improve the effectiveness of IA against novel or evolving jailbreak techniques.

4. Integration with Training Pipeline
   - As suggested in the paper's future work, integrate the IA technique into the model training process.
   - This could potentially reduce inference costs and further improve the model's inherent safety.

5. Real-time Attack Detection and Analysis
   - Implement a monitoring system that can detect potential jailbreak attempts in real-time.
   - Develop analytics tools to analyze attack patterns and automatically update defense strategies.