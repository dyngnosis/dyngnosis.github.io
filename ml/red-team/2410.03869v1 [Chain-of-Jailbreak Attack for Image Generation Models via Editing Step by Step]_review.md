#Tags
[[Research/Research Papers/2410.03869v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step
**Authors:** Wenxuan Wang, Kuiyi Gao, Zihan Jia, Youliang Yuan, Jen-tse Huang, Qiuzhi Liu, Shuai Wang, Wenxiang Jiao, Zhaopeng Tu
**Affiliations:** The Chinese University of Hong Kong, Tencent AI Lab, The Chinese University of Hong Kong (Shenzhen), The Hong Kong University of Science and Technology
**Publication Date:** October 4, 2024

Summary:
This paper introduces a novel jailbreaking method called Chain-of-Jailbreak (CoJ) attack for compromising image generation models through a step-by-step editing process. The method decomposes malicious queries into multiple sub-queries to bypass safeguards in widely-used image generation services.

Key Contributions:
- Introduction of the Chain-of-Jailbreak (CoJ) attack method
- Creation of CoJ-Bench, a comprehensive dataset for evaluating model safety
- Demonstration of CoJ attack effectiveness on GPT-4V, GPT-4o, Gemini 1.5, and Gemini 1.5 Pro
- Proposal of a defense method called Think Twice Prompting

Problem Statement:
How to assess and improve the safety of existing image generation models against jailbreaking attempts that aim to generate harmful content?

Methodology:
1. CoJ Attack Method:
   - Decompose malicious queries into multiple sub-queries
   - Use three basic edit operations: Insert, Delete, and Change
   - Apply edits to words, characters, or images

2. CoJ-Bench Dataset:
   - Covers 9 safety scenarios
   - Includes 3 types of editing operations
   - Incorporates 3 editing elements

3. Evaluation:
   - Tested on GPT-4V, GPT-4o, Gemini 1.5, and Gemini 1.5 Pro
   - Used human evaluation and automatic evaluation methods

4. Defense Method:
   - Proposed Think Twice Prompting technique

Main Results:
1. CoJ attack successfully bypassed safeguards in over 60% of cases
2. Significantly outperformed other jailbreaking methods (14% success rate)
3. Think Twice Prompting defense method successfully defended against 95% of CoJ attacks

Qualitative Analysis:
- The success of CoJ attack reveals significant vulnerabilities in current image generation models
- The step-by-step editing process allows for more subtle and effective bypassing of safeguards
- The effectiveness varies across different safety scenarios, with some being more vulnerable than others

Limitations:
- The study focuses on specific image generation services and may not generalize to all models
- The effectiveness of the attack may vary depending on the specific implementation of safeguards

Conclusion and Future Work:
- CoJ attack demonstrates a critical gap in existing safety mechanisms for image generation models
- The proposed Think Twice Prompting defense shows promise in mitigating these vulnerabilities
- Future work may include:
  1. Extending the attack and defense methods to other types of generative AI models
  2. Developing more robust safeguards against multi-step jailbreaking attempts
  3. Investigating the long-term implications of such attacks on AI safety and ethics

New Tools:
- CoJ-Bench: A comprehensive dataset for evaluating image generation model safety
- GitHub repository: https://docs.google.com/spreadsheets/d/1bevjLhc6RdT6_v-W7qf7HSV1-WYhfdAP1HkJZvx8x40/edit?usp=sharing