#Tags
[[Research/Research Papers/2305.12082v3.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0017/DevelopCapabilities
#AMLT0040/MLModelInferenceAPIAccess
#AMLT0042/VerifyAttack

**Title:** SneakyPrompt: Jailbreaking Text-to-image Generative Models
**Authors:** Yuchen Yang, Bo Hui, Haolin Yuan, Neil Gong, Yinzhi Cao
**Affiliations:** Johns Hopkins University, Duke University
**Publication Date:** May 20, 2023 (last updated November 10, 2023)

Key Contributions:
- First automated attack framework to jailbreak text-to-image generative models
- Successfully bypasses closed-box safety filters of DALL路E 2
- Outperforms existing text adversarial attacks in jailbreaking text-to-image models
- Open-source implementation available

Problem Statement:
How to bypass safety filters in text-to-image generative models to generate NSFW images, even when such filters are in place?

Methodology:
1. SneakyPrompt framework:
   - Uses reinforcement learning to guide token perturbation
   - Repeatedly queries the text-to-image model
   - Strategically perturbs tokens in the prompt based on query results
2. Evaluation on:
   - Closed-box safety filter: DALL路E 2
   - Open-source safety filters: Deployed on Stable Diffusion model
3. Comparison with existing text adversarial attacks
4. Datasets:
   - NSFW-200: 200 target prompts with NSFW content
   - Dog/Cat-100: 100 prompts describing scenarios with dogs or cats

Main Results:
1. Successfully jailbreaks DALL路E 2 with a 57.15% one-time bypass rate
2. Achieves 100% bypass rate on 4 out of 6 safety filters for Stable Diffusion
3. Outperforms existing text adversarial attacks in terms of:
   - Number of queries required
   - Quality of generated NSFW images (measured by FID score)
4. Effective against various types of safety filters:
   - Text-based
   - Image-based
   - Text-image-based

Qualitative Analysis:
- SneakyPrompt demonstrates the vulnerability of current safety filters in text-to-image models
- The success of the attack highlights the need for more robust safety measures in AI-generated content
- The framework's ability to maintain image semantics while bypassing filters shows sophisticated evasion techniques

Limitations:
- Ethical concerns regarding the generation of NSFW content
- Limited evaluation on closed-box safety filters (only DALL路E 2)
- Potential for misuse in generating harmful or inappropriate content

Conclusion and Future Work:
- SneakyPrompt effectively jailbreaks text-to-image models' safety filters
- Highlights the urgent need for new guardrails to limit societal harms of text-to-image models
- Future work suggestions:
  1. Developing more robust safety filters
  2. Exploring adversarial training for safety filters
  3. Investigating methods to edit model parameters to erase sensitive concepts intrinsically

New Tool:
Name: SneakyPrompt
GitHub Repository: https://github.com/Yuchen413/text2image_safety

## Repository Token Information
Total tokens in repository: 15116

Tokens per file:
- evaluate.py: 830 tokens
- search_utils.py: 1667 tokens
- model_utils.py: 450 tokens
- rl_utils.py: 1655 tokens
- rl_search.py: 3857 tokens
- test.py: 58 tokens
- main.py: 1563 tokens
- text2image_pipeline.py: 2690 tokens
- baseline/heuristic_search.py: 1428 tokens
- baseline/baseline.py: 245 tokens
- baseline/SafetyCheckerWrapper.py: 673 tokens


## Tutorial and Enhancement Suggestions

# SneakyPrompt: Jailbreaking Text-to-Image Generative Models

## Tutorial

### Project Overview

SneakyPrompt is an automated attack framework designed to jailbreak text-to-image generative models by bypassing their safety filters. The project implements a reinforcement learning-based approach to perturb input prompts, allowing the generation of NSFW (Not Safe For Work) images despite content restrictions.

### Project Structure

The repository is organized into several key components:

1. Main execution files:
   - `main.py`: Entry point for running the attack
   - `evaluate.py`: Evaluation script for the attack results

2. Core functionality:
   - `rl_search.py`: Implements the reinforcement learning-based search algorithm
   - `text2image_pipeline.py`: Handles the text-to-image model pipeline
   - `search_utils.py`: Utility functions for the search process

3. Model and utility files:
   - `model_utils.py`: Contains model-related utility functions
   - `rl_utils.py`: Reinforcement learning utility functions

4. Baseline methods:
   - `baseline/heuristic_search.py`: Implements baseline search methods
   - `baseline/baseline.py`: Runs baseline attacks
   - `baseline/SafetyCheckerWrapper.py`: Wrapper for safety checker models

### Key Components and Functionality

#### 1. Reinforcement Learning Search (`rl_search.py`)

The core of the SneakyPrompt attack is implemented in the `MPA_agent` class within `rl_search.py`. This class uses reinforcement learning to guide the token perturbation process. Key features include:

- Policy network (`p_pi`) for action selection
- `select_action` and `select_combo` methods for generating perturbed prompts
- `reinforcement_learn` method that implements the learning loop

#### 2. Text-to-Image Pipeline (`text2image_pipeline.py`)

This file contains two main classes:

- `SDPipeline`: Wrapper for the Stable Diffusion model
- `DL2Pipeline`: Wrapper for the DALL-E 2 API

These classes handle the interaction with the text-to-image models, including prompt processing, image generation, and safety checking.

#### 3. Search Utilities (`search_utils.py`)

This file provides various utility functions for the search process, including:

- `get_embedding`: Generates text embeddings
- `get_nsfw_match` and `get_nsfw_dl`: Detects NSFW content in prompts
- `get_nsfw_word_index`: Identifies sensitive words in prompts
- `get_adv_prompt`: Generates adversarial prompts

#### 4. Main Execution (`main.py`)

The `main.py` file orchestrates the entire attack process. It handles:

- Argument parsing
- Model and pipeline initialization
- Execution of the chosen search method (RL, greedy, beam, or brute force)
- Result logging and image saving

### Relation to Research Paper Concepts

The implementation closely follows the concepts discussed in the research paper:

1. Reinforcement Learning Approach: The `MPA_agent` class implements the RL-based token perturbation strategy described in the paper.

2. Safety Filter Bypassing: The `SDPipeline` and `DL2Pipeline` classes incorporate different safety checking mechanisms, allowing the attack to be tested against various filters.

3. Prompt Perturbation: The `get_adv_prompt` function in `search_utils.py` implements the token replacement strategy discussed in the paper.

4. Evaluation Metrics: The code tracks metrics such as bypass rate, number of queries, and image similarity, as mentioned in the paper's evaluation section.

### Notable Algorithms and Techniques

1. Actor-Critic Reinforcement Learning: Implemented in the `p_pi` class within `rl_search.py`.

2. CLIP-based NSFW Detection: Used in the `image_clip_check` method of `SDPipeline` class.

3. Beam Search and Greedy Search: Implemented as baseline methods in `baseline/heuristic_search.py`.

4. Cosine Similarity for Safety Checking: Used in the `base_check` method of `SDPipeline` class.

## Potential Enhancements

1. Multi-Model Generalization
   - Extend the framework to support a wider range of text-to-image models beyond Stable Diffusion and DALL-E 2.
   - Implement a modular architecture that allows easy integration of new models and safety filters.

2. Advanced Reinforcement Learning Techniques
   - Incorporate more sophisticated RL algorithms such as Proximal Policy Optimization (PPO) or Soft Actor-Critic (SAC) to potentially improve the efficiency of the search process.
   - Experiment with different reward shaping techniques to guide the RL agent more effectively.

3. Semantic Preservation Metrics
   - Develop and integrate more advanced metrics to ensure that the perturbed prompts maintain semantic similarity to the original prompts.
   - Explore the use of large language models to evaluate the semantic coherence of generated prompts.

4. Adversarial Training for Robust Safety Filters
   - Implement an adversarial training pipeline that uses the generated adversarial prompts to improve the robustness of safety filters.
   - Develop a cyclical training process where the attack model and safety filter are iteratively improved.

5. Ethical Considerations and Controllable Generation
   - Integrate ethical guidelines and content policies into the framework to ensure responsible use of the technology.
   - Develop fine-grained control mechanisms that allow for the generation of non-harmful content while still bypassing overly restrictive filters.