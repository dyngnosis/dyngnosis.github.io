#Tags
[[Research/Research Papers/2408.15221v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0051/LLMPromptInjection
#AMLT0031/ErodeMLModelIntegrity
#AMLT0043/CraftAdversarialData

**Title:** LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet
**Authors:** Nathaniel Li, Ziwen Han, Ian Steneker, Willow Primack, Riley Goodside, Hugh Zhang, Zifan Wang, Cristina Menghini, Summer Yue
**Affiliations:** Scale AI, UC Berkeley
**Publication Date:** September 4, 2024

Key Contributions:
- Demonstrated vulnerabilities in LLM defenses against multi-turn human jailbreaks
- Achieved 70%+ attack success rate (ASR) on HarmBench against defenses reporting single-digit ASRs with automated single-turn attacks
- Revealed vulnerabilities in machine unlearning defenses, recovering dual-use biosecurity knowledge
- Compiled Multi-Turn Human Jailbreaks (MHJ) dataset with 2,912 prompts across 537 multi-turn jailbreaks
- Released a taxonomy of jailbreak tactics developed from commercial red teaming engagements

Problem Statement:
Current LLM defenses are primarily evaluated against single-turn automated adversarial attacks, which is an insufficient threat model for real-world malicious use. The paper investigates the effectiveness of multi-turn human jailbreaks against existing LLM defenses.

Methodology:
1. Developed a multi-stage human red teaming pipeline for jailbreaking LLMs
2. Evaluated four LLM defenses: RR, LAT, DERTA, and CYGNET
3. Compared human jailbreaks with six automated attacks on HarmBench dataset
4. Tested effectiveness against machine unlearning methods using WMDP-Bio benchmark
5. Compiled successful jailbreaks into the MHJ dataset with associated metadata and tactics

Main Results:
1. Multi-turn human jailbreaks significantly outperformed automated attacks across all defenses:
   - 19% to 65% higher ASR than ensemble of automated attacks on HarmBench
   - 70.4% ASR against CYGNET, which reported 0% ASR for prior attacks
2. Human jailbreaks successfully recovered dual-use biosecurity knowledge from unlearned models
3. Identified vulnerabilities in existing defenses against multi-turn attacks

Qualitative Analysis:
- The study highlights the importance of considering multi-turn interactions in LLM threat models
- Human red teaming reveals vulnerabilities that automated attacks miss, emphasizing the need for more comprehensive robustness evaluations
- The effectiveness of human jailbreaks suggests that current defenses may be overfitted to single-turn automated attacks

Limitations:
- Differences in setup and threat model between human and automated attacks
- Potential variations in individual red teamer skills and experience
- High cost of human red teaming compared to automated attacks

Conclusion and Future Work:
- Current LLM defenses are not robust against multi-turn human jailbreaks
- The paper calls for stronger LLM defenses and more rigorous robustness evaluations
- Future work should focus on developing automated attacks that better mimic human multi-turn jailbreaking strategies

New Tools:
Multi-Turn Human Jailbreaks (MHJ) dataset: https://scale.com/research/mhj

Relevant Figures:
Figure 1: Comparison of attack success rates (ASR) between human and automated attacks against different LLM defenses on HarmBench behaviors.

Figure 3: Detailed breakdown of attack success rates for human and automated attacks across different defenses and HarmBench semantic categories.