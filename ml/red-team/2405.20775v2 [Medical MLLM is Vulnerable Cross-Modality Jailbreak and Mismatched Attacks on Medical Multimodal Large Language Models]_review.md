#Tags
[[Research/Research Papers/2405.20775v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0031/ErodeMLModelIntegrity
#AMLT0054/LLMJailbreak

**Title:** Medical MLLM is Vulnerable: Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models

**Authors:** Xijie Huang, Xinyuan Wang, Haotao Zhang, Yinghao Zhu, Jiawen Xi, Jingkun An, Hao Wang, Hao Liang, Chengwei Pan

**Affiliations:** Beihang University, University of Science and Technology of China, Peking University

**Publication Date:** May 26, 2024 (preprint)

Summary:
This paper investigates security vulnerabilities in Medical Multimodal Large Language Models (MedMLLMs), focusing on jailbreak attacks and mismatched inputs. The authors introduce new attack methods and a comprehensive dataset to evaluate MedMLLM robustness.

Key Contributions:
- Definition of mismatched malicious attack (2M-attack) and optimized mismatched malicious attack (O2M-attack)
- Creation of the 3MAD dataset for evaluating MedMLLM vulnerabilities
- Development of the Multimodal Cross-optimization Methodology (MCM) for jailbreaking MedMLLMs
- Comprehensive evaluation of attack methods on state-of-the-art MedMLLMs

Problem Statement:
The security implications of Multimodal Large Language Models in medical contexts (MedMLLMs) are insufficiently studied, particularly when deployed in clinical environments where accuracy and relevance are critical.

Methodology:
1. Dataset Creation:
   - Constructed 3MAD dataset covering various medical image modalities and harmful scenarios
   - Used GPT-4 to generate prompts paired with relevant images

2. Attack Methods:
   - 2M-attack: Injecting inputs with mismatched images and query attributes
   - O2M-attack: Applying jailbreak optimization methods before injection

3. Evaluation:
   - White-box attacks on LLaVA-Med
   - Transfer attacks (black-box) on four other SOTA models
   - Metrics: Attack Success Rate (ASR), Refusal Rate (RR), semantic similarity

4. MCM Optimization:
   - Iterative enhancement of adversarial strength for both image and text modalities
   - Cross-modal evaluation to select the most effective perturbations

Main Results:
1. MCM method achieved highest ASR in both Malicious attacks (0.8157) and 2M-attacks (0.8204)
2. MCM demonstrated lowest Refusal Rate (0.0074) in Mismatched attacks
3. 2M-attacks and O2M-attacks increased attack success rates by 10%-20% across four state-of-the-art MedMLLMs
4. Even MedMLLMs designed with enhanced security features remain vulnerable to these attacks

Qualitative Analysis:
- The study reveals that clinical mismatches and malicious queries can effectively jailbreak MedMLLMs
- The success of these attacks highlights the need for improved security measures in medical AI systems
- The vulnerability of MedMLLMs to mismatched inputs suggests a potential weakness in handling real-world clinical scenarios with data inconsistencies

Limitations:
- The study focuses on common clinical domains and may not cover all specialized areas of medicine
- The effectiveness of the proposed attacks on more recent or proprietary MedMLLMs is not explored

Conclusion and Future Work:
- The paper underscores the urgent need for robust security measures in open-source MedMLLMs
- Future work should focus on developing defensive strategies against mismatched and malicious inputs
- Enhancing the alignment between different modalities in MedMLLMs is crucial for improving their robustness

Tools Introduced:
- 3MAD dataset: A comprehensive dataset for evaluating MedMLLM vulnerabilities
- MCM optimization method: A novel approach for jailbreaking MedMLLMs
- GitHub repository: https://github.com/dirtycomputer/O2M_attack

## Repository Token Information
Total tokens in repository: 12314

Tokens per file:
- llava/model/llava.py: 3064 tokens
- llava/attacks/attacks.py: 2117 tokens
- llava/gcg/opti_utils.py: 2069 tokens
- run_jailbreak.py: 1911 tokens
- llava/gcg/conversation.py: 1105 tokens
- metric.py: 666 tokens
- llava/gcg/string_utils.py: 503 tokens
- api/gpt.py: 262 tokens
- llava/utils.py: 234 tokens
- llava/model/utils.py: 183 tokens
- api/gemini.py: 131 tokens
- api/util.py: 40 tokens
- llava/model/__init__.py: 15 tokens
- llava/__init__.py: 14 tokens
- api/__init__.py: 0 tokens
- llava/gcg/__init__.py: 0 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: Medical MLLM Vulnerability Analysis

## 1. Project Overview

This repository contains the implementation of the research paper "Medical MLLM is Vulnerable: Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models". The project aims to explore and demonstrate vulnerabilities in Medical Multimodal Large Language Models (MedMLLMs) through various attack methods.

### Project Structure

The repository is organized into several key directories:

- `llava/`: Contains the core model implementation and attack utilities
- `api/`: Includes API interfaces for external services like GPT and Gemini
- `run_jailbreak.py`: The main script for executing attacks
- `metric.py`: Implements similarity calculation metrics

## 2. Key Components

### 2.1 LLaVA Model (`llava/model/llava.py`)

This file contains the implementation of the LLaVA (Large Language and Vision Assistant) model, which is the base for the MedMLLM being attacked.

Key classes:
- `LlavaLlamaModel`: Extends the Llama model with vision capabilities
- `LlavaLlamaForCausalLM`: Implements the causal language modeling head on top of the LLaVA model

### 2.2 Attack Implementation (`llava/attacks/attacks.py`)

This file contains the core attack methods:

- `gcg_attack`: Implements the Gradient-based Controlled Generation attack
- `pgd_attack`: Implements the Projected Gradient Descent attack
- `run_mcm_attack`: Executes the Multimodal Cross-optimization Methodology (MCM) attack

### 2.3 Optimization Utilities (`llava/gcg/opti_utils.py`)

Contains utility functions for optimizing attacks, including:

- `token_gradients`: Calculates gradients for token optimization
- `sample_control`: Samples control tokens based on gradients
- `check_for_attack_success`: Evaluates if an attack was successful

### 2.4 Main Execution Script (`run_jailbreak.py`)

This script orchestrates the entire attack process, including:

- Loading the model and dataset
- Setting up attack parameters
- Executing attacks and logging results

## 3. Relation to Research Concepts

### 3.1 Mismatched Malicious Attack (2M-attack)

The code implements the 2M-attack concept through the combination of mismatched image-text pairs and malicious prompts. This is evident in the `run_jailbreak.py` script where both malicious and mismatched scenarios are set up.

### 3.2 Multimodal Cross-optimization Methodology (MCM)

The MCM attack is implemented in the `run_mcm_attack` function within `attacks.py`. It alternates between text-based (GCG) and image-based (PGD) attacks to optimize across modalities.

### 3.3 Attack Evaluation

The code uses metrics like Attack Success Rate (ASR) and Refusal Rate (RR) to evaluate attack effectiveness, aligning with the paper's methodology. These are calculated based on the model's responses to crafted inputs.

## 4. Notable Algorithms and Techniques

### 4.1 Gradient-based Controlled Generation (GCG)

Implemented in `gcg_attack`, this method uses token gradients to optimize adversarial suffixes that guide the model towards generating target content.

### 4.2 Projected Gradient Descent (PGD)

The `pgd_attack` function implements PGD to generate adversarial perturbations on input images, staying within a specified epsilon bound.

### 4.3 Cross-modal Optimization

The MCM attack (`run_mcm_attack`) alternates between text and image perturbations, selecting the most effective approach at each step. This novel technique addresses the multimodal nature of MedMLLMs.

# Potential Enhancements

1. **Adaptive Attack Strategies**
   - Implement a dynamic attack selection mechanism that chooses the most effective attack method based on real-time performance metrics.
   - This could involve developing a reinforcement learning agent that learns to select optimal attack strategies for different input types.

2. **Robustness Across Medical Specialties**
   - Extend the 3MAD dataset to cover a broader range of medical specialties and rare conditions.
   - Implement specialized attack methods tailored to different medical domains (e.g., radiology, pathology, dermatology).

3. **Defense Mechanism Integration**
   - Develop and integrate defensive techniques to counter the implemented attacks.
   - This could include adversarial training, input sanitization, or multi-modal consistency checks.

4. **Explainable AI for Attack Analysis**
   - Incorporate explainable AI techniques to visualize and interpret why certain attacks succeed or fail.
   - This could help in understanding model vulnerabilities and guide the development of more robust MedMLLMs.

5. **Real-time Attack Detection**
   - Implement a parallel system that monitors model inputs and outputs in real-time to detect potential attack attempts.
   - This could involve developing anomaly detection algorithms specific to medical content and multimodal inputs.