#Tags
[[Research/Research Papers/2402.10260v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0054/LLMJailbreak
#AMLT0042/VerifyAttack

**Title:** A StrongREJECT for Empty Jailbreaks
**Authors:** Alexandra Souly, Qingyuan Lu, Dillon Bowen, Tu Trinh, Elvis Hsieh, Sana Pandey, Pieter Abbeel, Justin Svegliato, Scott Emmons, Olivia Watkins, Sam Toyer
**Affiliation:** Center for Human-Compatible AI, UC Berkeley
**Publication Date:** February 15, 2024 (arXiv preprint)

Summary:
This paper introduces StrongREJECT, a new benchmark for evaluating the effectiveness of jailbreak attacks on large language models (LLMs). The authors argue that existing jailbreak evaluation methods often overstate attack success rates and propose a more rigorous approach to assess jailbreak performance.

Key Contributions:
- Introduction of the StrongREJECT benchmark, including a dataset of high-quality forbidden prompts and an automated evaluator
- Demonstration that StrongREJECT achieves state-of-the-art agreement with human judgments of jailbreak effectiveness
- Discovery of a novel phenomenon: jailbreaks that bypass safety fine-tuning tend to reduce model capabilities

Problem Statement:
Jailbreak researchers lack a standardized, high-quality benchmark for evaluating jailbreak performance, leading to exaggerated claims of effectiveness and hindering progress in the field.

Methodology:
1. Dataset creation:
   - Curated 313 high-quality forbidden prompts across six categories of harmful behavior
   - Filtered prompts for distinctness, refusal by multiple models, and answerability
   
2. Automated evaluator development:
   - Rubric-based evaluator using GPT-4o-mini
   - Fine-tuned open-source evaluator based on Gemma 2B
   
3. Human evaluation:
   - Recruited labelers to grade responses from four victim models to a 60-prompt subset
   - Compared automated evaluators to human judgments
   
4. Jailbreak effectiveness analysis:
   - Tested 37 jailbreak methods on three aligned LLMs
   - Conducted experiments to isolate the effect of jailbreaks on model capabilities

Main Results:
1. StrongREJECT automated evaluator achieves higher agreement with human judges compared to existing methods
2. Most jailbreaks tested did not result in high-quality responses to forbidden prompts
3. Jailbreaks that increase a model's willingness to respond to forbidden prompts tend to decrease its capabilities

Qualitative Analysis:
- The authors suggest that the discrepancy between StrongREJECT and previous evaluators is due to StrongREJECT considering both willingness and capabilities, while most previous evaluators over-emphasize willingness
- The discovery that jailbreaks harm model capabilities is a novel and surprising finding that has implications for the development and assessment of jailbreak techniques

Limitations:
1. Dataset size (313 prompts) balances cost and runtime against comprehensiveness
2. Focus on LLMs may not generalize to multimodal models
3. Forbidden prompts may not be robust to changes in providers' terms of service

Conclusion and Future Work:
- StrongREJECT provides a more accurate and comprehensive benchmark for evaluating jailbreak effectiveness
- The authors emphasize the need for researchers to use high-quality benchmarks like StrongREJECT when developing new jailbreak attacks
- Future work may involve expanding the dataset and adapting the benchmark for multimodal models

Tools Introduced:
- StrongREJECT benchmark, including:
  1. Dataset of 313 high-quality forbidden prompts
  2. Rubric-based automated evaluator
  3. Fine-tuned open-source evaluator based on Gemma 2B
- GitHub repository: https://strong-reject.readthedocs.io/