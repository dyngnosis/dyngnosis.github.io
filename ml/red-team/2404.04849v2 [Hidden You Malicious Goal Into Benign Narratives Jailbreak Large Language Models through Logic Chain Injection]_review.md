#Tags
[[Research/Research Papers/2404.04849v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0043/CraftAdversarialData
#AMLT0015/EvadeMLModel

**Title:** Hidden You Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Logic Chain Injection
**Authors:** Zhilong Wang, Yebo Cao, Peng Liu
**Affiliations:** The Pennsylvania State University, Carnegie Mellon University
**Publication Date:** April 16, 2024

Summary:
This paper introduces a novel jailbreak attack method called "Logic-Chain Injection" that can deceive both Large Language Models (LLMs) and human reviewers. The attack embeds malicious intentions into benign narratives, making it difficult to detect through traditional defense mechanisms.

Key Contributions:
- Introduces the Logic-Chain Injection attack method
- Demonstrates the ability to deceive both LLMs and humans
- Provides a formalization of the attack method
- Presents two attack instances: Paragraphed Logic Chain and "Acrostic" Style Logic Chain

Problem Statement:
Existing jailbreak attacks on LLMs can successfully deceive the models but are easily detectable by humans. The research aims to develop a more sophisticated attack method that can evade both LLM safeguards and human detection.

Methodology:
1. Disassemble malicious query into semantically equivalent benign narrations
2. Embed disassembled logic-chain into a benign article
3. Ensure LLM can connect scattered logic through strategic placement
4. Utilize writing techniques like paragraph structure and acrostic-style formatting

Main Results:
- Successfully demonstrated the Logic-Chain Injection attack on ChatGPT
- Achieved jailbreaking without revealing the hidden prompt to the LLM
- Showed that the attack can manipulate LLMs while remaining undetected by the model itself

Qualitative Analysis:
- The attack leverages social psychology insights, hiding lies within truth
- Draws parallels with sophisticated software attacks like Return-Oriented Programming
- Exploits LLMs' ability to connect logically related information across a document

Limitations:
- Reproducibility may be affected by rapid evolution of LLM-integrated applications
- Effectiveness may vary depending on the specific LLM and its training

Conclusion and Future Work:
The paper introduces a novel and more advanced jailbreak attack method that poses significant challenges for both LLM security measures and human reviewers. Future work should focus on developing robust defenses against such sophisticated attacks.

New Tools:
No specific new tools or GitHub repositories were mentioned in the paper.