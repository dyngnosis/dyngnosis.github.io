#Tags
[[Research/Research Papers/2408.09326v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0042/VerifyAttack

**Title:** Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks
**Authors:** Kexin Chen, Yi Liu, Dongxia Wang, Jiaying Chen, Wenhai Wang
**Affiliations:** Zhejiang University, Nanyang Technological University
**Publication Date:** August 18, 2024

Summary:
This paper introduces a comprehensive evaluation framework to assess the reliability of Large Language Models (LLMs) against jailbreak attacks. The study examines 10 jailbreak strategies across 3 categories, using 1525 questions from 61 harmful categories to test 13 popular LLMs. Multiple metrics are used to evaluate LLM outputs, including Attack Success Rate, Toxicity Score, Fluency, Token Length, and Grammatical Errors.

Key Contributions:
- Comprehensive evaluation framework for assessing LLM reliability against jailbreak attacks
- Large-scale empirical study covering 13 popular LLMs and 10 jailbreak strategies
- Multi-dimensional analysis using various metrics to assess LLM outputs
- Normalized and aggregated reliability scores for different LLMs
- Analysis of relationships between models, attack strategies, and harmful content types

Problem Statement:
The study addresses the challenge of assessing LLMs' robustness against jailbreak attacks, which can manipulate models to produce harmful or unethical content despite implemented safeguards.

Methodology:
1. Dataset Construction:
   - Expanded an existing dataset to 1525 queries across 61 specific harm categories
   - Used a three-level hierarchical taxonomy for comprehensive categorization

2. Jailbreak Attack Strategies:
   - Implemented 10 strategies across 3 categories: Manual Crafting, Longtail Encoding, and Prompt Refinement

3. LLMs Evaluated:
   - Tested 13 popular LLMs, including GPT-3.5-Turbo, GPT-4, LLaMA2, Vicuna, Mistral, Baichuan2, Gemma, and Llama-3

4. Evaluation Metrics:
   - Attack Success Rate (ASR)
   - Toxicity Score (toxic, obscene, insult, threat)
   - Fluency (measured by perplexity)
   - Token Length
   - Grammatical Errors

5. Metrics Aggregation:
   - Normalized metrics to a 0-1 range
   - Calculated weighted average for overall reliability score

Main Results:
1. Jailbreak vulnerability varies across LLMs and attack strategies
2. Prompt Refinement-based attacks generally yielded higher ASR
3. GPT-4 showed low toxicity and relatively low ASR across categories
4. Llama3 demonstrated low ASR, low toxicity, and few grammatical errors
5. Vicuna and Mistral were found to be most vulnerable to jailbreak attacks

Qualitative Analysis:
- The study reveals that even well-aligned models like GPT-3.5 and GPT-4 are not entirely resistant to harmful queries
- Longtail Encoding-based attacks showed model-specific effectiveness, suggesting that more advanced LLMs may have better recognition capabilities for low-resource languages or encoded text
- The effectiveness of human-crafted jailbreak prompts highlights the importance of actively collecting and analyzing such prompts for improving LLM safety

Limitations:
- The study did not extend to larger models (33B and 70B parameters) or some commercial models like Claude and Gemini due to resource constraints
- The assessment of jailbreak success remains a challenging issue, relying on automated evaluators which may have limitations in accuracy

Conclusion and Future Work:
The study emphasizes the need for continued focus on improving the reliability of LLMs against jailbreak attacks. It provides a framework for evaluating LLM robustness and highlights areas where current models fall short. Future work may involve extending the evaluation to larger models and refining the assessment methods for jailbreak success.

Relevant Figures/Tables:
- Table 3: ASR results for different jailbreak strategies across LLMs
- Table 4: Evaluation metrics of responses with jailbreak attacks on LLMs
- Figure 3: Heatmaps showing relationships between jailbreak attacks, harm types, and target models

New Tools:
The paper does not introduce new tools, but it presents a comprehensive evaluation framework that could be adapted for future LLM security assessments.