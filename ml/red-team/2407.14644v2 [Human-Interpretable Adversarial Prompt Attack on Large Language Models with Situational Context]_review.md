#Tags
[[Research/Research Papers/2407.14644v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection
#AMLT0054/LLMJailbreak

**Title:** Human-Interpretable Adversarial Prompt Attack on Large Language Models with Situational Context
**Authors:** Nilanjana Das, Edward Raff, Manas Gaur
**Affiliations:** UMBC, MD, USA; Booz Allen Hamilton
**Publication Date:** July 25, 2024

Summary:
This paper explores a novel approach to creating human-interpretable adversarial prompts for attacking Large Language Models (LLMs) using situational context. The authors develop a method to convert nonsensical adversarial suffixes into sensible prompts using situation-driven contextual re-writing, demonstrating successful attacks on both open-source and proprietary LLMs.

Key Contributions:
- Development of a situation-driven contextual adversarial prompt attack
- Conversion of nonsensical adversarial suffixes to human-readable prompts without gradient information
- Demonstration of attack transferability between different LLMs
- Successful attacks with as few as one attempt across various LLMs

Problem Statement:
Previous research on LLM vulnerabilities focused on nonsensical prompt injections, which are easily detectable. This study addresses the need for exploring innocuous, human-understandable malicious prompts augmented with adversarial injections.

Methodology:
1. Generate adversarial suffixes using existing methods
2. Convert suffixes to human-interpretable adversarial insertions using GPT-3.5
3. Combine adversarial insertions with malicious prompts and situational contexts from movie overviews
4. Test attacks on various LLMs using few-shot chain-of-thought prompting
5. Evaluate harmfulness of responses using GPT-4 Judge

Datasets:
- IMDB top 1000 movies dataset (focusing on crime genre)

Main Results:
- Successful attacks on both open-source (e.g., Llama-2, phi-1.5) and proprietary (e.g., GPT-3.5, GPT-4) LLMs
- Attacks transfer between different LLMs
- As few as one demonstration is sufficient for successful attacks on some LLMs
- GPT-4 showed more robustness, requiring up to 10 demonstrations for a maximum harmfulness score of 4

Qualitative Analysis:
- The study demonstrates that human-readable adversarial prompts can be generated without access to model weights or gradient information
- The use of situational context (movie overviews) makes the prompts more innocuous and harder to detect
- The success of the attacks across different LLMs suggests a common vulnerability in current language models

Limitations:
- The human-interpretable adversarial insertion is rigid, and changes in words may render it ineffective
- Success rates for the latest LLMs (e.g., Claude 3) need further investigation
- Some LLMs (e.g., gemma-7b, Llama-3-8B) do not always follow instructions precisely

Conclusion and Future Work:
The paper demonstrates the feasibility of creating human-interpretable adversarial prompts for attacking LLMs using situational context. The authors suggest that there is room for improvement in the safety and robustness of most open-source and proprietary LLMs. Future work may focus on enhancing the flexibility of adversarial insertions and improving success rates on the latest LLM iterations.

Tools Introduced:
- No specific new tools were introduced in this paper. The authors used existing frameworks and models, including PromptBench, GPT-3.5, GPT-4, and various open-source LLMs.