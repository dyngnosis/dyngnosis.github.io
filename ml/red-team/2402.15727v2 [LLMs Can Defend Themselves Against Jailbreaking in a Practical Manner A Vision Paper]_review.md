#Tags
[[Research/Research Papers/2402.15727v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner: A Vision Paper
**Authors:** Daoyuan Wu, Shuai Wang, Yang Liu, Ning Liu
**Affiliations:** Nanyang Technological University, Hong Kong University of Science and Technology, City University of Hong Kong
**Publication Date:** 24 Feb 2024 (initial submission to arXiv)

Summary:
This vision paper proposes SELFDEFEND, a lightweight and practical defense mechanism against jailbreak attacks on large language models (LLMs). The authors demonstrate that existing LLMs can effectively recognize harmful prompts, which forms the basis of their defense strategy.

Key Contributions:
- Proposal of SELFDEFEND, a generic LLM jailbreak defense mechanism
- Demonstration of SELFDEFEND's effectiveness against various jailbreak scenarios
- Identification of future research directions to enhance SELFDEFEND

Problem Statement:
Jailbreaking attacks bypass safety alignments in LLMs, posing security risks. While numerous attack strategies exist, defensive measures are relatively unexplored.

Methodology:
- Manual analysis of SELFDEFEND using GPT-3.5 and GPT-4
- Testing against three categories of jailbreak attacks: GCG, template-based, and multilingual
- Design of a shadow stack architecture for concurrent checking of harmful prompts

Main Results:
- SELFDEFEND can defend against all existing jailbreak attacks with minimal delay
- Existing LLMs (GPT-3.5/4) can effectively recognize harmful prompts
- The proposed shadow stack architecture allows for concurrent checking and checkpoint triggering

Qualitative Analysis:
- SELFDEFEND offers a practical approach to jailbreak defense, addressing limitations of existing methods
- The method's ability to generate explainable LLM responses to adversarial prompts enhances its utility
- The approach leverages existing LLM capabilities, potentially reducing implementation complexity

Limitations:
- Current evaluation is based on manual analysis; more extensive empirical studies are needed
- The approach may not handle pure multimodal jailbreaks without text prompts

Future Work:
1. Design a low-cost, fast, and robust LLM for recognizing harmful prompts
2. Use discovered adversarial examples to further align LLMs
3. Develop mechanisms to reduce/cache the calling of the shadow stack
4. Extend SELFDEFEND to support defense against multimodal jailbreak attacks

Relevant Figures:
Figure 1: Overview of SELFDEFEND and its three future research directions
Figure 2: Motivating example showing successful jailbreak and effective identification of harmful prompt

New Tools:
SELFDEFEND: A proposed lightweight jailbreak defense mechanism for LLMs (no GitHub repository mentioned)