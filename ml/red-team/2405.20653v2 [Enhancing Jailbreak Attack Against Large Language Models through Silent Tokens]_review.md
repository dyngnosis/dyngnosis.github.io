#Tags
[[Research/Research Papers/2405.20653v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0031/ErodeMLModelIntegrity

**Title:** Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens
**Authors:** Jiahao Yu, Haozheng Luo, Jerry Yao-Chieh Hu, Wenbo Guo, Han Liu, Xinyu Xing
**Affiliations:** Northwestern University, University of California Santa Barbara
**Publication Date:** June 4, 2024 (last updated)

Key Contributions:
- Introduction of BOOST: a simple jailbreak attack using eos tokens
- Demonstration of BOOST's effectiveness in enhancing existing jailbreak methods
- Analysis of how eos tokens affect LLM's ethical boundary and attention mechanism

Problem Statement:
How to develop a simple and effective jailbreak attack against Large Language Models (LLMs) that can bypass safety alignments and enhance existing jailbreak methods?

Methodology:
1. Developed BOOST (Enhanced JailBreak Of Large Language Model via Silent eos Tokens)
2. Applied BOOST to four representative jailbreak methods: GCG, GPTFuzzer, In-Context-Attack, and Competing Objectives
3. Evaluated BOOST on 12 LLMs, including Llama-2, Qwen, and Gemma
4. Analyzed the impact of eos tokens on LLM's ethical boundary and attention mechanism
5. Visualized hidden representations and attention values

Main Results:
1. BOOST significantly improved attack success rates across various LLMs
2. Adding eos tokens shifted hidden representations of harmful prompts towards benign prompts
3. eos tokens had lower attention values compared to other tokens, minimizing distraction

Qualitative Analysis:
- BOOST exploits a vulnerability in LLMs' safety alignment by manipulating the ethical boundary learned during fine-tuning
- The effectiveness of BOOST highlights the fragility of current LLM safety measures
- The simplicity of BOOST (adding eos tokens) makes it a potential threat for real-world applications

Limitations:
- Proprietary LLMs may filter out eos tokens, reducing BOOST's effectiveness
- The optimal number of eos tokens can vary, requiring some trial and error

Conclusion and Future Work:
- BOOST demonstrates a significant vulnerability in LLM safety mechanisms
- Future research should focus on developing more robust safety alignment approaches
- Potential defenses include filtering eos tokens and incorporating them in red teaming fine-tuning

Relevant Figures:
- Figure 2: Visualization of hidden representations for benign and harmful prompts
- Figure 3: Visualization of jailbreak prompts generated by different methods
- Figure 4: Visualization of hidden representation shifts caused by adding eos tokens
- Figure 5: Comparison of attention values for eos tokens and GCG tokens

New Tools:
- BOOST: A simple jailbreak attack method using eos tokens (no GitHub repository mentioned)