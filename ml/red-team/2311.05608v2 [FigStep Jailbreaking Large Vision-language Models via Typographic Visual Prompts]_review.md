#Tags
[[Research/Research Papers/2311.05608v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0054/LLMJailbreak
#AMLT0043/CraftAdversarialData
#AMLT0040/MLModelInferenceAPIAccess

**Title:** FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts
**Authors:** Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, Xiaoyun Wang
**Affiliations:** Tsinghua University, Shandong University, Carnegie Mellon University
**Publication Date:** November 9, 2023 (v1), December 13, 2023 (v2)

Summary:
This paper introduces FigStep, a jailbreaking algorithm that exploits vulnerabilities in large vision-language models (VLMs) by converting harmful textual instructions into typographic images. The method bypasses safety alignments in the underlying language models, demonstrating that VLMs are susceptible to attacks that leverage their multi-modal capabilities.

Key Contributions:
- Introduction of FigStep, a simple yet effective jailbreaking algorithm for VLMs
- Comprehensive evaluation of FigStep on multiple open-source VLMs
- Demonstration of FigStep's effectiveness against GPT-4V
- Creation of SafeBench, a benchmark for evaluating VLM safety

Problem Statement:
The safety of VLMs lacks systematic evaluation, and there may be overconfidence in the safety guarantees provided by their underlying language models. The paper aims to demonstrate that introducing additional modality modules leads to unforeseen AI safety issues.

Methodology:
1. FigStep Algorithm:
   - Paraphrase harmful questions into declarative statements
   - Convert statements into typographic images
   - Use benign inciting text prompts to stimulate VLM responses
2. Evaluation:
   - Created SafeBench, a dataset of 500 harmful queries across 10 topics
   - Tested on 6 open-source VLMs from 3 families: LLaVA-v1.5, MiniGPT4, and CogVLM
   - Manual review of 46,500 model responses
3. GPT-4V Attack:
   - Developed FigStep-Pro to bypass OCR detection in GPT-4V

Main Results:
- FigStep achieved an average attack success rate (ASR) of 82.50% across 6 open-source VLMs
- Baseline text-only queries had an average ASR of 44.80%
- FigStep-Pro achieved a 70% ASR against GPT-4V, compared to 34% for the original FigStep

Qualitative Analysis:
- The effectiveness of FigStep demonstrates a lack of cross-modal safety alignment in VLMs
- The high success rate suggests that visual inputs introduce new vulnerabilities to content safety mechanisms
- The ability to jailbreak GPT-4V indicates that even advanced VLMs with additional safety measures are susceptible to attack

Limitations:
- The study focuses on open-source VLMs and one closed-source model (GPT-4V)
- The evaluation relies on manual review, which may introduce some subjectivity
- The attack's effectiveness may vary depending on the specific implementation of safety measures in different VLMs

Conclusion and Future Work:
- VLMs are vulnerable to jailbreaking attacks that exploit their multi-modal capabilities
- There is a critical need for novel safety alignment approaches that consider both visual and textual modalities
- Future work should focus on developing robust cross-modal safety mechanisms for VLMs

Relevant Figures:
- Figure 1: Illustration of how FigStep bypasses safety alignment in VLMs
- Figure 2: Pipeline of the FigStep algorithm
- Figure 3: Attack success rates across different forbidden AI topics
- Figure 10: Example of FigStep-Pro jailbreaking GPT-4V

New Tools:
- FigStep: Jailbreaking algorithm for VLMs
- FigStep-Pro: Enhanced version of FigStep for attacking GPT-4V
- SafeBench: Benchmark dataset for evaluating VLM safety

GitHub Repository: https://github.com/ThuCCSLab/FigStep

## Repository Token Information
Total tokens in repository: 2895

Tokens per file:
- README.md: 2055 tokens
- src/generate_prompts.py: 628 tokens
- src/README.md: 69 tokens
- data/README.md: 139 tokens
- data/images/README.md: 1 tokens
- data/images/FigStep-Pro/README.md: 1 tokens
- data/images/SafeBench-Tiny/README.md: 1 tokens
- data/images/SafeBench/README.md: 1 tokens


## Tutorial and Enhancement Suggestions

# FigStep: A Tutorial and Enhancement Proposal

## 1. Tutorial

### 1.1 Project Overview

FigStep is a novel approach to jailbreaking large vision-language models (VLMs) using typographic visual prompts. The project structure is as follows:

```
FigStep/
├── src/
│   ├── generate_prompts.py
│   └── README.md
├── data/
│   ├── images/
│   │   ├── FigStep-Pro/
│   │   ├── SafeBench-Tiny/
│   │   └── SafeBench/
│   ├── question/
│   └── README.md
└── README.md
```

The main components are:
- `src/`: Contains the source code for generating prompts
- `data/`: Stores datasets and images used in the experiments
- `README.md`: Provides an overview of the project, methodology, and results

### 1.2 Key Components and Functionality

#### 1.2.1 generate_prompts.py

This script is the core of the FigStep algorithm. It contains functions to:

1. Load and process images
2. Convert text to images
3. Generate different types of queries

Key functions:

- `text_to_image(text: str)`: Converts a given text into an image
- `text_step_by_step(text: str, steps=3, wrap=False)`: Formats text into a numbered list
- `gen_query(query_type, question, instruction)`: Generates different types of queries based on the specified type

The `QueryType` enum defines various query types:
- `prompt_6`: A text-based prompt with empty numbered items
- `figstep`: The main FigStep query type, combining text and image
- `baseline`: A simple text-only query
- `prompt_5`: A text-only numbered list

### 1.3 Relation to Research Concepts

The code implements the core concepts discussed in the research paper:

1. **Typographic Visual Prompts**: The `text_to_image()` function creates the typographic images used to bypass text-based safety measures.

2. **Query Generation**: The `gen_query()` function implements different query types, including the FigStep method and baseline approaches.

3. **Jailbreaking Technique**: By combining benign text prompts with harmful instructions encoded in images, the code realizes the jailbreaking technique described in the paper.

### 1.4 Notable Algorithms and Techniques

1. **Text-to-Image Conversion**: The script uses the PIL (Python Imaging Library) to render text as images, which is crucial for creating the typographic visual prompts.

2. **Text Wrapping**: The `wrap_text()` function ensures that long texts fit within the image boundaries, maintaining readability.

3. **Query Type Enumeration**: The `QueryType` enum provides a structured way to generate different types of queries, facilitating experimentation and comparison.

## 2. Potential Enhancements

### 2.1 Improve Efficiency and Scalability

The current implementation generates prompts on-the-fly. To enhance performance, especially for large-scale experiments:

- Implement caching mechanisms for generated images
- Parallelize image generation and query processing
- Optimize image rendering for faster text-to-image conversion

### 2.2 Extend Functionality to More VLMs

The current code focuses on a limited set of VLMs. To broaden the research:

- Develop adapters for additional VLMs, including proprietary models
- Create a modular architecture that allows easy integration of new models
- Implement a standardized evaluation pipeline for comparing results across different VLMs

### 2.3 Enhance Visual Prompt Generation

To make the jailbreaking technique more robust and harder to detect:

- Implement advanced image processing techniques to obfuscate text while maintaining readability for VLMs
- Explore using GANs or other generative models to create more natural-looking images with embedded text
- Develop methods to dynamically adjust visual prompts based on model responses

### 2.4 Incorporate Adversarial Learning

To continuously improve the jailbreaking technique:

- Implement an adversarial learning framework where the jailbreaking method evolves to counter improving safety measures
- Develop a self-improving system that learns from successful and failed jailbreaking attempts
- Create a benchmark suite that automatically evaluates and ranks different jailbreaking techniques

### 2.5 Ethical Considerations and Safety Research

To balance the research with responsible AI development:

- Implement a "safety mode" that demonstrates how to detect and prevent FigStep-like attacks
- Develop tools for auditing VLMs for vulnerabilities to multi-modal jailbreaking attempts
- Create educational modules to raise awareness about the potential risks and mitigation strategies for VLM safety