#Tags
[[Research/Research Papers/2403.09792v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection

**Title:** Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models
**Authors:** Yifan Li, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen
**Affiliations:** Renmin University of China, Beijing Key Laboratory of Big Data Management and Analysis Methods
**Publication Date:** March 14, 2024 (arXiv)

Summary:
This paper investigates the harmlessness alignment problem in multimodal large language models (MLLMs) and proposes a novel jailbreak method called HADES to exploit visual vulnerabilities in these models.

Key Contributions:
- Systematic empirical analysis of harmlessness performance in MLLMs
- Identification of image input as a key alignment vulnerability in MLLMs
- Development of HADES, a jailbreak method using crafted images
- Demonstration of high attack success rates on popular MLLMs

Problem Statement:
The research addresses the vulnerability of MLLMs to harmful content when processing image inputs, despite being trained for harmlessness alignment.

Methodology:
1. Data Collection:
   - Created a dataset of 750 harmful instructions across 5 scenarios
   - Paired instructions with relevant harmful images
   
2. Evaluation Settings:
   - Tested open-source (LLaVA-1.5, MiniGPT-v2, MiniGPT-4) and closed-source (Gemini Pro Vision, GPT-4V) MLLMs
   - Four evaluation settings: Backbone, Text-only, Blank, and Toxic

3. HADES Jailbreak Method:
   - Step 1: Hiding harmfulness from text to image using typography
   - Step 2: Amplifying image harmfulness with LLMs and diffusion models
   - Step 3: Amplifying image harmfulness with gradient updates

Main Results:
1. Images can be alignment backdoors for MLLMs
2. Cross-modal finetuning undermines alignment abilities of backbone LLMs
3. Harmfulness of MLLM responses correlates with image content harmfulness
4. HADES achieves high attack success rates:
   - 90.26% on LLaVA-1.5
   - 71.60% on Gemini Pro Vision

Qualitative Analysis:
- The visual modality introduces additional alignment vulnerabilities in MLLMs
- Existing MLLMs struggle to resist harmful image inputs
- The effectiveness of HADES varies across different harmful categories

Limitations:
- The study focuses on a limited set of MLLMs and harmful scenarios
- The effectiveness of HADES may vary depending on the OCR capabilities of the target model

Conclusion and Future Work:
- The paper demonstrates that the visual modality poses a critical alignment vulnerability in MLLMs
- Future work should focus on developing cross-modal alignment methods to enhance MLLM harmlessness

Relevant Figures:
- Figure 1: Example showing the influence of visual modality on harmlessness alignment
- Figure 2: Overview of the HADES jailbreak approach

New Tool:
HADES (Hiding and Amplifying harmfulness in images to DEStroy multimodal alignment)
GitHub: https://github.com/RUCAIBox/HADES