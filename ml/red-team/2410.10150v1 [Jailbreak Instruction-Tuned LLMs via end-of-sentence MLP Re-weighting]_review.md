# Tags
[[Research/Research Papers/2410.10150v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0054/LLMJailbreak
#AMLT0043/CraftAdversarialData
#AMLT0044/FullMLModelAccess

**Title:** Jailbreak Instruction-Tuned LLMs via end-of-sentence MLP Re-weighting

**Authors:** Yifan Luo, Zhennan Zhou, Meitan Wang, Bin Dong

**Affiliations:** Peking University, Westlake University

**Publication date:** October 14, 2024 (under review for ICLR 2025)

## Key Contributions:
- Discovery of the critical role of MLP layers in end-of-sentence inferences for LLM safety mechanisms
- Development of two novel white-box jailbreak methods: prompt-specific and prompt-general
- Demonstration of the methods' effectiveness across 7 popular open-source LLMs (2B to 72B parameters)
- Insights into vulnerabilities of instruction-tuned LLM safety mechanisms

## Problem Statement:
The study investigates the safety mechanisms of instruction fine-tuned large language models (LLMs) and explores methods to compromise these safety constraints.

## Methodology:
1. MLP Re-weighting: Introduce re-weighting factors for MLP neurons, focusing on end-of-sentence inferences
2. Prompt-specific method: Optimizes independent MLP re-weighting factors for individual target prompts
3. Prompt-general method: Pre-trains MLP factors on a dataset to generalize to unseen harmful prompts
4. Evaluation: Tested on 7 open-source LLMs using HarmBench dataset and compared with other jailbreak methods

## Main Results:
1. Re-weighting MLP neurons in end-of-sentence inferences significantly compromises model safety
2. Prompt-specific method outperforms state-of-the-art approaches with less computational time
3. Prompt-general method is comparable to state-of-the-art and has a smaller impact on original model capabilities
4. High attack success rates (ASR) achieved across various LLMs (e.g., 96.9% for LLaMA-3 8B with prompt-specific method)

## Qualitative Analysis:
- The study reveals that LLMs likely evaluate prompt harmfulness during end-of-sentence inferences
- MLP layers play a crucial role in safety mechanisms, particularly in these specific inferences
- The effectiveness of minimal modifications to end-of-sentence MLPs suggests a vulnerability in current safety designs

## Limitations:
- The approach is heuristic-driven and may not represent an optimal solution
- The exact role of MLP layers in the safety mechanism remains unclear
- Ethical concerns about potential misuse of the jailbreak methods

## Conclusion and Future Work:
- The study provides new insights into LLM safety mechanisms and their vulnerabilities
- Proposed methods contribute to understanding mechanism interpretability in LLMs
- Future work should focus on developing more robust and transparent AI systems based on these findings

## Relevant Figures:
- Figure 1: Examples of MLP re-weighting factors and corresponding responses
- Figure 2: MLP factors and distribution for LLaMA-3 8B-Instruct
- Table 1: Attack success rates comparison with other jailbreak methods
- Table 2: Model performance comparison before and after MLP re-weighting

## New Tools:
No specific new tools or GitHub repositories were mentioned in the paper.
