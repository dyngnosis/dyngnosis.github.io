#Tags
[[Research/Research Papers/2407.16686v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** Can Large Language Models Automatically Jailbreak GPT-4V?
**Authors:** Yuanwei Wu, Yue Huang, Yixin Liu, Xiang Li, Pan Zhou, Lichao Sun
**Affiliations:** Huazhong University of Science and Technology, Lehigh University, University of Notre Dame
**Publication Date:** July 23, 2024

Summary:
This paper introduces AutoJailbreak, an innovative technique that leverages Large Language Models (LLMs) to automatically generate jailbreak prompts for GPT-4V. The method achieves a high Attack Success Rate (ASR) of 95.3%, surpassing conventional jailbreak methods.

Key Contributions:
- Introduction of AutoJailbreak, an automated jailbreak strategy using LLMs
- Integration of weak-to-strong in-context learning for prompt optimization
- Development of an efficient search mechanism with early stopping
- Demonstration of high effectiveness in facial identity recognition tasks across multiple languages

Problem Statement:
The study addresses the vulnerability of GPT-4V to jailbreak attacks, particularly in the context of facial recognition and privacy concerns.

Methodology:
1. Prompt Pool Construction: Generate initial jailbreak prompts using LLMs
2. Prompt Evaluation: Use GPT-4V to score prompts based on Recognition Success Rate (RSR)
3. Weak-to-strong Contrastive Prompting: Refine prompts using a novel learning approach
4. Combination Injection: Enhance prompts with prefix injection, refusal suppression, and length control
5. Efficient Search with Hypothesis Testing: Implement early stopping to reduce optimization time and token usage

Main Results:
- AutoJailbreak achieved an ASR exceeding 95.3% on facial identity recognition tasks
- The method outperformed baseline attacks, including combination injection and adversarial image attacks
- GPT-4 as a red-team model produced more effective jailbreak prompts compared to GPT-3.5
- The weak-to-strong template showed superior performance over traditional in-context learning

Qualitative Analysis:
- The study reveals a potential bias in GPT-4V's training dataset, with higher recognition rates for Hollywood celebrities compared to Asian celebrities
- Semantic analysis of jailbreak prompts shows a gradual transition in semantics correlating with shifts in RSR
- The research highlights the vulnerability of MLLMs to sophisticated jailbreak attacks, emphasizing the need for improved security measures

Limitations:
- The study focuses primarily on facial recognition tasks and may not generalize to other MLLM capabilities
- The method relies on access to powerful language models like GPT-4 for generating jailbreak prompts
- The approach may be computationally expensive and time-consuming

Conclusion and Future Work:
The paper demonstrates the effectiveness of using LLMs for automated jailbreak attacks on GPT-4V, highlighting the urgent need for enhanced security measures in MLLMs. Future work may include:
- Expanding the scope of attacks to various jailbreak tasks
- Developing cost-effective defense strategies against automated jailbreak attempts
- Investigating the generalizability of the method to other MLLMs

Relevant Figures:
Figure 2: Framework of AutoJailbreak, illustrating the three-stage process of prompt pool construction, evaluation, and weak-to-strong contrastive prompting.

New Tools:
AutoJailbreak: An automated jailbreak technique using LLMs for prompt optimization (no GitHub repository mentioned)