#Tags
[[Research/Research Papers/2404.05880v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0018/BackdoorMLModel
#AMLT0031/ErodeMLModelIntegrity
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge
**Authors:** Weikai Lu, Ziqian Zeng, Jianwei Wang, Zhengdong Lu, Zelin Chen, Huiping Zhuang, Cen Chen
**Affiliations:** South China University of Technology, China; Pazhou Laboratory, China
**Publication Date:** April 8, 2024

Summary:
This paper introduces Eraser, a novel defense method against jailbreaking attacks on Large Language Models (LLMs). Eraser aims to unlearn harmful knowledge, retain general knowledge, and maintain safety alignment in LLMs.

Key Contributions:
- Proposes a method to achieve three goals: unlearning harmful knowledge, retaining general knowledge, and maintaining safety alignment
- Demonstrates superior defense capabilities while maintaining general performance compared to existing methods
- Shows that unlearning random data can achieve defense capabilities, providing insights for future jailbreak defense research

Problem Statement:
Existing jailbreaking defense methods fail to address the fundamental issue of harmful knowledge residing within LLMs, leading to potential jailbreak risks.

Methodology:
1. Unlearning harmful knowledge:
   - Use gradient ascent on harmful answers in simulated jailbreaking mode
   - Utilize publicly available uncensored models to obtain harmful answers
   - Add random prefixes and suffixes to simulate jailbreaking attack scenarios

2. Retaining general knowledge:
   - Preserve the model's ability to understand entities
   - Generate entity-related comprehension questions using GPT-3.5
   - Perform distillation on next word prediction

3. Maintaining safety alignment:
   - Encourage the model to maintain refusal capabilities for harmful queries

Experimental Setup:
- Base model: Llama2-chat-7b
- Datasets: AdvBench (520 harmful queries) and AdvExtent (417 samples)
- Attack methods: AIM, AutoDAN, GCG
- Baselines: RSFT, GAM
- Evaluation metrics: Harmfulness score, Attack Success Rate (ASR), general capability benchmarks

Main Results:
1. Eraser significantly reduces ASR and harmfulness scores across all attack methods and datasets
2. Eraser maintains comparable general performance on 7 benchmarks, unlike baselines that show performance degradation
3. Eraser achieves a better trade-off between harmlessness and usefulness compared to existing methods

Qualitative Analysis:
- Eraser's responses to jailbreaking prompts are more responsible and similar to the base model for general instructions
- The method of unlearning random data shows promising defense effects but at the cost of general performance degradation

Limitations:
- Eraser is only applicable to LLMs that have undergone safety alignment
- The method is inefficient as it only defends against specific harmful issues
- Enumerating all harmful issues remains challenging

Conclusion and Future Work:
- Eraser successfully reduces jailbreaking risks without compromising general capabilities
- Future research should focus on maintaining general capabilities while improving defense mechanisms
- The authors encourage more researchers to contribute to developing ethical and responsible LLMs

Tools Introduced:
- Eraser: A jailbreaking defense method for LLMs
- GitHub repository: https://github.com/ZeroNLP/Eraser

## Repository Token Information
Total tokens in repository: 15502

Tokens per file:
- environment.yaml: 3052 tokens
- README.md: 492 tokens
- train_Eraser/train.py: 1211 tokens
- train_Eraser/config/train_agrs.json: 394 tokens
- train_Eraser/utils/Datasets.py: 2258 tokens
- train_Eraser/utils/Llama_trainer.py: 1665 tokens
- train_Eraser/utils/DataCollator.py: 1071 tokens
- Evaluate_defense_capability/start/judge_prompt.txt: 1309 tokens
- Evaluate_defense_capability/start/test.py: 95 tokens
- Evaluate_defense_capability/start/AIM.py: 1537 tokens
- generate_data/generate_harm_data.py: 381 tokens
- generate_data/generate_help_data.py: 819 tokens
- generate_data/generate_algn_data.py: 538 tokens
- generate_data/entity_extraction_prompt.txt: 51 tokens
- generate_data/entity_extraction.py: 459 tokens
- models/Eraser_Llama2_7b_Lora/adapter_config.json: 170 tokens


## Tutorial and Enhancement Suggestions

# Eraser: Jailbreaking Defense in Large Language Models

## Tutorial

### Project Overview

The Eraser project aims to develop a defense mechanism against jailbreaking attacks on Large Language Models (LLMs). The repository contains code for training the Eraser model, generating necessary datasets, and evaluating the model's defense capabilities.

### Project Structure

1. `environment.yaml`: Conda environment configuration file
2. `README.md`: Project documentation
3. `train_Eraser/`: Directory containing training-related code
4. `Evaluate_defense_capability/`: Directory for evaluating the model's defense capabilities
5. `generate_data/`: Scripts for generating training data
6. `models/`: Directory for storing model files

### Key Components

#### 1. Data Generation

The `generate_data/` directory contains scripts for creating three types of datasets:

- `generate_harm_data.py`: Generates harmful data using an uncensored LLM
- `generate_help_data.py`: Creates helpful, non-harmful data
- `generate_algn_data.py`: Produces alignment data for maintaining safety
- `entity_extraction.py`: Extracts named entities from generated data

These scripts implement the data generation process described in the methodology section of the paper, creating datasets for unlearning harmful knowledge, retaining general knowledge, and maintaining safety alignment.

#### 2. Model Training

The `train_Eraser/train.py` script is the main entry point for training the Eraser model. It uses the following key components:

- `LlamaTrainer` (in `utils/Llama_trainer.py`): A custom trainer class that implements the Eraser training algorithm
- `TrainDataset` and `EvalDataset` (in `utils/Datasets.py`): Custom dataset classes for handling the three types of training data
- `DataCollatorForSeq2Seq` (in `utils/DataCollator.py`): A data collator for efficiently batching the training data

The training process uses LoRA (Low-Rank Adaptation) for efficient fine-tuning of the base Llama2 model. The `compute_loss` method in `LlamaTrainer` implements the core Eraser algorithm, which includes:

1. Unlearning harmful knowledge through gradient ascent
2. Retaining general knowledge via distillation
3. Maintaining safety alignment

#### 3. Evaluation

The `Evaluate_defense_capability/start/AIM.py` script evaluates the model's defense capabilities against the AIM (Always Intelligent and Machiavellian) attack. It uses OpenAI's GPT-3.5 model to judge the harmfulness of the model's responses.

### Notable Techniques

1. **LoRA Fine-tuning**: The project uses LoRA for efficient adaptation of the base Llama2 model, allowing for quick training of the defense mechanism.

2. **Gradient Ascent for Unlearning**: The `compute_loss` method in `LlamaTrainer` implements gradient ascent on harmful answers to unlearn harmful knowledge.

3. **Knowledge Distillation**: The training process includes a distillation component to retain general knowledge while unlearning harmful information.

4. **Multi-task Training**: The model is trained simultaneously on harmful, helpful, and alignment data to achieve a balance between defense and general capabilities.

## Potential Enhancements

1. **Improved Data Generation**
   - Implement more sophisticated techniques for generating harmful data, such as using multiple uncensored models or incorporating human feedback.
   - Develop a more robust entity extraction process, possibly using named entity recognition models instead of relying on GPT-3.5.

2. **Enhanced Unlearning Algorithm**
   - Explore alternative unlearning techniques, such as selective forgetting or targeted fine-tuning, to improve efficiency and effectiveness.
   - Implement adaptive unlearning rates based on the harmfulness of the content being unlearned.

3. **Expanded Evaluation Framework**
   - Develop a more comprehensive evaluation suite that includes a wider range of jailbreaking attacks and safety benchmarks.
   - Implement automated evaluation metrics to reduce reliance on external APIs for judging harmfulness.

4. **Transfer Learning and Cross-model Applicability**
   - Investigate the transferability of the Eraser defense mechanism to other LLM architectures beyond Llama2.
   - Develop techniques for applying the Eraser method to already fine-tuned models without requiring access to the original training data.

5. **Integration with Continuous Learning**
   - Develop a system for continuously updating the Eraser defense as new harmful content or attack methods emerge.
   - Implement a feedback loop that incorporates real-world usage data to improve the model's defense capabilities over time.