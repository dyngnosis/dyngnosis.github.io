#Tags
[[Research/Research Papers/2408.08464v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0040/MLModelInferenceAPIAccess

**Title:** MMJ-Bench: A Comprehensive Study on Jailbreak Attacks and Defenses for Vision Language Models
**Authors:** Fenghua Weng, Yue Xu, Chengyan Fu, Wenjie Wang
**Affiliation:** ShanghaiTech University, China
**Publication Date:** September 4, 2024

Summary:
This paper introduces MMJ-Bench, a unified pipeline for evaluating jailbreak attacks and defense techniques for Vision Language Models (VLMs). The study assesses the effectiveness of various attack methods against state-of-the-art VLMs and evaluates the impact of defense mechanisms on both defense effectiveness and model utility for normal tasks.

Key Contributions:
- Proposal of MMJ-Bench, a systematic and unified pipeline for comprehensive evaluation of existing jailbreak attacks and defense techniques in VLMs.
- Extensive experimental results comparing attack and defense methods systematically.
- Development and public release of the first benchmark for VLM jailbreak research.

Problem Statement:
The paper addresses the lack of unified and comprehensive evaluations of jailbreak attacks and defenses for VLMs, as current methods use different datasets, target models, and evaluation metrics.

Methodology:
1. Data Collection: Using HarmBench for harmful queries and MM-Vet for normal tasks evaluation.
2. Jailbreak Cases Generation: Evaluating six attack methods (3 generation-based, 3 optimization-based).
3. Response Generation: Testing on six open-sourced VLMs from 4 popular model families.
4. Evaluation: Using GPT-4 and a binary classifier from HarmBench to determine harmful content.

Main Results and Findings:
1. Attack Effectiveness:
   - Effectiveness varies among VLMs.
   - Generation-based attacks are more effective according to GPT-4 evaluator.
   - Optimization-based techniques perform better according to HarmBench classifier.
   - No VLM is uniformly robust to all jailbreak attacks.

2. Defense Effectiveness:
   - VLGuard is the most effective defense across all attacks.
   - AdaShield-A and CIDER are effective for specific attack types.
   - Detection-based defenses (JailGuard and CIDER) negatively impact VLM utility on regular tasks.

Qualitative Analysis:
- The study highlights the challenge of developing a universal defense method that balances model utility and defense effectiveness for all VLMs.
- The findings underscore the need to account for inherent diversities among VLMs when designing universal defense strategies.

Limitations:
- The study focuses on a limited set of attack and defense methods.
- Evaluation is conducted on a specific set of VLMs, which may not represent all possible model architectures and training approaches.

Conclusion and Future Work:
The paper concludes by emphasizing the importance of continuous updates to MMJ-bench with new attacks and defenses to advance the development of safer and more secure VLMs.

Tools Introduced:
MMJ-Bench: A unified pipeline for evaluating jailbreak attacks and defense techniques for VLMs.
GitHub Repository: https://github.com/thunxxx/MLLM-Jailbreak-evaluation-MMJ-bench

## Repository Token Information
Total tokens in repository: 200000

Tokens per file:
- multimodalmodels/qwen/modeling_qwen.py: 9830 tokens
- multimodalmodels/minigpt4/minigpt4/models/Qformer.py: 9583 tokens
- attacks/xidian/utils/minigpt4/models/Qformer.py: 9583 tokens
- attacks/xidian/utils/torchattacks/attacks/_differential_evolution.py: 8822 tokens
- attacks/xidian/utils/torchattacks/attacks/fab.py: 8271 tokens
- multimodalmodels/qwen/tokenization_qwen.py: 5139 tokens
- attacks/xidian/utils/minigpt4/models/eva_vit.py: 5045 tokens
- attacks/xidian/utils/minigpt4/models/.ipynb_checkpoints/eva_vit-checkpoint.py: 5041 tokens
- multimodalmodels/minigpt4/minigpt4/models/eva_vit.py: 5032 tokens
- attacks/xidian/utils/torchattacks/attacks/square.py: 4668 tokens
- multimodalmodels/minigpt4/minigpt4/runners/runner_base.py: 4646 tokens
- attacks/xidian/utils/minigpt4/runners/runner_base.py: 4622 tokens
- attacks/xidian/utils/torchattacks/attack_noise.py: 4417 tokens
- attacks/xidian/utils/torchattacks/attack.py: 4379 tokens
- attacks/xidian/xidian.py: 4136 tokens
- multimodalmodels/qwen/openai_api.py: 4002 tokens
- multimodalmodels/minigpt4/minigpt4/datasets/builders/image_text_pair_builder.py: 3874 tokens
- multimodalmodels/minigpt4/minigpt4/models/minigpt_base.py: 3821 tokens
- attacks/xidian/utils/minigpt4/models/minigpt_base.py: 3803 tokens
- multimodalmodels/qwen/visual.py: 3791 tokens
- attacks/xidian/utils/minigpt4/models/.ipynb_checkpoints/minigpt_base-checkpoint.py: 3791 tokens
- attacks/xidian/utils/minigpt4/common/config_1.py: 3686 tokens
- eval_utils.py: 3680 tokens
- attacks/xidian/utils/minigpt4/datasets/builders/image_text_pair_builder.py: 3639 tokens
- attacks/xidian/utils/torchattacks/attacks/apgd.py: 3548 tokens
- multimodalmodels/minigpt4/minigpt4/datasets/datasets/coco_dataset.py: 3487 tokens
- attacks/xidian/utils/minigpt4/datasets/datasets/coco_dataset.py: 3481 tokens
- attacks/xidian/utils/torchattacks/attacks/apgdt.py: 3457 tokens
- multimodalmodels/qwen/qwen_generation_utils.py: 3333 tokens
- multimodalmodels/minigpt4/minigpt4/processors/randaugment.py: 3297 tokens
- attacks/xidian/utils/minigpt4/processors/randaugment.py: 3297 tokens
- attacks/xidian/utils/minigpt4/common/config.py: 3294 tokens
- multimodalmodels/minigpt4/minigpt4/common/config.py: 3288 tokens
- multimodalmodels/minigpt4/minigpt4/common/utils.py: 3169 tokens
- attacks/xidian/utils/minigpt4/common/utils.py: 3166 tokens
- attacks/xidian/utils/torchattacks/attacks/pixle.py: 3140 tokens
- attacks/baseline.py: 3066 tokens
- multimodalmodels/qwen/finetune.py: 2957 tokens
- multimodalmodels/minigpt4/minigpt4/common/vqa_tools/vqa_eval.py: 2912 tokens
- attacks/xidian/utils/minigpt4/common/vqa_tools/vqa_eval.py: 2912 tokens
- multimodalmodels/minigpt4/minigpt4/common/vqa_tools/VQA/PythonEvaluationTools/vqaEvaluation/vqaEval.py: 2834 tokens
- attacks/xidian/utils/minigpt4/common/vqa_tools/VQA/PythonEvaluationTools/vqaEvaluation/vqaEval.py: 2834 tokens
- attacks/qr/QR.py: 2677 tokens
- attacks/hades/utils.py: 2403 tokens
- multimodalmodels/minigpt4/minigpt_utils/text_attacker.py: 2389 tokens
- multimodalmodels/qwen/web_demo_mm.py: 2382 tokens
- attacks/xidian/utils/torchattacks/attacks/eaden.py: 2290 tokens
- multimodalmodels/minigpt4/minigpt_utils/visual_attacker.py: 2288 tokens
- attacks/xidian/utils/torchattacks/attacks/eadl1.py: 798 tokens
