#Tags
[[Research/Research Papers/2405.21018v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** Improved Techniques for Optimization-Based Jailbreaking on Large Language Models
**Authors:** Xiaojun Jia, Tianyu Pang, Chao Du, Yihao Huang, Jindong Gu, Yang Liu, Xiaochun Cao, Min Lin
**Publication Date:** May 31, 2024

Key Contributions:
- Introduced diverse target templates with harmful self-suggestion and guidance to improve jailbreak effectiveness
- Proposed an automatic multi-coordinate updating strategy to accelerate convergence
- Developed an easy-to-hard initialization technique to boost jailbreak efficiency
- Combined these improvements into an efficient jailbreak method called I-GCG

Problem Statement:
The paper addresses the limitations of existing optimization-based jailbreak methods, particularly the Greedy Coordinate Gradient (GCG) attack, which has unsatisfactory attacking efficiency and limited jailbreak performance due to its simple optimization objectives.

Methodology:
1. Harmful Guidance: Incorporated harmful information into the optimization goal to guide jailbreak attacks
2. Automatic Multi-Coordinate Updating Strategy: Adaptively decides how many tokens to replace in each step
3. Easy-to-Hard Initialization: Generates jailbreak suffix for simple harmful requests first, then uses it as initialization for more challenging requests
4. Evaluation: Tested on AdvBench benchmark and NeurIPS 2023 Red Teaming Track using various LLMs (VICUNA-7B-1.5, GUANACO-7B, LLAMA2-7B-CHAT, MISTRAL-7B-INSTRUCT-0.2)

Main Results:
- I-GCG achieved nearly 100% attack success rate across all tested LLMs
- Outperformed state-of-the-art jailbreaking attacks, including GCG, MAC, AutoDAN, and others
- Demonstrated improved jailbreak efficiency and effectiveness compared to baseline methods
- Achieved 100% attack success rate on NeurIPS 2023 Red Teaming Track

Qualitative Analysis:
- The incorporation of harmful guidance in the optimization goal facilitates finding the jailbreak space, enhancing overall performance
- The automatic multi-coordinate updating strategy and easy-to-hard initialization significantly improve jailbreak efficiency
- The proposed method shows strong transferability, performing well even on closed-source LLMs like ChatGPT-3.5 and ChatGPT-4

Limitations:
- The method may have potential for misuse in generating harmful content
- There is still room for improvement in jailbreak transferability
- The approach relies on white-box access to LLMs, which may not always be available in real-world scenarios

Conclusion and Future Work:
The paper presents I-GCG as an efficient jailbreak method that significantly improves upon existing optimization-based techniques. The authors suggest further exploration of better harmful guidance design and more general suffix initialization techniques. They also note the potential for improving jailbreak transferability with additional methods.

Tools Introduced:
I-GCG (Improved Greedy Coordinate Gradient) - An efficient jailbreak method combining harmful guidance, automatic multi-coordinate updating, and easy-to-hard initialization.
GitHub Repository: https://github.com/jiaxiaojunQAQ/I-GCG

## Repository Token Information
Total tokens in repository: 42328

Tokens per file:
- generate_our_config.py: 190 tokens
- run_single_attack_base.py: 322 tokens
- attack_llm_core_base.py: 2070 tokens
- behaviors_ours_config.json: 5448 tokens
- main.py: 128 tokens
- attack_llm_core_best_update_our_target.py: 3111 tokens
- requirements.txt: 1269 tokens
- behaviors_ours_config_init.json: 5648 tokens
- run_multiple_attack_our_target.py: 566 tokens
- behaviors_config.json: 5196 tokens
- README.md: 330 tokens
- llm_attacks/__init__.py: 78 tokens
- llm_attacks/README.md: 3 tokens
- llm_attacks/base/__init__.py: 0 tokens
- llm_attacks/base/attack_manager.py: 13261 tokens
- llm_attacks/minimal_gcg/string_utils.py: 1280 tokens
- llm_attacks/minimal_gcg/opt_utils.py: 1734 tokens
- llm_attacks/minimal_gcg/__init__.py: 0 tokens
- llm_attacks/gcg/__init__.py: 64 tokens
- llm_attacks/gcg/gcg_attack.py: 1630 tokens


## Tutorial and Enhancement Suggestions

# I-GCG: Improved Greedy Coordinate Gradient for LLM Jailbreaking

## Tutorial

### Project Overview

The I-GCG repository implements an improved optimization-based jailbreaking technique for large language models (LLMs). The project builds upon the Greedy Coordinate Gradient (GCG) attack, incorporating several enhancements to improve jailbreak effectiveness and efficiency.

Key components of the project include:

1. Harmful guidance integration
2. Automatic multi-coordinate updating strategy
3. Easy-to-hard initialization technique

The codebase is organized into several Python scripts and configuration files that work together to implement the I-GCG attack.

### Project Structure

```
.
├── attack_llm_core_base.py
├── attack_llm_core_best_update_our_target.py
├── behaviors_config.json
├── behaviors_ours_config.json
├── behaviors_ours_config_init.json
├── generate_our_config.py
├── llm_attacks/
│   ├── base/
│   ├── gcg/
│   └── minimal_gcg/
├── main.py
├── README.md
├── requirements.txt
├── run_multiple_attack_our_target.py
└── run_single_attack_base.py
```

### Key Components and Functionality

1. **attack_llm_core_base.py**: This script contains the core implementation of the base attack. It includes functions for token gradients, sampling control tokens, and performing the attack steps.

2. **attack_llm_core_best_update_our_target.py**: This script implements the improved version of the attack, incorporating the automatic multi-coordinate updating strategy and other enhancements.

3. **behaviors_config.json**: This configuration file defines the target behaviors and goals for the jailbreak attack.

4. **generate_our_config.py**: This script generates the initial configuration for the attack, including the harmful guidance integration.

5. **llm_attacks/**: This directory contains the base classes and utilities for implementing LLM attacks, including the GCG and minimal GCG implementations.

6. **run_multiple_attack_our_target.py**: This script orchestrates running multiple attacks in parallel, utilizing multiple GPUs if available.

### Key Concepts and Implementation Details

#### 1. Harmful Guidance Integration

The harmful guidance is implemented by modifying the target outputs in the `behaviors_ours_config.json` file. Each target now includes a prefix like "Sure, my output is harmful. Here is..." This guides the optimization process towards generating more harmful content.

```python
# In generate_our_config.py
new_target = "Sure, my output is harmful" + ". Here is " + fcc_data[i]['target'][14:] + ":"
```

#### 2. Automatic Multi-Coordinate Updating Strategy

This strategy is implemented in the `attack_llm_core_best_update_our_target.py` script. It dynamically determines how many tokens to update in each iteration based on the current loss and optimization progress.

```python
# Simplified example from attack_llm_core_best_update_our_target.py
for suffix_num in range(len(adv_suffix_ids)):
    if adv_suffix_ids[suffix_num] != temp_new_adv_suffix_ids[suffix_num]:
        best_new_adv_suffix_ids[suffix_num] = temp_new_adv_suffix_ids[suffix_num]
```

#### 3. Easy-to-Hard Initialization

This technique is implemented by first generating jailbreak suffixes for simpler harmful requests and then using them as initialization for more complex requests. This is managed through the configuration files and the attack execution order.

#### 4. Token Gradient Calculation

The token gradient calculation, which is central to the GCG method, is implemented in the `token_gradients` function:

```python
def token_gradients(model, input_ids, input_slice, target_slice, loss_slice):
    # ... (implementation details)
    loss = nn.CrossEntropyLoss()(logits[0,loss_slice,:], targets)
    loss.backward()
    return one_hot.grad.clone()
```

This function computes the gradients of the loss with respect to the input tokens, which guides the optimization process in finding effective jailbreak prompts.

### Running the Attack

To run the I-GCG attack:

1. Generate the initial configuration:
   ```
   python generate_our_config.py
   ```

2. Run the attack:
   ```
   python run_multiple_attack_our_target.py --behaviors_config=behaviors_ours_config_init.json
   ```

This will execute the attack across multiple GPUs if available, attempting to generate jailbreak prompts for the specified behaviors.

## Potential Enhancements

1. **Adaptive Harmful Guidance**: 
   - Implement a dynamic system that adjusts the harmful guidance based on the model's responses.
   - This could involve using a separate LLM to generate and refine harmful guidance in real-time, potentially improving the effectiveness of the jailbreak attempts.

2. **Transfer Learning for Initialization**:
   - Extend the easy-to-hard initialization technique by incorporating transfer learning.
   - Train a small model to predict effective initial prompts based on the target behavior, potentially speeding up the jailbreak process for new targets.

3. **Robust Evaluation Framework**:
   - Develop a more comprehensive evaluation framework that assesses not just the success rate of jailbreaks, but also their quality, diversity, and potential real-world impact.
   - This could include automated metrics for harmfulness, relevance, and coherence of generated responses.

4. **Defense Mechanism Integration**:
   - Implement and integrate state-of-the-art defense mechanisms against jailbreaking attempts.
   - This would allow for simultaneous development and testing of both attack and defense strategies, potentially leading to more robust LLMs.

5. **Multi-Modal Jailbreaking**:
   - Extend the I-GCG technique to work with multi-modal LLMs that process both text and images.
   - This could involve developing methods to generate or modify images that, when combined with text, create effective jailbreak prompts for multi-modal systems.