#Tags
[[Research/Research Papers/2405.14023v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData

**Title:** WordGame: Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response
**Authors:** Tianrong Zhang, Bochuan Cao, Yuanpu Cao, Lu Lin, Prasenjit Mitra, Jinghui Chen
**Affiliation:** College of Information Science and Technology, Pennsylvania State University
**Publication Date:** May 22, 2024

Summary:
This paper introduces WordGame, a novel jailbreaking attack on large language models (LLMs) that exploits weaknesses in current safety alignment measures through simultaneous obfuscation in queries and responses.

Key Contributions:
- Identification of query obfuscation and response obfuscation as crucial features for successful jailbreaks
- Introduction of WordGame attack, implementing simultaneous query and response obfuscation
- Demonstration of WordGame's effectiveness against leading proprietary and open-source LLMs
- Ablation studies showing the merits of the attack strategy beyond individual attacks

Problem Statement:
Current safety alignment measures in LLMs are vulnerable to jailbreaking attacks that exploit patterns in the alignment process, potentially leading to the generation of harmful or unsafe content.

Methodology:
1. Analysis of common patterns in current safety alignment measures
2. Development of WordGame attack:
   - Query obfuscation: Replacing malicious words with word games
   - Response obfuscation: Encouraging benign content to precede harmful content
3. Evaluation against leading LLMs (Claude 3, GPT 4, Llama 3)
4. Ablation studies on simultaneous obfuscation in query and response

Main Results:
1. WordGame attack achieves higher jailbreaking effectiveness and efficiency compared to existing methods
2. Consistent attack success rates over 90% against various LLMs
3. Improved performance even with limited query budgets (5 attempts)

Qualitative Analysis:
- WordGame exploits vulnerabilities in preference learning pipelines used for safety alignment
- The attack creates contexts not covered by safety alignment corpora
- Simultaneous obfuscation strategy proves effective across different LLM architectures and safety measures

Limitations:
- The study focuses on text-based jailbreaking and may not apply to other modalities
- The effectiveness of the attack may vary with future improvements in LLM safety measures

Conclusion and Future Work:
- WordGame demonstrates the ongoing vulnerability of LLMs to jailbreaking attacks
- The proposed simultaneous obfuscation strategy provides insights for improving safety alignment in LLMs
- Future work may explore adaptations of the attack strategy and development of more robust safety measures

New Tools:
WordGame attack (no GitHub repository mentioned)