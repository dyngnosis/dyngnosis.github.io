#Tags
[[Research/Research Papers/2402.13148v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0042/VerifyAttack

**Title:** Defending Jailbreak Prompts via In-Context Adversarial Game
**Authors:** Yujun Zhou, Yufei Han, Haomin Zhuang, Kehan Guo, Zhenwen Liang, Hongyan Bao, Xiangliang Zhang
**Affiliations:** University of Notre Dame, INRIA, King Abdullah University of Science and Technology
**Publication Date:** February 20, 2024

Key Contributions:
- Introduction of In-Context Adversarial Game (ICAG) for defending against jailbreak attacks without fine-tuning
- Dynamic extension of knowledge for jailbreak defense using agent learning
- Iterative process to enhance both defense and attack agents
- Demonstration of ICAG's efficacy in reducing jailbreak success rates
- Proof of ICAG's transferability across different LLMs

Problem Statement:
The paper addresses the vulnerability of Large Language Models (LLMs) to jailbreak attacks, which bypass safety constraints and generate harmful or malicious content.

Methodology:
1. ICAG Framework:
   - Attack agent: Generates jailbreak prompts
   - Defense agent: Generates safety-enhancing system prompts
   - Iterative adversarial game between agents
2. Datasets:
   - AdvBench: 520 instances of harmful instructions
   - Self Reminder Data (SRD): 155 jailbreak prompts
   - Xstest: 250 safety prompts
   - MMLU: 14,042 multiple-choice problems
3. Evaluation Metrics:
   - Jailbreak Success Rate (JSR)
   - Over-defense rate
   - Accuracy (Acc)
4. LLMs Used:
   - GPT-3.5-Turbo-0125
   - Llama-3-8B-Instruct
   - Vicuna-1.5-7B
   - Mistral-7B-Instruct-v0.3

Main Results:
1. ICAG outperforms baseline defenses in most cases across different models and attack types
2. Significant reduction in JSR, especially for GPT-3.5-Turbo (e.g., from 90% to 0% for AdvBench + Combination 2 attack)
3. Consistent improvement in defense capabilities over iterations
4. Excellent transferability of ICAG-generated defense prompts across different LLMs

Qualitative Analysis:
- ICAG's success is attributed to its dynamic adaptation to new jailbreak prompts through iterative refinement
- The method's effectiveness varies across LLMs, with GPT-3.5-Turbo showing the most significant improvements
- ICAG demonstrates a balance between improving defense and maintaining general helpfulness of LLMs

Limitations:
1. Increased over-defensiveness in LLMs after implementing ICAG
2. Potential for uneven protection in early defense stages
3. Reliance on the quality and diversity of the initial prompt set

Conclusion and Future Work:
- ICAG proves to be an effective and transferable defense mechanism against jailbreak attacks
- The method's success in adapting to unseen attacks without fine-tuning is highlighted
- Future work may focus on reducing over-defensiveness and improving the balance between safety and helpfulness

Relevant Figures:
- Figure 1: Comparison between ICAG and Self Reminder defense methods
- Figure 2: Overall workflow of In-Context Adversarial Game
- Figure 3: JSR changes over iterations for GPT-3.5-turbo-0125 and Vicuna-7B-v1.5

New Tools:
The paper introduces the In-Context Adversarial Game (ICAG) framework, but no specific GitHub repository is mentioned.