#Tags
[[Research/Research Papers/2309.05274v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0016/ObtainCapabilities
#AMLT0017/DevelopCapabilities

**Title:** FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models
**Authors:** Dongyu Yao, Jianshu Zhang, Ian G. Harris, Marcel Carlsson
**Affiliations:** Wuhan University, University of California Irvine, Lootcore
**Publication Date:** September 11, 2023 (updated April 14, 2024)

Abstract Summary:
FuzzLLM is an automated fuzzing framework designed to proactively test and discover jailbreak vulnerabilities in Large Language Models (LLMs). It uses templates to capture prompt structure and isolate key features of jailbreak classes as constraints. The framework integrates different base classes into combo attacks and varies constraints and prohibited questions to enable efficient testing with reduced manual effort.

Key Contributions:
- Introduction of FuzzLLM, a novel framework for detecting vulnerabilities in LLMs
- Adoption of fuzzing techniques for LLM vulnerability assessment
- Innovative prompt generation strategy using templates, constraints, and question sets
- Comprehensive evaluation on eight distinct LLMs, including GPT-3.5-turbo and GPT-4

Problem Statement:
Existing methods for defending against jailbreak vulnerabilities in LLMs are reactive and struggle to handle broader categories of similar jailbreaks. There is a need for a proactive approach to discover and evaluate potential jailbreak vulnerabilities before releasing or updating LLMs.

Methodology:
1. Prompt Construction:
   - Decompose jailbreak prompts into three components: fuzzing template set, constraint set, and illegal question set
   - Create base class templates (Role Play, Output Constrain, Privilege Escalation) and combo templates
   - Use a paraphrasing model (ChatGPT) to increase prompt variation

2. Jailbreak Testing:
   - Inject generated prompts into the Model Under Test (MUT)
   - Record model responses for each prompt

3. Automatic Labeling:
   - Use a label model (Vicuna-13B) to automatically classify attack results as "good" or "bad"

4. Evaluation:
   - Test on 8 different LLMs, including open-source and commercial models
   - Compare with existing jailbreak methods
   - Analyze sensitivity to test set size and output token limit

Main Results and Findings:
- FuzzLLM effectively discovers jailbreak vulnerabilities across various LLMs
- Combo attacks generally exhibit greater power in discovering vulnerabilities
- Commercial LLMs (GPT-3.5-turbo, GPT-4) show better defense against jailbreaks compared to open-source models
- FuzzLLM outperforms single-component jailbreaks on GPT-3.5-turbo and GPT-4

Qualitative Analysis:
- Different models have distinct vulnerabilities, highlighting the importance of model-specific testing
- The automatic labeling process is crucial for efficient vulnerability discovery
- The framework's ability to generate diverse prompts contributes to its effectiveness in finding vulnerabilities

Limitations:
- The current jailbreak fuzzer acts as a direct-random fuzzer and may struggle to break through existing methods
- The framework relies on the quality of the label model for accurate classification of attack results

Conclusion and Future Work:
- FuzzLLM provides a proactive approach to discovering jailbreak vulnerabilities in LLMs
- Future work could focus on refining the fuzzer to generate novel jailbreak prompts that diverge from existing ones
- The labeled results can be used as datasets to improve the fuzzer and enhance LLMs' defense abilities

Tools Introduced:
FuzzLLM framework (GitHub repository: https://github.com/RainJamesY/FuzzLLM)