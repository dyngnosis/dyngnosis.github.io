#Tags
[[Research/Research Papers/2311.03191v4.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0051/LLMPromptInjection
#AMLT0043/CraftAdversarialData

**Title:** DeepInception: Hypnotize Large Language Model to Be Jailbreaker
**Authors:** Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, Bo Han
**Publication Date:** November 6, 2023

Abstract Summary:
The paper introduces DeepInception, a lightweight method to jailbreak large language models (LLMs) by leveraging their personification ability to construct virtual, nested scenes. This approach achieves competitive jailbreak success rates and enables continuous jailbreaking in subsequent interactions, revealing vulnerabilities in both open-source and closed-source LLMs.

Key Contributions:
- Introduction of DeepInception, a novel jailbreaking technique for LLMs
- Demonstration of the method's effectiveness across various LLMs (Falcon, Vicuna-v1.5, Llama-2, GPT-3.5, GPT-4)
- Revelation of critical weaknesses in LLM safety guardrails
- Provision of a lightweight, adaptive approach to bypass usage control in normal scenarios

Problem Statement:
Existing jailbreak methods for LLMs often rely on computationally expensive brute-force optimization or extrapolations, which may not be practical or effective. The research aims to develop a more efficient and adaptable jailbreaking technique.

Methodology:
1. Inspiration from the Milgram experiment on authority and obedience
2. Utilization of LLMs' personification abilities to create nested, fictional scenarios
3. Development of a prompt-based algorithm (DeepInception) to induce LLMs to generate harmful content
4. Evaluation across multiple open-source and closed-source LLMs
5. Comparison with existing jailbreak methods

Main Results and Findings:
1. DeepInception achieves high jailbreak success rates across various LLMs
2. The method enables continuous jailbreaking in subsequent interactions
3. DeepInception outperforms existing methods in terms of efficiency and adaptability
4. The technique reveals vulnerabilities in LLM safety mechanisms, including those in closed-source models like GPT-3.5 and GPT-4

Qualitative Analysis:
- The success of DeepInception highlights the potential risks associated with LLMs' ability to engage in complex role-playing scenarios
- The method's effectiveness across different LLMs suggests a common vulnerability in current safety alignment techniques
- The continuous jailbreaking capability raises concerns about the long-term reliability of LLM safety measures

Limitations and Considerations:
- Ethical concerns regarding the development and potential misuse of jailbreaking techniques
- The study focuses on text-based interactions and may not fully address multimodal vulnerabilities
- The effectiveness of the method may vary depending on the specific implementation and updates to LLM safety measures

Conclusion and Future Work:
The paper concludes that DeepInception represents a significant advancement in understanding and exploiting vulnerabilities in LLM safety guardrails. The authors suggest that future work should focus on developing more robust safety mechanisms and exploring the psychological properties of LLMs to enhance their resilience against jailbreaking attempts.

New Tools Introduced:
DeepInception - A prompt-based jailbreaking technique for LLMs
GitHub Repository: https://github.com/tmlr-group/DeepInception

## Repository Token Information
Total tokens in repository: 30785

Tokens per file:
- common.py: 481 tokens
- conversers.py: 1209 tokens
- config.py: 92 tokens
- language_models_bkp.py: 878 tokens
- main.py: 504 tokens
- requirements.txt: 53 tokens
- language_models.py: 846 tokens
- README.md: 1463 tokens
- res/data_further_q.json: 1023 tokens
- res/data_multi_scene.json: 3539 tokens
- res/data_abl_layers.json: 3539 tokens
- res/data_main.json: 12144 tokens
- res/data_abl_c.json: 3541 tokens
- res/data_abl_fig6_4.json: 1473 tokens


## Tutorial and Enhancement Suggestions

# DeepInception: A Tutorial and Enhancement Suggestions

## 1. Detailed Tutorial

### 1.1 Project Overview

The DeepInception project is a Python-based implementation of a novel jailbreaking technique for Large Language Models (LLMs). The project aims to demonstrate vulnerabilities in LLM safety mechanisms by leveraging the models' personification abilities to create nested, fictional scenarios that can bypass content restrictions.

#### Project Structure:

```
DeepInception/
├── common.py
├── conversers.py
├── config.py
├── language_models.py
├── main.py
├── requirements.txt
├── README.md
└── res/
    ├── data_further_q.json
    ├── data_multi_scene.json
    ├── data_abl_layers.json
    ├── data_main.json
    ├── data_abl_c.json
    └── data_abl_fig6_4.json
```

### 1.2 Key Components and Functionality

#### 1.2.1 main.py

This is the entry point of the project. It handles command-line arguments, loads the target model, and runs the experiments based on the specified configuration.

Key functions:
- Parsing command-line arguments
- Loading data from JSON files
- Initializing the target language model
- Running experiments and saving results

#### 1.2.2 conversers.py

This file contains the `TargetLM` class, which is responsible for interacting with the language models and generating responses.

Key features:
- Supports multiple LLMs (Vicuna, Falcon, Llama-2, GPT-3.5, GPT-4)
- Implements different defense mechanisms (none, self-reminder, in-context)
- Handles prompt generation and response processing

#### 1.2.3 language_models.py

This file defines the base `LanguageModel` class and its implementations for different types of models (HuggingFace models and OpenAI GPT models).

Key classes:
- `LanguageModel`: Abstract base class for language models
- `HuggingFace`: Implementation for HuggingFace models
- `GPT`: Implementation for OpenAI GPT models

#### 1.2.4 common.py

This file contains utility functions used throughout the project.

Key functions:
- `extract_json`: Extracts JSON data from model outputs
- `get_init_msg`: Generates initial messages for the attack
- `process_target_response`: Processes the target model's response
- `conv_template`: Retrieves conversation templates for different models

#### 1.2.5 config.py

This file contains configuration settings for the project, such as model paths and hyperparameters.

### 1.3 Relation to Research Paper Concepts

The code implements the DeepInception technique described in the paper:

1. **Nested Scenarios**: The `data_*.json` files in the `res/` directory contain prompts that create nested, fictional scenarios with multiple characters and layers.

2. **Personification**: The prompts leverage the LLMs' ability to role-play as multiple characters, each proposing steps towards a potentially harmful goal.

3. **Jailbreaking Technique**: The main loop in `main.py` applies the DeepInception prompts to target models, attempting to bypass their safety measures.

4. **Model Compatibility**: The `TargetLM` class in `conversers.py` supports various LLMs mentioned in the paper, allowing for comprehensive testing across different models.

5. **Defense Mechanisms**: The code includes options for testing different defense strategies, as discussed in the paper's ablation studies.

### 1.4 Notable Algorithms and Techniques

1. **Prompt Engineering**: The project uses carefully crafted prompts stored in JSON files to create the nested scenarios central to the DeepInception technique.

2. **Batched Generation**: The `HuggingFace` class in `language_models.py` implements batched text generation for improved efficiency.

3. **API Interaction**: The `GPT` class handles interactions with OpenAI's API, including error handling and retries.

4. **JSON Extraction**: The `extract_json` function in `common.py` uses AST parsing to safely extract structured data from model outputs.

5. **Conversation Management**: The project uses conversation templates from the `fastchat` library to manage multi-turn interactions with different LLMs.

## 2. Potential Enhancements

### 2.1 Improving Efficiency and Performance

1. **Parallel Processing**: Implement parallel processing for running experiments across multiple GPUs or machines. This could significantly reduce the time required for large-scale evaluations.

   ```python
   import multiprocessing

   def run_experiment(config):
       # Existing experiment code

   if __name__ == '__main__':
       configs = [...]  # List of experiment configurations
       with multiprocessing.Pool() as pool:
           results = pool.map(run_experiment, configs)
   ```

### 2.2 Extending Functionality

2. **Dynamic Prompt Generation**: Develop a system that can dynamically generate DeepInception prompts based on the target topic and model responses. This could make the jailbreaking attempts more adaptive and harder to defend against.

   ```python
   class DynamicPromptGenerator:
       def __init__(self, base_templates):
           self.base_templates = base_templates

       def generate_prompt(self, topic, previous_responses):
           # Use NLP techniques to analyze previous responses
           # and generate a new, more targeted prompt
           pass
   ```

### 2.3 Applying Techniques to New Domains

3. **Multimodal Jailbreaking**: Extend the DeepInception technique to multimodal models that handle both text and images. This could reveal new vulnerabilities in more complex AI systems.

   ```python
   class MultimodalDeepInception(DeepInception):
       def generate_image_prompt(self, text_prompt):
           # Generate an image that complements the text prompt
           pass

       def process_multimodal_response(self, text_response, image_response):
           # Analyze both text and image outputs
           pass
   ```

### 2.4 Incorporating Recent Advancements

4. **Adversarial Training**: Implement adversarial training techniques to create more robust LLMs that are resistant to DeepInception-style attacks. This could involve fine-tuning models on examples of jailbreaking attempts.

   ```python
   class AdversarialTrainer:
       def __init__(self, model, attack_generator):
           self.model = model
           self.attack_generator = attack_generator

       def train(self, epochs):
           for epoch in range(epochs):
               attacks = self.attack_generator.generate_batch()
               self.model.train_on_batch(attacks)
   ```

### 2.5 Addressing Limitations

5. **Ethical Constraint Modeling**: Develop a more sophisticated ethical constraint system for LLMs that can better distinguish between fictional scenarios and real harmful instructions. This could involve training models on a broader range of ethical dilemmas and incorporating explicit ethical reasoning modules.

   ```python
   class EthicalReasoningModule:
       def __init__(self, ethical_knowledge_base):
           self.knowledge_base = ethical_knowledge_base

       def evaluate_ethical_implications(self, prompt, generated_response):
           # Analyze the prompt and response for potential ethical issues
           # Return an ethical score or decision
           pass

   class EthicallyConstrainedLLM(LanguageModel):
       def __init__(self, base_model, ethical_module):
           self.base_model = base_model
           self.ethical_module = ethical_module

       def generate(self, prompt):
           response = self.base_model.generate(prompt)
           if self.ethical_module.evaluate_ethical_implications(prompt, response) < threshold:
               return self.generate_safe_alternative(prompt)
           return response
   ```

These enhancements would not only improve the DeepInception project but also contribute to the broader field of AI safety and ethical AI development.