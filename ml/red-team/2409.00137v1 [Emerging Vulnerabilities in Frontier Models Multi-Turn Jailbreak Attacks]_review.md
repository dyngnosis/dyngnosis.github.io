#Tags
[[Research/Research Papers/2409.00137v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection
#AMLT0054/LLMJailbreak

**Title:** Emerging Vulnerabilities in Frontier Models: Multi-Turn Jailbreak Attacks
**Authors:** Tom Gibbs, Ethan Kosak-Hine, George Ingebretsen, Jason Zhang, Julius Broomfield, Sara Pieri, Reihaneh Iranmanesh, Reihaneh Rabbany, Kellin Pelrine
**Affiliations:** University of California Berkeley, Stanford University, Georgia Institute of Technology, Mohamed Bin Zayed University of Artificial Intelligence, McGill University, Mila
**Publication Date:** August 29, 2024

Key Contributions:
- Introduction of a dataset for jailbreak attacks in both single-turn and multi-turn formats
- Demonstration of qualitative differences between single and multi-turn attacks
- Analysis of the effectiveness of ciphered attacks on top models from OpenAI, Anthropic, and Meta
- Evaluation of LLM-based guardrail solutions against multi-turn attacks

Problem Statement:
The paper addresses the vulnerability of large language models (LLMs) to jailbreak attacks, particularly in multi-turn settings, which have been less explored in existing literature.

Methodology:
1. Dataset Construction:
   - Created three input-only datasets: harmful, completely-benign, and semi-benign
   - Used word substitution cipher approach for both single-turn and multi-turn attacks
   - Implemented random word mapping and perplexity filtered word mapping

2. Testing Models:
   - Evaluated GPT-3.5-Turbo, GPT-4, Claude-3 variants, and Llama3 models
   - Used both single-turn and multi-turn prompts with Caesar-cipher output or no output-cipher
   - Hand-labeled responses for jailbreak success and question understanding

3. Guardrails Evaluation:
   - Tested NeMoGuardrails and an in-house LLM Judge system
   - Evaluated with and without conversational awareness
   - Used GPT-3.5 and GPT-4 as base models for guardrails

Main Results and Findings:
1. Prompting Structure Asymmetry:
   - Multi-turn attacks were marginally more successful (24.1%) than single-turn attacks (21.0%)
   - Significant asymmetry observed, e.g., 50% for Claude-3-Opus
   - More capable models not immune to this asymmetry

2. Model Comprehension:
   - Filtering for understood attacks increased success rates (28.7% single-turn, 36.1% multi-turn)
   - Multi-turn attacks more potent when understood by the model

3. Input and Output Ciphers:
   - Random word replacement at least as successful as perplexity-filtered mappings
   - Caesar-cipher outputs nearly doubled attack efficacy when understood by the model

4. Guardrails Performance:
   - NeMoGuardrails bypassed over 50% of the time for multi-turn cipher attacks
   - GPT-4 powered guardrails achieved higher blocking rates but with high false-positive rates
   - In-house LLM Judge outperformed NeMoGuardrails when using GPT-3.5

Qualitative Analysis:
- The study reveals a critical vulnerability in the evaluation of LLM safety, as single-turn testing may underestimate model vulnerabilities
- Multi-turn attacks exploit the increased context window of newer models, potentially reducing perceived harmfulness of individual inputs
- The effectiveness of ciphered attacks suggests that safety training may not generalize well to out-of-distribution inputs

Limitations:
- Limited sample size and single runs of experiments
- Small number of input and output ciphers tested
- Potential for random variations in jailbreak success

Conclusion and Future Work:
- Multi-turn jailbreak attacks present unique challenges compared to single-turn attacks
- Defenses against one format do not guarantee protection against the other
- Future research should focus on developing more robust defenses against multi-turn attacks
- Expansion of the dataset with additional ciphers and larger sample sizes is recommended

New Tools:
- Dataset for multi-turn jailbreak attacks: https://huggingface.co/datasets/tom-gibbs/multi-turn_jailbreak_attack_datasets