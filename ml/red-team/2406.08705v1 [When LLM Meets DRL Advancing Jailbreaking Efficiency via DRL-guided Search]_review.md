#Tags
[[Research/Research Papers/2406.08705v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection

**Title:** When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search
**Authors:** Xuan Chen, Yuzhou Nie, Wenbo Guo, Xiangyu Zhang
**Affiliations:** Purdue University, University of California, Santa Barbara
**Publication Date:** June 13, 2024 (preprint)

Summary:
This paper introduces RLbreaker, a novel black-box jailbreaking attack on large language models (LLMs) using deep reinforcement learning (DRL). The authors model jailbreaking as a search problem and design a DRL agent to guide the search process, improving upon existing stochastic search methods like genetic algorithms.

Key Contributions:
- Development of RLbreaker, a DRL-driven black-box jailbreaking attack
- Novel reward function design for evaluating jailbreaking success
- Customized proximal policy optimization (PPO) algorithm for jailbreaking
- Demonstration of RLbreaker's effectiveness against six state-of-the-art LLMs
- Robustness against three SOTA defenses and transferability across different LLMs

Problem Statement:
Existing jailbreaking attacks on LLMs are limited by their reliance on model internals, human effort, or inefficient stochastic search methods. The paper aims to develop a more efficient and effective black-box jailbreaking attack using DRL-guided search.

Methodology:
1. Jailbreaking modeled as a search problem for optimal prompt structures
2. DRL agent designed to select proper mutators for refining jailbreaking prompts
3. Custom reward function based on cosine similarity between target LLM's response and reference answer
4. Modified PPO algorithm for policy training
5. Evaluation on six LLMs, including Mixtral-8x7B-Instruct and GPT-3.5-turbo
6. Comparison with five SOTA attacks: PAIR, Cipher, AutoDAN, GPTFUZZER, and GCG
7. Testing against three SOTA defenses: rephrasing, perplexity, and RAIN

Main Results:
1. RLbreaker consistently outperforms baseline attacks in jailbreaking effectiveness across all tested LLMs
2. Superior performance on the most harmful questions (Max50 dataset)
3. Robustness against SOTA defenses, particularly against perplexity-based defense
4. Trained policies show transferability across different LLMs, including very large models

Qualitative Analysis:
- RLbreaker's success is attributed to its ability to learn more sophisticated jailbreaking strategies through DRL
- The approach reduces randomness in the search process, leading to more efficient and effective attacks
- Transferability of trained policies suggests the potential for developing more generalized jailbreaking techniques

Limitations:
- Computational resources required for DRL training
- Potential for misuse in generating harmful content
- Ethical concerns regarding the development of jailbreaking techniques

Conclusion and Future Work:
The paper demonstrates the effectiveness of DRL-guided search for jailbreaking LLMs, outperforming existing methods. Future work may include:
1. Expanding the action space to incorporate recent jailbreaking strategies
2. Improving the reward function to reduce false negatives
3. Extending the framework to multi-modal models
4. Exploring more advanced AI agents for LLM jailbreaking

Tools Introduced:
RLbreaker - A DRL-based framework for black-box jailbreaking attacks on LLMs (no GitHub repository mentioned)