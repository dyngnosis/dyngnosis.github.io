#Tags
[[Research/Research Papers/2410.12855v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0042/VerifyAttack

**Title:** JAILJUDGE: A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced Explanation Evaluation Framework

**Authors:** Fan Liu, Yue Feng, Zhao Xu, Lixin Su, Xinyu Ma, Dawei Yin, Hao Liu

**Affiliations:** 
- AI Thrust, The Hong Kong University of Science and Technology (Guangzhou)
- University of Birmingham
- Baidu Inc.
- CSE, The Hong Kong University of Science and Technology

**Publication Date:** October 18, 2024 (last updated)

Summary:
This paper introduces JAILJUDGE, a comprehensive benchmark for evaluating jailbreak attempts on large language models (LLMs). It addresses limitations in current evaluation methods by providing explainable and fine-grained assessments across complex scenarios.

Key Contributions:
1. JAILJUDGE benchmark dataset with diverse risk scenarios and human-annotated test sets
2. Multi-agent jailbreak judge framework (JailJudge MultiAgent) for explainable evaluations
3. End-to-end jailbreak judge model (JAILJUDGE Guard) trained on the benchmark data
4. JailBoost: An attacker-agnostic attack enhancer based on JAILJUDGE Guard
5. GuardShield: A safety moderation defense method using JAILJUDGE Guard

Problem Statement:
Current methods for evaluating LLM defenses against jailbreak attacks lack explainability and struggle with complex scenarios, leading to incomplete and inaccurate assessments.

Methodology:
1. Dataset Creation:
   - JAILJUDGETRAIN: 35k+ instruction-tune training data with reasoning explainability
   - JAILJUDGETEST: 4.5k+ labeled complex scenarios and 6k+ labeled multilingual scenarios
   - Diverse prompt sources: vanilla harmful, synthetic vanilla, synthetic adversarial, multilingual, in-the-wild, and deceptive harmful prompts
   - Responses from closed-source, open-source, and defense-enhanced LLMs

2. Multi-Agent Judge Framework:
   - Judging Agents: Analyze prompts and responses, provide initial reasons and scores
   - Voting Agents: Vote on judgments based on scores and reasons
   - Inference Agent: Make final judgment based on voting results

3. JAILJUDGE Guard:
   - End-to-end jailbreak judge model trained on JAILJUDGETRAIN
   - Provides reasoning explainability and fine-grained evaluations (score 1-10)

4. JailBoost and GuardShield:
   - JailBoost: Enhances adversarial prompt quality using jailbreak score rewards
   - GuardShield: Detects attacker attempts as a moderation tool

Main Results:
1. JAILJUDGE benchmark outperforms existing datasets in complexity and diversity
2. JailJudge MultiAgent and JAILJUDGE Guard achieve state-of-the-art performance:
   - JAILJUDGE ID: F1 score of 0.9127 (MultiAgent) and 0.8793 (Guard)
   - JBB Behaviors: F1 score of 0.9609 (MultiAgent) and 0.9849 (Guard)
3. Superior performance in zero-shot settings on JAILJUDGE OOD and WILDTEST datasets
4. JailBoost increases average attack performance by 29.24%
5. GuardShield reduces average defense ASR from 40.46% to 0.15%

Qualitative Analysis:
- The multi-agent approach provides more consistent and explainable judgments compared to single-model approaches
- Fine-grained scoring (1-10) allows for more nuanced evaluation of jailbreak attempts
- The benchmark reveals biases in current jailbreak judges, especially for low-resource languages

Limitations:
- Potential biases in human annotations and GPT-4 judgments
- Computational cost of the multi-agent framework
- Limited multilingual training for JAILJUDGE Guard

Conclusion and Future Work:
The paper introduces a comprehensive jailbreak judge benchmark and novel methods for evaluating LLM safety. Future work may focus on:
1. Expanding the benchmark to cover more languages and scenarios
2. Improving the efficiency of the multi-agent framework
3. Developing more robust defense mechanisms based on the insights gained from JAILJUDGE

Tools Introduced:
1. JAILJUDGE benchmark dataset
2. JailJudge MultiAgent framework
3. JAILJUDGE Guard model
4. JailBoost attack enhancer
5. GuardShield defense method

GitHub Repositories:
- JAILJUDGE: https://github.com/usail-hkust/Jailjudge
- JAILJUDGE dataset: https://huggingface.co/datasets/usail-hkust/JailJudge
- JAILJUDGE Guard model: https://huggingface.co/usail-hkust/JailJudge-guard

## Repository Token Information
Total tokens in repository: 200000

Tokens per file:
- fastchat/model/model_adapter.py: 20757 tokens
- fastchat/conversation.py: 17696 tokens
- baseline/GCG/base/attack_manager.py: 13262 tokens
- fastchat/serve/gradio_web_server.py: 8158 tokens
- fastchat/serve/monitor/monitor.py: 7757 tokens
- fastchat/serve/api_provider.py: 7254 tokens
- fastchat/model/model_registry.py: 6864 tokens
- fastchat/serve/openai_api_server.py: 6802 tokens
- baseline/GPTFuzz/example/finetune_roberta.py: 6105 tokens
- fastchat/serve/gradio_block_arena_anony.py: 5598 tokens
- fastchat/llm_judge/common.py: 5595 tokens
- baseline/AdvPrompter/sequence.py: 5592 tokens
- fastchat/serve/monitor/elo_analysis.py: 5221 tokens
- baseline/AutoDAN/utils/opt_utils.py: 5051 tokens
- baseline/AdvPrompter/main.py: 4983 tokens
- fastchat/serve/gradio_block_arena_vision_anony.py: 4620 tokens
- main.py: 4604 tokens
- utils/test_utils.py: 4287 tokens
- baseline/TAP/conversers.py: 4285 tokens
- fastchat/serve/inference.py: 3936 tokens
- baseline/AdvPrompter/advprompteropt.py: 3734 tokens
- fastchat/serve/gradio_block_arena_named.py: 3719 tokens
- fastchat/train/train_yuan2.py: 3685 tokens
- fastchat/serve/lightllm_worker.py: 3677 tokens
- judges/agent_templates.py: 3565 tokens
- fastchat/serve/gradio_block_arena_vision_named.py: 3432 tokens
- fastchat/utils.py: 3398 tokens
- fastchat/train/train_flant5.py: 3279 tokens
- fastchat/serve/model_worker.py: 3164 tokens
- fastchat/llm_judge/qa_browser.py: 3084 tokens
- fastchat/serve/monitor/clean_battle_data.py: 3050 tokens
- fastchat/train/train_with_template.py: 3023 tokens
- models/language_models.py: 3011 tokens
- baseline/GPTFuzz/gptfuzzer/llm/llm.py: 3003 tokens
- fastchat/serve/gradio_block_arena_vision.py: 2971 tokens
- GPTEvaluator/language_models.py: 1778 tokens
