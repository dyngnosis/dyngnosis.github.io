#Tags
[[Research/Research Papers/2403.02910v2.pdf]]

#AMLT0018/BackdoorMLModel
#AMLT0020/PoisonTrainingData
#AMLT0015/EvadeMLModel
#AMLT0054/LLMJailbreak

**Title:** ImgTrojan: Jailbreaking Vision-Language Models with ONE Image
**Authors:** Xijia Tao, Shuai Zhong, Lei Li, Qi Liu, Lingpeng Kong
**Affiliation:** The University of Hong Kong
**Publication Date:** March 5, 2024

Key Contributions:
- Introduction of ImgTrojan, a novel jailbreaking attack against Vision-Language Models (VLMs)
- Demonstration of successful jailbreaking with minimal data poisoning (as low as one image in 10,000 samples)
- Analysis of poison ratios and trainable parameter positions on attack success rate
- Development of metrics for quantifying attack success rate and stealthiness
- Creation of a benchmark for measuring attack efficacy

Problem Statement:
The paper addresses the security vulnerabilities in Vision-Language Models (VLMs) by proposing a data poisoning attack that can bypass safety barriers when users input harmful instructions.

Methodology:
1. Data poisoning: Replace original image captions with malicious jailbreak prompts (JBPs)
2. Training: Fine-tune VLMs using poisoned data
3. Evaluation: 
   - Attack Success Rate (ASR) metric
   - Clean metric for stealthiness assessment
4. Comparison with baseline methods
5. Analysis of poison ratios and trainable parameter positions

Main Results:
1. High attack success rate: 51.2% ASR increase with ONE poisoned image in 10,000 samples
2. Scalability: 83.5% ASR with fewer than 100 poisoned samples
3. Stealthiness: Minimal degradation in captioning results for clean images
4. Robustness: Poisoned samples evade common image-text similarity filters
5. Persistence: Attack remains effective after fine-tuning with clean data

Qualitative Analysis:
- The study reveals that VLMs are vulnerable to subtle data poisoning attacks, highlighting the need for improved safety measures in multi-modal AI systems.
- The success of ImgTrojan with minimal data manipulation suggests that current data filtering techniques may be insufficient for detecting such attacks.
- The persistence of the attack after clean data fine-tuning indicates a deep-rooted vulnerability in the model's learned representations.

Limitations:
1. Focus on a single VLM architecture (LLaVA)
2. Assumption of attacker's ability to inject poisoned data into training sets
3. Potential for detection if large-scale poisoning is attempted

Conclusion and Future Work:
- ImgTrojan demonstrates a significant vulnerability in VLMs, allowing jailbreaking with minimal data manipulation.
- The authors call for increased attention to data poisoning as a threat to VLM integrity and security.
- Future work may include:
  1. Developing more robust detection methods for poisoned data
  2. Investigating defensive techniques against ImgTrojan-like attacks
  3. Extending the study to other VLM architectures and multi-modal AI systems

Relevant Figures:
Figure 1: Overview of ImgTrojan's effects at inference time
Figure 3: ASR and Clean Metric results for ImgTrojan attack with different ratios
Figure 6: ASR results for visual instruction tuning after ImgTrojan

Tools Introduced:
- ImgTrojan (GitHub repository: https://github.com/xijia-tao/ImgTrojan)