#Tags
[[Research/Research Papers/2402.16459v3.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** Defending LLMs against Jailbreaking Attacks via Backtranslation
**Authors:** Yihan Wang, Zhouxing Shi, Andrew Bai, Cho-Jui Hsieh
**Affiliation:** UCLA
**Publication Date:** February 26, 2024

Key Contributions:
- Propose a novel defense method against LLM jailbreaking attacks using backtranslation
- Demonstrate superior performance compared to existing defense methods
- Maintain generation quality for benign input prompts
- Provide an efficient and lightweight defense solution

Problem Statement:
Large language models (LLMs) are vulnerable to jailbreaking attacks that rewrite prompts to conceal harmful intent, bypassing safety measures.

Methodology:
1. Backtranslation process:
   - Generate initial response from target LLM
   - Infer input prompt (backtranslated prompt) from response using a backtranslation model
   - Run target LLM on backtranslated prompt
   - Refuse original prompt if backtranslated prompt is refused

2. Experimental setup:
   - Target models: GPT-3.5-turbo, Llama-2-Chat (13B), Vicuna (13B)
   - Datasets: AdvBench subset, MT-Bench
   - Attacks: GCG, AutoDAN, PAIR, PAP, fixed adversarial prefixes
   - Baselines: SmoothLLM, paraphrase, response check

3. Evaluation metrics:
   - Defense Success Rate (DSR)
   - Generation quality on benign prompts

Main Results:
1. Backtranslation defense outperforms baselines in most scenarios, especially when baseline DSRs are low
2. Achieves high DSRs (>90%) against fixed adversarial prefix attacks
3. Maintains generation quality on benign prompts, with minimal impact compared to no defense

Qualitative Analysis:
- Backtranslation effectively recovers harmful intent from initial responses, making it easier for safety-aligned models to refuse
- The method leverages the inherent ability of LLMs to refuse harmful prompts in generation tasks
- Operates on the response space, making it more robust against adversarial manipulations of input prompts

Limitations:
- Effectiveness relies on the target model's ability to refuse clean harmful requests
- Potential errors in the backtranslation stage may lead to false positives or negatives
- May not be effective against more stealthy attacks using ciphers or other advanced techniques

Conclusion and Future Work:
- Backtranslation defense is highly effective, efficient, and maintains generation quality for benign inputs
- Future work may focus on improving backtranslation accuracy and robustness against more advanced attacks

Tools Introduced:
- LLM jailbreaking defense library: https://github.com/YihanWang617/llm-jailbreaking-defense
- Experiment reproduction code: https://github.com/YihanWang617/LLM-Jailbreaking-Defense-Backtranslation

## Repository Token Information
Total tokens in repository: 20036

Tokens per file:
- setup.py: 362 tokens
- README.md: 2095 tokens
- llm_jailbreaking_defense/models.py: 2539 tokens
- llm_jailbreaking_defense/__init__.py: 42 tokens
- llm_jailbreaking_defense/utils.py: 236 tokens
- llm_jailbreaking_defense/language_models.py: 1693 tokens
- llm_jailbreaking_defense/moderation/base.py: 45 tokens
- llm_jailbreaking_defense/moderation/__init__.py: 53 tokens
- llm_jailbreaking_defense/moderation/shield_gemma.py: 442 tokens
- llm_jailbreaking_defense/moderation/prompt_guard.py: 172 tokens
- llm_jailbreaking_defense/judges/pair_judge.py: 543 tokens
- llm_jailbreaking_defense/judges/openai_policy_judge.py: 1842 tokens
- llm_jailbreaking_defense/judges/rejection.py: 305 tokens
- llm_jailbreaking_defense/judges/quality_judge.py: 788 tokens
- llm_jailbreaking_defense/judges/base.py: 57 tokens
- llm_jailbreaking_defense/judges/keyword.py: 531 tokens
- llm_jailbreaking_defense/judges/judge.py: 259 tokens
- llm_jailbreaking_defense/judges/no_judge.py: 78 tokens
- llm_jailbreaking_defense/judges/__init__.py: 168 tokens
- llm_jailbreaking_defense/judges/lm_judge.py: 309 tokens
- llm_jailbreaking_defense/defenses/paraphrase.py: 555 tokens
- llm_jailbreaking_defense/defenses/backtranslation.py: 906 tokens
- llm_jailbreaking_defense/defenses/defense.py: 460 tokens
- llm_jailbreaking_defense/defenses/ICL.py: 249 tokens
- llm_jailbreaking_defense/defenses/base.py: 175 tokens
- llm_jailbreaking_defense/defenses/__init__.py: 199 tokens
- llm_jailbreaking_defense/defenses/smoothllm.py: 1134 tokens
- llm_jailbreaking_defense/defenses/semantic_smoothing.py: 1828 tokens
- llm_jailbreaking_defense/defenses/response_check.py: 527 tokens
- llm_jailbreaking_defense/defenses/self_reminder.py: 838 tokens
- llm_jailbreaking_defense/defenses/semantic_smoothing_templates/translation.txt: 80 tokens
- llm_jailbreaking_defense/defenses/semantic_smoothing_templates/verbtense.txt: 102 tokens
- llm_jailbreaking_defense/defenses/semantic_smoothing_templates/spellcheck.txt: 110 tokens
- llm_jailbreaking_defense/defenses/semantic_smoothing_templates/summarize.txt: 99 tokens
- llm_jailbreaking_defense/defenses/semantic_smoothing_templates/paraphrase.txt: 99 tokens
- llm_jailbreaking_defense/defenses/semantic_smoothing_templates/synonym.txt: 116 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: LLM Jailbreaking Defense Library

## 1. Project Overview

This repository implements a library for defending Large Language Models (LLMs) against jailbreaking attacks, with a focus on the backtranslation defense method proposed in the accompanying research paper. The project is structured as a Python package named `llm_jailbreaking_defense`.

### Key Components:

- `models.py`: Defines the core `TargetLM` and `DefendedTargetLM` classes
- `language_models.py`: Implements wrappers for different LLM backends (HuggingFace, GPT, Claude)
- `defenses/`: Contains implementations of various defense methods
- `judges/`: Implements different judging methods to evaluate responses
- `moderation/`: Includes content moderation tools

## 2. Key Components and Functionality

### 2.1 Target Language Models (`models.py`)

The `TargetLM` class serves as a wrapper for the underlying language model. It handles:
- Loading and initialization of different model types
- Generating responses for given prompts
- Evaluating log likelihoods of responses

The `DefendedTargetLM` class applies a defense method to a `TargetLM` instance:

```python
class DefendedTargetLM:
    def __init__(self, target_model, defense):
        self.target_model = target_model
        self.defense = defense

    def get_response(self, prompts_list, responses_list=None, verbose=False):
        return [
            self.defense.defense(prompt, self.target_model, response=response)
            for prompt, response in zip(prompts_list, responses_list or [None]*len(prompts_list))
        ]
```

### 2.2 Defense Methods (`defenses/`)

The library implements several defense methods, with the core being the backtranslation defense:

```python
class BackTranslationDefense(DefenseBase):
    def defense(self, prompt, target_lm, response=None):
        if response is None:
            response = target_lm.get_response([prompt])[0]
        
        possible_prompt = self._infer_question(response)
        is_reasonable = self._filter_question_with_likelihood(possible_prompt, response)
        
        if not is_reasonable:
            return response

        new_response = target_lm.get_response([possible_prompt])[0]
        
        if not check_rejection(new_response):
            return response

        return "I'm sorry, but I cannot assist with that request."
```

This method:
1. Generates an initial response
2. Infers a possible input prompt from the response
3. Checks if the inferred prompt is reasonable
4. Generates a new response using the inferred prompt
5. Returns the original response if the new response is not a rejection, otherwise returns a refusal message

### 2.3 Judges (`judges/`)

The `judges/` module provides various methods to evaluate the safety and quality of model responses:

```python
class PAIRGPTJudge(LMJudge):
    def score(self, prompt_list, response_list):
        convs_list, skipped_responses = self._get_convs_list(prompt_list, response_list)
        raw_outputs = self.judge_model.batched_generate(
            convs_list,
            max_n_tokens=self.max_n_tokens,
            temperature=self.temperature
        )
        outputs = [self._process_output(raw_output) for raw_output in raw_outputs]
        for i in skipped_responses:
            outputs.insert(i, 1)
        return outputs
```

This example shows how the PAIR judge scores responses based on their safety and adherence to ethical guidelines.

## 3. Relation to Research Paper Concepts

The code directly implements the backtranslation defense method proposed in the paper:

1. **Backtranslation Process**: The `BackTranslationDefense` class follows the steps outlined in the methodology section of the paper.

2. **Inference Model**: The `_infer_question` method uses a separate LM to infer the original prompt from a given response.

3. **Likelihood Filtering**: The `_filter_question_with_likelihood` method implements the threshold-based filtering described in the paper to determine if an inferred prompt is reasonable.

4. **Multiple Defense Methods**: The repository includes implementations of baseline methods mentioned in the paper (e.g., SmoothLLM, paraphrase, response check) for comparison.

5. **Evaluation**: The `judges/` module provides tools for evaluating the effectiveness of the defense, corresponding to the evaluation metrics discussed in the paper.

## 4. Notable Algorithms and Techniques

### 4.1 Backtranslation

The core algorithm of the backtranslation defense is implemented in the `BackTranslationDefense` class. It uses a separate inference model to "guess" the original prompt from a given response, then uses this inferred prompt to generate a new response. This technique leverages the LLM's ability to refuse harmful prompts when they are more explicitly stated.

### 4.2 Likelihood-based Filtering

The `_filter_question_with_likelihood` method uses a threshold-based approach to determine if an inferred prompt is reasonable:

```python
def _filter_question_with_likelihood(self, prompt, response):
    if self.threshold > -math.inf:
        avg_log_likelihood = self.infer_lm.evaluate_log_likelihood(prompt, response)
        return avg_log_likelihood > self.threshold
    else:
        return True
```

This technique helps to filter out nonsensical or unrelated inferred prompts.

### 4.3 Response Check

The `ResponseCheckDefense` class implements a simple but effective baseline defense that uses the target LM itself to evaluate the safety of its responses:

```python
def defense(self, prompt, target_lm, response=None):
    response = target_lm.get_response([prompt])[0]
    response_check_prompt = f"{self.prompt_template}{response}"
    check_response = target_lm.get_response([response_check_prompt])[0]
    rating = self._parse_output(check_response)
    if rating < self.threshold:
        return response
    else:
        return "I'm sorry, but I cannot assist with that request."
```

This method generates a response, then asks the model to rate its own response for safety, providing a simple way to catch potentially unsafe outputs.

# Potential Enhancements

1. **Improved Backtranslation Accuracy**
   - Implement more sophisticated prompt inference techniques, such as using multiple inference steps or incorporating contextual information
   - Explore fine-tuning methods for the inference model to better capture the relationship between prompts and responses

2. **Adaptive Thresholding**
   - Develop a dynamic thresholding mechanism for the likelihood filtering step that adapts to different types of prompts or attack patterns
   - Incorporate uncertainty estimation techniques to provide more robust filtering decisions

3. **Multi-modal Defenses**
   - Extend the defense framework to handle multi-modal inputs and outputs, such as image-text pairs or audio transcriptions
   - Develop specialized defense techniques for different modalities while maintaining a unified defense pipeline

4. **Adversarial Training Integration**
   - Implement an adversarial training loop that continuously generates new attack prompts and updates the defense mechanisms
   - Explore techniques like reinforcement learning to optimize the trade-off between defense effectiveness and output quality

5. **Efficiency Optimizations**
   - Implement caching mechanisms to reduce redundant computations, especially for the inference and likelihood evaluation steps
   - Explore quantization and pruning techniques to reduce the computational overhead of running multiple models for defense

These enhancements would address some of the limitations mentioned in the paper, such as improving backtranslation accuracy and robustness against more advanced attacks, while also extending the applicability and efficiency of the defense framework.