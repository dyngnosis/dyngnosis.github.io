#Tags
[[Research/Research Papers/2409.17699v3.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection
#AMLT0054/LLMJailbreak

**Title:** MoJE: Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks
**Authors:** Giandomenico Cornacchia, Giulio Zizzo, Kieran Fraser, Muhammad Zaid Hameed, Ambrish Rawat, Mark Purcell
**Affiliation:** IBM Research Europe, Dublin, Ireland
**Publication Date:** September 26, 2024

Summary:
This paper introduces MoJE (Mixture of Jailbreak Experts), a novel guardrail architecture designed to detect and prevent jailbreak attacks on Large Language Models (LLMs). MoJE employs simple linguistic statistical techniques to achieve superior performance in detecting jailbreak attacks while maintaining minimal computational overhead during model inference.

Key Contributions:
- Introduction of MoJE, a modular guardrail architecture for detecting jailbreak attacks
- Demonstration of MoJE's superior performance in detecting 90% of attacks without compromising benign prompts
- Comparison with state-of-the-art guardrails, showing improved detection accuracy and computational efficiency

Problem Statement:
The proliferation of LLMs in various applications has led to an increased need for robust security measures against jailbreak attacks, which exploit vulnerabilities in LLMs and endanger data integrity and user privacy. Existing guardrails often fall short in terms of detection accuracy and computational efficiency.

Methodology:
1. Dataset preparation: Various jailbreak and benign datasets were used, including harmful behaviors, gandalf, gcg-vicuna, jailbreak prompts, puffin, alpaca, and awesome chatgpt prompts.
2. Model architecture: MoJE uses an ensemble of simple tabular classifiers, each trained on a specific jailbreak dataset and benign prompts.
3. Feature extraction: N-gram occurrences count (uni-gram) was used to transform prompts into feature vectors.
4. Model selection: Logistic Regression (LR) and eXtreme Gradient Boost Machine (XGB) were used as base classifiers.
5. Evaluation: Performance was compared against state-of-the-art guardrails, including ProtectAI, Llama-Guard, OpenAI Moderator API, and Azure AI Content Safety API.

Main Results:
1. MoJE outperformed existing guardrails in terms of AUC (0.9947), accuracy (0.9944), and F-beta score (0.9529).
2. MoJE demonstrated superior true positive rates across various jailbreak datasets while maintaining low false positive rates on benign datasets.
3. The modular nature of MoJE allowed for easy integration of new classifiers for out-of-distribution datasets, improving performance on new attack types.

Qualitative Analysis:
- MoJE's success can be attributed to its ensemble approach, which allows for specialized detection of different jailbreak types.
- The use of simple tabular classifiers and linguistic features enables MoJE to maintain low computational overhead compared to LLM-based guardrails.
- The modular architecture of MoJE provides flexibility in adapting to new attack types, addressing a key challenge in the rapidly evolving field of LLM security.

Limitations:
1. MoJE showed limitations in handling complex linguistic prompts, such as those in the "xstest" dataset, where LLM-based guardrails like Llama-Guard performed better.
2. The approach relies on the availability of labeled jailbreak datasets, which may not always be readily available for new attack types.

Conclusion and Future Work:
MoJE demonstrates the effectiveness of using simple, modular classifiers for detecting jailbreak attacks on LLMs. Future work will focus on enhancing MoJE's adaptability by exploring other low-weight language model architectures, new feature engineering techniques, and linguistic data augmentation. The authors also suggest investigating hybrid approaches combining statistical methods with deep learning to address more complex attack scenarios.

Relevant Figures/Tables:
- Table 2: Classification results comparing MoJE with other guardrails
- Figure 2: True positive rates for each jailbreak dataset across different models
- Figure 3: False positive rates for each benign dataset across different models

New Tool:
MoJE (Mixture of Jailbreak Experts) - A novel guardrail architecture for detecting jailbreak attacks on LLMs. No GitHub repository was mentioned in the paper.