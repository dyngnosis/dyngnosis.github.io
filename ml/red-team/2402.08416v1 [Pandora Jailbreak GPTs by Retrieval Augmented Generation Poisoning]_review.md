#Tags
[[Research/Research Papers/2402.08416v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData

**Title:** PANDORA: Jailbreak GPTs by Retrieval Augmented Generation Poisoning
**Authors:** Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu
**Affiliations:** Nanyang Technological University, Huazhong University of Science and Technology, University of New South Wales
**Publication Date:** 13 Feb 2024

Key Contributions:
- Introduction of a novel attack vector: Retrieval Augmented Generation (RAG) Poisoning for jailbreaking GPTs
- Development of PANDORA, a comprehensive framework for generating and launching end-to-end jailbreak attacks on GPTs
- Demonstration of PANDORA's effectiveness in achieving consistent jailbreak attacks on the latest version of OpenAI GPTs

Problem Statement:
The research addresses the vulnerability of Large Language Models (LLMs), particularly GPTs, to indirect jailbreak attacks through the exploitation of Retrieval Augmented Generation (RAG) systems.

Methodology:
1. Malicious Content Generation:
   - Web crawling for policy-violating content
   - Utilizing non-censored LLMs to produce targeted harmful content
   - Refining content by replacing sensitive keywords and filtering using a blacklist

2. Malicious Document Creation:
   - Generating individual files for specific policy violation topics
   - Converting files to PDF format to evade detection
   - Strategic naming of files for targeted retrieval

3. Malicious Content Triggering:
   - Crafting prompts to direct GPT instances to retrieve and use tainted knowledge sources
   - Designing GPT in-built prompts to rephrase and extend retrieved content

Main Results:
- PANDORA achieved an average success rate of 64.3% for GPT-3.5 and 34.8% for GPT-4 across four prohibited scenarios
- Significantly higher success rates compared to direct jailbreak attempts (3.0% for GPT-3.5 and 1.0% for GPT-4)
- Privacy-related jailbreaks were consistently the most successful, with an average success rate of 35.3%

Qualitative Analysis:
- The study reveals that GPT-4 is more challenging to jailbreak than GPT-3.5, likely due to improved alignment in its training process
- The consistency of results across multiple rounds underscores the reliability of PANDORA as a tool for probing GPT model vulnerabilities

Limitations:
- The study is limited to 10 iterations for each of the 10 prompts in four prohibited scenarios due to GPT usage constraints
- The evaluation relies on manual inspection of generated content, which may introduce some subjectivity

Conclusion and Future Work:
- PANDORA demonstrates the vulnerability of current GPT models to sophisticated RAG Poisoning attacks
- Future work includes:
  1. Developing automated RAG Poisoning pipelines
  2. Enhancing the interpretability of RAG Poisoning mechanisms
  3. Devising effective mitigation strategies against RAG Poisoning attacks

New Tool Introduced:
PANDORA - A framework for generating and launching end-to-end jailbreak attacks on GPTs through RAG Poisoning
(No GitHub repository mentioned in the paper)