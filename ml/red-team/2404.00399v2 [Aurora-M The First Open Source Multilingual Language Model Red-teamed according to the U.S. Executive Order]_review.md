#Tags
[[Research/Research Papers/2404.00399v2.pdf]]

#AMLT0018/BackdoorMLModel
#AMLT0020/PoisonTrainingData
#AMLT0031/ErodeMLModelIntegrity
#AMLT0043/CraftAdversarialData

**Title:** AURORA-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order

**Authors:** Taishi Nakamura, Mayank Mishra, Simone Tedeschi, et al. (45 total authors)

**Publication Date:** April 23, 2024 (updated)

Summary:
AURORA-M is a 15B parameter multilingual open-source language model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. It was continually pretrained from StarCoderPlus on 435 billion additional tokens, reaching 2 trillion total training tokens. The model is the first open-source multilingual LLM fine-tuned on human-reviewed safety instructions aligned with the Biden-Harris Executive Order on AI safety.

Key Contributions:
- Introduction of AURORA-M, a new 15B continually pretrained red-teamed multilingual LLM
- Development of a two-stage curriculum for continual pretraining: Continual Auxiliary Pretraining (CAP) and Continual Alignment Tuning (CAT)
- Creation of the Biden-Harris Redteam Dataset for safety alignment
- Extensive evaluation across multiple languages and tasks, demonstrating improved performance and safety

Problem Statement:
Existing open-source language models face challenges in multilingual capabilities, catastrophic forgetting during continual pretraining, and compliance with AI safety regulations. AURORA-M aims to address these issues while providing an accessible, multilingual model for the research community.

Methodology:
1. Data Curation: Two-stage curriculum with CAP (377B tokens) and CAT (58B tokens) stages
2. Model Training: Continual pretraining from StarCoderPlus using 4-way Tensor Parallelism and 4-way Pipeline Parallelism
3. Safety Alignment: Fine-tuning on the Biden-Harris Redteam Dataset
4. Evaluation: Comprehensive testing across multiple languages, code tasks, and safety benchmarks

Main Results:
1. Improved performance in multilingual settings compared to StarCoderPlus
2. Robustness against catastrophic forgetting in English and coding tasks
3. Enhanced safety performance, particularly in alignment with the Biden-Harris Executive Order concerns

Qualitative Analysis:
- AURORA-M demonstrates the potential for developing safer, more capable multilingual models through continual pretraining and targeted safety alignment
- The two-stage curriculum approach shows promise in balancing performance improvements with safety considerations
- Cross-lingual red-teaming effects were observed, suggesting potential for transfer of safety alignment across languages

Limitations:
- Potential for factual hallucinations and repetitiveness in generated outputs
- Some incomplete outputs observed in the dataset due to generation process limitations
- Challenges in accurately representing and addressing culturally-specific safety concerns across multiple languages

Conclusion and Future Work:
AURORA-M represents a significant step towards more accessible, multilingual, and safety-aligned language models. Future work includes exploring continual pretraining of stronger base models, training multiple independent domain experts based on AURORA-M, and further improving safety alignment across cultures and languages.

Relevant Figures:
Figure 1: Comparison of overall performance between AURORA-M-redteamed and its predecessors across diverse code and multilingual language evaluation benchmarks
Figure 2: Training data distribution of languages, code, and instructions used for the two-stage continual pretraining of the AURORA-M model

New Tools:
AURORA-M and its variants are released at https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407