#Tags
[[Research/Research Papers/2401.17256v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0054/LLMJailbreak
#AMLT0043/CraftAdversarialData
#AMLT0031/ErodeMLModelIntegrity

**Title:** Weak-to-Strong Jailbreaking on Large Language Models
**Authors:** Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, William Yang Wang
**Publication Date:** January 30, 2024

Summary:
This paper introduces a novel and efficient method called "weak-to-strong jailbreaking" to attack aligned large language models (LLMs) and produce harmful text. The attack exploits vulnerabilities in safety alignment by using smaller models to modify the decoding probabilities of larger, safer models.

Key Contributions:
- Proposes the weak-to-strong jailbreaking attack method
- Demonstrates high effectiveness (>99% misalignment rate) on multiple LLMs
- Requires only one forward pass per example, making it computationally efficient
- Exposes critical vulnerabilities in current LLM safety alignment techniques
- Proposes an initial defense strategy against the attack

Problem Statement:
Existing jailbreaking methods for LLMs are computationally expensive. This research aims to develop an efficient attack method that can bypass safety alignments in LLMs to produce harmful content.

Methodology:
1. Analyze token distribution differences between safe and unsafe models
2. Develop the weak-to-strong attack using two smaller models (safe and unsafe) to modify a larger safe model's decoding probabilities
3. Evaluate the attack on 5 LLMs from 3 organizations
4. Test on two datasets: AdvBench and MaliciousInstruct
5. Compare results with existing attack methods
6. Propose and evaluate a defense strategy using gradient ascent

Main Results:
1. The weak-to-strong attack achieves >99% attack success rate on both datasets
2. The attack outperforms previous state-of-the-art methods in terms of success rate and harmfulness of generated content
3. The method is effective across different model families and sizes (13B to 70B parameters)
4. The attack generalizes to multiple languages (English, Chinese, French)
5. Even extremely small models (1.3B parameters) can successfully attack much larger models (70B parameters)

Qualitative Analysis:
- The study reveals that current safety measures in LLMs are often only effective for initial tokens, with diminishing divergence for later tokens
- The weak-to-strong attack exploits this vulnerability by steering stronger models to produce harmful responses through distribution shifts induced by weaker models
- The attack's success across different model families and languages suggests a universal vulnerability in current LLM safety alignment techniques

Limitations:
- The study focuses on open-source models for reproducibility, leaving the attack on closed-source models for future work
- The proposed defense strategy, while showing promise, is not fully effective against the attack

Conclusion and Future Work:
The paper concludes that the weak-to-strong jailbreaking attack exposes critical vulnerabilities in LLM safety alignment. The authors propose further research into more advanced defense mechanisms and a deeper exploration of the risks associated with jailbreaking LLMs.

Tools Introduced:
The code for replicating the weak-to-strong jailbreaking method is available at https://github.com/XuandongZhao/weak-to-strong

## Repository Token Information
Total tokens in repository: 27328

Tokens per file:
- eval_asr.py: 404 tokens
- run.py: 1379 tokens
- generate.py: 1952 tokens
- eval_harm.py: 494 tokens
- eval_gpt.py: 2428 tokens
- README.md: 607 tokens
- data/malicious.txt: 1273 tokens
- data/advbench.txt: 7187 tokens
- data/french.txt: 5794 tokens
- data/chinese.txt: 5810 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: Weak-to-Strong Jailbreaking on Large Language Models

## Project Overview

This repository contains the implementation of the weak-to-strong jailbreaking attack on large language models (LLMs) as described in the research paper. The project aims to demonstrate how smaller, less secure models can be used to manipulate the output of larger, more secure models, effectively bypassing their safety measures.

### Project Structure

The repository is organized as follows:

- `run.py`: Main script for running the jailbreaking attack
- `generate.py`: Contains the core logic for generating text using the weak-to-strong method
- `eval_asr.py`: Script for evaluating the attack success rate
- `eval_harm.py`: Script for evaluating the harmfulness of generated content
- `eval_gpt.py`: Script for evaluating generated content using GPT-4
- `data/`: Directory containing datasets for testing (advbench, malicious instructions, and multilingual data)

## Key Components and Functionality

### 1. EmulatorGenerator (generate.py)

The `EmulatorGenerator` class is the core component of the weak-to-strong jailbreaking attack. It implements two main methods:

- `generate()`: Standard text generation using a single model
- `generate_with_ref()`: Implements the weak-to-strong attack by using reference models to modify the target model's output distribution

Key features:
- Uses three models: reference base model, reference fine-tuned model, and target model
- Modifies the target model's output probabilities based on the difference between the reference models
- Implements temperature-based sampling and top-p filtering

### 2. Main Execution (run.py)

The `run.py` script orchestrates the entire attack process:

- Loads the necessary models (reference base, reference fine-tuned, and target)
- Prepares the prompts from the specified dataset
- Executes the generation process using the `EmulatorGenerator`
- Saves the results to a JSON file for further analysis

### 3. Evaluation Scripts

- `eval_asr.py`: Calculates the attack success rate by checking if the generated text contains certain prefixes that indicate safety measures
- `eval_harm.py`: Uses a pre-trained reward model to assess the harmfulness of the generated content
- `eval_gpt.py`: Utilizes GPT-4 to evaluate the generated content for harmfulness and jailbreaking success

## Relation to Research Paper Concepts

The implementation closely follows the methodology described in the paper:

1. **Model Setup**: The code uses three models as described in the paper - a reference base model, a fine-tuned (less safe) reference model, and a target (safe) model.

2. **Probability Modification**: The `generate_with_ref()` method implements the core idea of modifying the target model's output probabilities based on the difference between the reference models.

3. **Efficient Computation**: The attack requires only one forward pass per token, aligning with the paper's emphasis on computational efficiency.

4. **Multilingual Support**: The data directory includes prompts in multiple languages, demonstrating the attack's language-agnostic nature.

5. **Evaluation Metrics**: The evaluation scripts implement the metrics discussed in the paper, including attack success rate and harmfulness assessment.

## Notable Algorithms and Techniques

1. **Weak-to-Strong Probability Modification**:
   ```python
   new_lprobs = ori_lprobs + beta * (ref_finetune_lprobs - ref_base_lprobs)
   ```
   This line in `generate_with_ref()` encapsulates the core of the weak-to-strong attack, modifying the target model's log probabilities.

2. **Temperature-based Sampling with Top-p Filtering**:
   The `sample_next()` and `sample_next_with_ref()` methods implement temperature-based sampling with top-p (nucleus) filtering, ensuring diverse yet controlled text generation.

3. **Attack Success Rate Calculation**:
   The `eval_asr.py` script uses a list of safety-indicating prefixes to determine if the model has been successfully jailbroken, providing a quantitative measure of the attack's effectiveness.

# Potential Enhancements

1. **Adaptive Beta Parameter**:
   Implement an adaptive mechanism for the `beta` parameter that controls the strength of the attack. This could involve dynamically adjusting `beta` based on the content being generated or the model's responses, potentially improving attack success rates while maintaining believability.

2. **Advanced Defense Mechanisms**:
   Extend the project to include more sophisticated defense strategies against the weak-to-strong attack. This could involve techniques like adversarial training, robust optimization, or detection mechanisms that identify and mitigate jailbreaking attempts in real-time.

3. **Cross-Model Transferability Study**:
   Expand the research to study how well the attack transfers across different model architectures and training paradigms. This could provide insights into the fundamental vulnerabilities of LLMs and lead to more generalizable defense strategies.

4. **Fine-grained Content Control**:
   Develop methods for more precise control over the generated content. This could involve incorporating additional conditioning signals or developing a more nuanced understanding of how different prompt components influence the jailbreaking process.

5. **Integration with Continuous Learning Systems**:
   Explore how the weak-to-strong attack interacts with models that undergo continuous learning or updates. This could involve developing techniques for persistent jailbreaking that remain effective as models evolve, or conversely, creating adaptive defense mechanisms that become more robust over time.