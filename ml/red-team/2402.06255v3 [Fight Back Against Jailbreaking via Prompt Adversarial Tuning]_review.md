#Tags
[[Research/Research Papers/2402.06255v3.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** Fight Back Against Jailbreaking via Prompt Adversarial Tuning
**Authors:** Yichuan Mo, Yuji Wang, Zeming Wei, Yisen Wang
**Affiliations:** 
- National Key Lab of General Artificial Intelligence, School of Intelligence Science and Technology, Peking University
- School of Cyber Science and Engineering, Wuhan University
- School of Mathematical Sciences, Peking University
- Institute for Artificial Intelligence, Peking University
**Publication date:** February 9, 2024 (preprint)

Summary:
This paper introduces Prompt Adversarial Tuning (PAT), a novel approach to defend Large Language Models (LLMs) against jailbreak attacks. PAT optimizes a defensive prefix attached to user prompts, effectively reducing the success rate of advanced attacks while maintaining the model's utility on benign tasks.

Key Contributions:
- Introduction of PAT, a min-min optimization framework for prompt tuning to improve jailbreak robustness
- Demonstration of PAT's effectiveness in both grey-box and black-box settings
- Negligible computational overhead for deployment
- Transferability across open-source and closed-source models

Problem Statement:
How to achieve intrinsic robustness against jailbreak attacks in LLMs through prompts while maintaining benign utility?

Methodology:
1. Design of PAT algorithm:
   - Alternating optimization of attack and defense controls
   - Use of greedy coordinate gradient strategy for updating controls
   - Integration of benign and adversarial prompts in the optimization process

2. Experimental setup:
   - Dataset: Combination of Advbench and MS MARCO dataset
   - Models: Vicuna-7B, Llama-2-7B, Mistral-7B, Llama-3-8B, GPT-3.5, GPT-4
   - Attacks: GCG, AutoDAN, ICA, PAIR, TAP
   - Baselines: PPL, ICD, DRO, RPO, SafeDecoding, SmoothLLM, Self-reminder

3. Evaluation metrics:
   - Attack Success Rate (ASR)
   - MT-bench scores for model utility

Main Results:
1. Grey-box setting:
   - PAT reduces ASR of all attacks to nearly 0% while maintaining high MT-bench scores
   - Outperforms all baseline defenses in both robustness and utility preservation

2. Transferability to open-source models:
   - PAT effectively transfers across different model architectures
   - Achieves low ASR even when the surrogate and target models differ

3. Transferability to closed-source models (GPT-3.5 and GPT-4):
   - PAT demonstrates strong performance in defending against jailbreak attacks
   - Maintains higher MT-bench scores compared to baseline methods

4. Real-world attack defense:
   - PAT significantly reduces ASR for multilingual and in-the-wild jailbreak prompts on GPT-3.5 and GPT-4

Qualitative Analysis:
- PAT's success lies in its ability to balance robustness and usability, addressing a critical challenge in LLM security
- The method's transferability across different model architectures suggests a general approach to improving LLM robustness
- PAT's effectiveness against real-world attacks demonstrates its practical applicability in protecting deployed LLM systems

Limitations:
- Potential decrease in effectiveness when facing adaptive attacks with knowledge of the defense strategy
- Ongoing development of jailbreak methods may pose future challenges to the current defense approach

Conclusion and Future Work:
- PAT offers a promising approach to defending LLMs against jailbreak attacks with minimal computational overhead
- Future work may focus on improving robustness against adaptive attacks and exploring applications in other areas of LLM security

Figures:
Figure 1: Pipeline of the proposed method at the inference stage, illustrating how PAT protects LLMs from malicious attacks while maintaining responses to legitimate requests.

New Tool:
Name: PAT (Prompt Adversarial Tuning)
GitHub Repository: https://github.com/rain152/PAT

## Repository Token Information
Total tokens in repository: 29871

Tokens per file:
- llm_attacks/base/attack_manager.py: 16755 tokens
- llm_attacks/gcg/gcg_attack.py: 3234 tokens
- experiments/evaluate_transfer.py: 2003 tokens
- llm_attacks/minimal_gcg/opt_utils.py: 1734 tokens
- llm_attacks/minimal_gcg/string_utils.py: 1280 tokens
- experiments/main.py: 1007 tokens
- experiments/generate_answers.py: 852 tokens
- experiments/collect_data.py: 692 tokens
- experiments/configs/template.py: 478 tokens
- experiments/configs/transfer_vicuna_guanaco.py: 346 tokens
- experiments/configs/transfer_vicuna_multi.py: 283 tokens
- experiments/configs/transfer_vicuna.py: 277 tokens
- experiments/configs/transfer_Guanaco_multi.py: 249 tokens
- experiments/configs/transfer_llama2_multi.py: 186 tokens
- experiments/configs/transfer_all.py: 162 tokens
- experiments/configs/individual_llama2.py: 98 tokens
- experiments/configs/individual_vicuna.py: 87 tokens
- llm_attacks/__init__.py: 84 tokens
- llm_attacks/gcg/__init__.py: 64 tokens
- llm_attacks/base/__init__.py: 0 tokens
- llm_attacks/minimal_gcg/__init__.py: 0 tokens
- experiments/__init__.py: 0 tokens
- experiments/configs/__init__.py: 0 tokens


## Tutorial and Enhancement Suggestions

# PAT (Prompt Adversarial Tuning) Tutorial

## Project Overview

PAT is a novel approach to defend Large Language Models (LLMs) against jailbreak attacks. The project implements a min-min optimization framework for prompt tuning to improve jailbreak robustness while maintaining model utility on benign tasks.

### Project Structure

The repository is organized as follows:

- `llm_attacks/`: Core implementation of attack and defense mechanisms
  - `base/`: Base classes for attack management
  - `gcg/`: Implementation of Greedy Coordinate Gradient (GCG) attack
  - `minimal_gcg/`: Minimal implementation of GCG attack utilities
- `experiments/`: Scripts for running experiments and evaluations
  - `configs/`: Configuration files for different experimental setups
- `data/`: Directory for storing datasets (not included in the repository)

## Key Components

### 1. Attack Manager (`llm_attacks/base/attack_manager.py`)

This file contains the core classes for managing attacks and defenses:

- `AttackPrompt`: Represents a single attack prompt
- `PromptManager`: Manages multiple attack prompts
- `MultiPromptAttack`: Coordinates attacks across multiple prompts and models
- `ProgressiveMultiPromptAttack`: Implements a progressive attack strategy
- `IndividualPromptAttack`: Manages attacks for individual target strings
- `EvaluateAttack`: Evaluates the effectiveness of attacks

These classes handle the generation, optimization, and evaluation of attack and defense prompts.

### 2. GCG Attack (`llm_attacks/gcg/gcg_attack.py`)

This file implements the Greedy Coordinate Gradient (GCG) attack:

- `GCGAttackPrompt`: Extends `AttackPrompt` with GCG-specific functionality
- `GCGPromptManager`: Extends `PromptManager` with GCG-specific prompt sampling
- `GCGMultiPromptAttack`: Implements the GCG attack strategy

The GCG attack iteratively optimizes attack prompts by computing gradients and sampling new tokens.

### 3. Experiment Runner (`experiments/main.py`)

This script is the entry point for running experiments. It:

1. Loads configuration settings
2. Sets up workers and models
3. Initializes attack and defense prompts
4. Runs the attack and defense optimization process
5. Logs results and performance metrics

### 4. Evaluation Script (`experiments/evaluate_transfer.py`)

This script evaluates the transferability of attack and defense prompts across different models.

## Key Concepts and Algorithms

### Min-Min Optimization Framework

The core of PAT is a min-min optimization framework that alternates between optimizing attack and defense prompts. This is implemented in the `MultiPromptAttack.run()` method:

```python
def run(self, n_steps=100, batch_size=1024, topk=256, temp=1, allow_non_ascii=True,
        target_weight=None, control_weight=None, benign_weight=None, refuse_target_weight=None,
        anneal=True, anneal_from=0, prev_loss=np.infty, prev_def_loss=np.infty,
        stop_on_success=True, test_steps=50, log_first=False, filter_cand=True,
        verbose=True, attack_freq=1, defense_freq=1, run_defense=False):
    # ... (initialization code)

    for i in range(n_steps):
        # Attack step
        if i % attack_freq == 0:
            control, loss = self.step(
                batch_size=batch_size,
                topk=topk,
                temp=temp,
                allow_non_ascii=allow_non_ascii,
                target_weight=target_weight_fn(i),
                control_weight=control_weight_fn(i),
                filter_cand=filter_cand,
                verbose=verbose
            )
            # ... (update control prompt)

        # Defense step
        if i % defense_freq == 0 and run_defense:
            def_control, def_loss = self.defense_step(
                batch_size=batch_size,
                topk=topk,
                temp=temp,
                allow_non_ascii=allow_non_ascii,
                target_weight=target_weight_fn(i),
                control_weight=control_weight_fn(i),
                refuse_target_weight=refuse_target_weight,
                benign_weight=benign_weight,
                filter_cand=filter_cand,
                verbose=verbose,
            )
            # ... (update defense prompt)

        # ... (logging and evaluation)
```

### Greedy Coordinate Gradient (GCG) Strategy

The GCG strategy is implemented in the `GCGMultiPromptAttack.step()` method:

```python
def step(self, batch_size=1024, topk=256, temp=1, allow_non_ascii=True, 
         target_weight=1, control_weight=0.1, verbose=False, opt_only=False,
         filter_cand=True):
    # ... (initialization)

    # Compute gradients
    for j, worker in enumerate(self.workers):
        worker(self.prompts[j], "grad", worker.model)

    # Aggregate gradients
    grad = None
    for j, worker in enumerate(self.workers):
        new_grad = worker.results.get().to(main_device) 
        new_grad = new_grad / new_grad.norm(dim=-1, keepdim=True)
        if grad is None:
            grad = torch.zeros_like(new_grad)
        grad += new_grad

    # Sample new control tokens
    with torch.no_grad():
        control_cand = self.prompts[j].sample_control(grad, batch_size, topk, temp, allow_non_ascii)

    # Evaluate candidates
    loss = torch.zeros(len(control_cands) * batch_size).to(main_device)
    with torch.no_grad():
        for j, cand in enumerate(control_cands):
            # ... (compute loss for each candidate)

    # Select best candidate
    min_idx = loss.argmin()
    next_control, cand_loss = control_cands[model_idx][batch_idx], loss[min_idx]

    return next_control, cand_loss.item() / len(self.prompts[0]) / len(self.workers)
```

This strategy computes gradients, samples new control tokens, and selects the best candidate based on the loss.

## Relation to Research Paper

The code implements the key concepts discussed in the research paper:

1. Min-min optimization framework for attack and defense
2. Greedy Coordinate Gradient (GCG) strategy for updating control prompts
3. Integration of benign and adversarial prompts in the optimization process
4. Evaluation of transferability across different model architectures

The `MultiPromptAttack.run()` method implements the alternating optimization process described in the paper, while the `GCGMultiPromptAttack.step()` method implements the GCG strategy for updating prompts.

# Potential Enhancements

1. Adaptive Attack Resistance

Enhance the defense mechanism to be more robust against adaptive attacks that may have knowledge of the PAT strategy. This could involve:
- Implementing a meta-learning approach to anticipate and counteract potential adaptive strategies
- Introducing randomness or uncertainty in the defense prompt generation to make it harder for attackers to predict

2. Multi-Objective Optimization

Extend the optimization framework to explicitly balance multiple objectives:
- Jailbreak prevention
- Benign task performance
- Output diversity
- Computational efficiency

This could be implemented using techniques from multi-objective optimization literature, such as Pareto optimization or weighted sum methods with adaptive weights.

3. Dynamic Prompt Adaptation

Implement a system that can dynamically adapt the defense prompt based on the detected characteristics of the input:
- Develop a classifier to categorize input prompts (e.g., benign, potentially malicious)
- Create a bank of specialized defense prompts for different input types
- Implement a selection mechanism to choose the most appropriate defense prompt in real-time

4. Cross-Lingual and Cross-Domain Generalization

Improve the transferability of PAT across languages and domains:
- Incorporate multilingual pre-training techniques
- Develop domain-specific fine-tuning strategies
- Implement a modular architecture that can easily adapt to new languages or domains

5. Interpretability and Explainability

Enhance the interpretability of the PAT mechanism:
- Implement attention visualization techniques to understand which parts of the input and defense prompts are most influential
- Develop a system to generate human-readable explanations for why a particular input was flagged as potentially malicious
- Create tools for analyzing the evolution of attack and defense prompts during the optimization process

These enhancements would address limitations mentioned in the paper, extend the functionality of PAT, and potentially open up new research directions in the field of LLM security and robustness.