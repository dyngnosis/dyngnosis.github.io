#Tags
[[Research/Research Papers/2402.16717v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0054/LLMJailbreak
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models
**Authors:** Huijie Lv, Xiao Wang, Yuansen Zhang, Caishuang Huang, Shihan Dou, Junjie Ye, Tao Gui, Qi Zhang, Xuanjing Huang
**Affiliations:** School of Computer Science, Fudan University; Institute of Modern Languages and Linguistics, Fudan University
**Publication Date:** February 26, 2024

Summary:
This paper introduces CodeChameleon, a novel jailbreak framework for Large Language Models (LLMs) based on personalized encryption tactics. The authors propose a hypothesis for the safety mechanism of aligned LLMs and develop a method to bypass intent security recognition while ensuring successful query execution.

Key Contributions:
- Proposal of a safety mechanism hypothesis for aligned LLMs
- Introduction of CodeChameleon, a personalized encryption framework for jailbreaking LLMs
- Demonstration of state-of-the-art average Attack Success Rate (ASR) across 7 LLMs
- Achievement of 86.6% ASR on GPT-4-1106

Problem Statement:
The research addresses the challenge of adversarial misuse in LLMs, particularly through 'jailbreaking' that circumvents safety and ethical protocols. The authors aim to understand the mechanisms behind successful attacks and develop a more effective jailbreaking method.

Methodology:
1. Hypothesis formulation: Intent security recognition followed by response generation
2. Development of CodeChameleon framework:
   - Reformulation of tasks into code completion format
   - Creation of personalized encryption functions
   - Embedding of decryption functions within instructions
3. Experimental evaluation:
   - Testing on 7 LLMs, including open-source and proprietary models
   - Comparison with baseline jailbreaking methods
   - Use of Attack Success Rate (ASR) as the primary metric

Main Results:
1. CodeChameleon achieved an average ASR of 77.5% across 7 LLMs
2. 86.6% ASR achieved on GPT-4-1106
3. Consistent outperformance of baseline methods on Llama2 and GPT series models
4. Effectiveness demonstrated across four distinct encryption and decryption functions

Qualitative Analysis:
- The success of CodeChameleon suggests that current safety mechanisms in LLMs are vulnerable to sophisticated encryption-based attacks
- The framework's effectiveness across various LLMs indicates a common weakness in intent recognition systems
- The high ASR on advanced models like GPT-4 highlights the need for more robust safety measures in LLMs

Limitations:
- The study focuses primarily on jailbreaking and does not address potential countermeasures
- Ethical considerations of developing such attack methods are not extensively discussed
- Long-term implications of these vulnerabilities on LLM deployment are not explored

Conclusion and Future Work:
The paper demonstrates the effectiveness of CodeChameleon in bypassing LLM safety mechanisms. The authors suggest that their work highlights the need for more advanced safety protocols in LLMs. Future work may involve developing countermeasures against such encryption-based attacks and exploring the ethical implications of jailbreaking research.

Tools Introduced:
CodeChameleon framework
GitHub repository: https://github.com/huizhang-L/CodeChameleon

## Repository Token Information
Total tokens in repository: 7537

Tokens per file:
- decrypt.py: 567 tokens
- attack.py: 1188 tokens
- template.py: 1698 tokens
- gpt_evaluate.py: 2301 tokens
- utils.py: 602 tokens
- encrypt.py: 681 tokens
- README.md: 500 tokens


## Tutorial and Enhancement Suggestions

# CodeChameleon Tutorial

## Project Overview

CodeChameleon is a framework for jailbreaking Large Language Models (LLMs) using personalized encryption techniques. The project implements the methods described in the research paper to bypass intent security recognition in LLMs while still executing potentially harmful queries.

### Project Structure

The repository contains the following key files:

- `attack.py`: Main script for executing jailbreak attacks
- `decrypt.py`: Implements decryption functions
- `encrypt.py`: Implements encryption functions  
- `template.py`: Defines prompt templates
- `utils.py`: Utility functions
- `gpt_evaluate.py`: Script for evaluating attack results

## Key Components

### 1. Encryption Methods

The `encrypt.py` file implements several encryption methods:

- Binary tree encryption
- Reverse sentence encryption
- Odd-even word reordering
- Length-based word reordering

These methods transform the original query into an encrypted form that aims to bypass intent recognition.

### 2. Decryption Functions

The `decrypt.py` file contains the corresponding decryption functions. These are embedded within the prompts sent to the LLM, allowing it to recover the original query.

### 3. Prompt Templates

`template.py` defines text and code-based prompt templates. The code-based template reformulates the task as a code completion problem using a `ProblemSolver` class.

### 4. Attack Execution

`attack.py` orchestrates the jailbreak attack:

1. Loads and encrypts queries
2. Generates prompts using templates
3. Sends prompts to the target LLM
4. Saves the model's responses

### 5. Evaluation

`gpt_evaluate.py` uses GPT-4 to evaluate the success of jailbreak attempts by analyzing the model's responses against OpenAI's usage policies.

## Key Algorithms and Techniques

### Personalized Encryption

The project implements multiple encryption methods to create a diverse set of transformations. This variety helps overcome potential pattern recognition in LLM safety systems.

### Code-Based Reformulation

By presenting the task as a code completion problem, the system leverages the LLM's code generation capabilities while potentially bypassing content filters designed for natural language.

### Two-Step Process

The attack follows a two-step process:
1. Decrypt the query (using embedded decryption function)
2. Execute the decrypted query

This separation aims to bypass intent recognition while still achieving the desired outcome.

### Automated Evaluation

The use of GPT-4 for evaluating attack success provides a scalable way to assess large numbers of jailbreak attempts against complex safety guidelines.

# Potential Enhancements

1. **Dynamic Encryption Selection**
   - Implement an algorithm to dynamically choose the best encryption method based on query characteristics or past success rates.
   - This could improve overall attack success rates by tailoring the approach to each specific query.

2. **Adversarial Training for Robustness**
   - Develop a training regime that exposes the system to various LLM defenses and safety updates.
   - Use reinforcement learning to evolve attack strategies that remain effective against improving safety measures.

3. **Multi-Modal Jailbreaking**
   - Extend the framework to handle multi-modal inputs (text, images, code) for more sophisticated attacks.
   - This could exploit potential vulnerabilities in how LLMs process different types of data.

4. **Prompt Optimization**
   - Implement genetic algorithms or other optimization techniques to evolve more effective prompt structures.
   - This could lead to the discovery of novel prompt patterns that are particularly effective at bypassing safety measures.

5. **Defensive Applications**
   - Adapt the framework to generate adversarial examples for testing and improving LLM safety systems.
   - Develop a "vaccine" approach where controlled exposure to jailbreak attempts helps models build immunity to such attacks.