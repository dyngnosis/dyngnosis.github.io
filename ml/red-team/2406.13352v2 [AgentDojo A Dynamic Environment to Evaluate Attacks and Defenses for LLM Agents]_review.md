#Tags
[[Research/Research Papers/2406.13352v2.pdf]]

#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0042/VerifyAttack

**Title:** AgentDojo: A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents
**Authors:** Edoardo Debenedetti, Jie Zhang, Mislav Balunović, Luca Beurer-Kellner, Marc Fischer, Florian Tramèr
**Affiliations:** ETH Zurich, Invariant Labs
**Publication Date:** June 19, 2024

Summary:
AgentDojo is an evaluation framework for assessing the adversarial robustness of AI agents that execute tools over untrusted data. It provides a dynamic environment for designing and evaluating new agent tasks, defenses, and adaptive attacks, focusing on prompt injection vulnerabilities.

Key Contributions:
- Introduction of AgentDojo, a dynamic benchmarking framework for AI agent security
- 97 realistic tasks and 629 security test cases across multiple environments
- Evaluation of state-of-the-art LLMs and prompt injection attacks
- Extensible framework for developing new attacks and defenses

Problem Statement:
AI agents combining LLMs with external tool calls are vulnerable to prompt injection attacks, where malicious instructions in tool outputs can hijack the agent's behavior. Existing benchmarks lack the complexity and dynamism needed to evaluate these security risks effectively.

Methodology:
1. Design of four environments: Workspace, Slack, Travel Agency, and e-banking
2. Implementation of 74 tools for agent interaction
3. Creation of user tasks and injection tasks with formal evaluation criteria
4. Development of attack vectors and defense mechanisms
5. Evaluation of multiple LLM-based agents on the benchmark

Main Results:
1. Current LLMs solve less than 66% of AgentDojo tasks without attacks
2. Best-performing agents are vulnerable to attacks in up to 25% of cases
3. Existing defenses (e.g., attack detectors) can reduce attack success rate to 8%
4. Attacks benefit marginally from side information about the system or victim
5. Attacks rarely succeed when the attacker's goal is highly security-sensitive

Qualitative Analysis:
- The inverse scaling law observed in attack success rates suggests that more capable models may be more vulnerable to prompt injections
- The effectiveness of simple defenses like tool filtering highlights the importance of carefully designed agent architectures
- The challenge posed by AgentDojo to both attackers and defenders indicates the need for significant advancements in LLM security

Limitations:
- The current version of AgentDojo does not cover scenarios where users give multiple tasks over time without resetting the agent's context
- The benchmark may not fully capture the capabilities of realistic adversaries in terms of prompt injection constraints

Conclusion and Future Work:
AgentDojo provides a foundation for evaluating and improving the security of AI agents. Future research should focus on developing new agent and defense designs to enhance utility and robustness. Significant breakthroughs in LLMs' ability to distinguish instructions from data will be necessary to thwart stronger, adaptive attacks.

New Tool:
Name: AgentDojo
GitHub Repository: https://github.com/ethz-spylab/agentdojo

Relevant Figures:
1. Figure 1: Illustration of AgentDojo's evaluation framework
2. Figure 6: Plots of agent utility vs. attack success rates
3. Figure 9: Evaluation of prompt injection defenses