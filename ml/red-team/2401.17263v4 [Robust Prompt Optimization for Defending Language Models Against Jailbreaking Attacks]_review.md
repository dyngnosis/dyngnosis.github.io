#Tags
[[Research/Research Papers/2401.17263v4.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection

**Title:** Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks
**Authors:** Andy Zhou, Bo Li, Haohan Wang
**Affiliations:** Lapis Labs, University of Illinois at Urbana-Champaign, University of Chicago
**Publication Date:** January 30, 2024

Summary:
This paper proposes a novel defense mechanism called Robust Prompt Optimization (RPO) to protect large language models (LLMs) against jailbreaking attacks. RPO uses an optimization-based objective and algorithm to create robust system-level defenses that can adapt to worst-case adaptive attacks.

Key Contributions:
- Formalization of a minimax defensive objective for LLM robustness
- Development of the RPO algorithm to optimize for this objective
- Theoretical analysis showing improved robustness to both seen and unknown jailbreaks
- Experimental results demonstrating state-of-the-art performance in reducing attack success rates

Problem Statement:
Despite advances in AI alignment, LLMs remain vulnerable to jailbreaking attacks where adversaries modify prompts to induce unwanted behavior. Existing defenses have not been adapted to newly proposed attacks and more challenging threat models.

Methodology:
1. Formalize a minimax defensive objective motivated by adversarial training
2. Develop RPO algorithm using discrete optimization to create robust system-level defenses
3. Optimize a lightweight and transferable suffix to enforce safe outputs
4. Evaluate on JailbreakBench and HarmBench, covering various harmful risk categories and attack methods
5. Compare against baseline defenses on open-source and closed-source models

Main Results:
1. RPO reduces attack success rate (ASR) on GPT-4 to 6% and Llama-2 to 0% on JailbreakBench
2. Outperforms existing defenses for both open-source and closed-source models
3. RPO suffixes transfer across models and to unknown attacks
4. Minimal impact on benign prompts and negligible inference cost

Qualitative Analysis:
- RPO's effectiveness stems from directly incorporating the adversary into the defensive objective
- The approach's ability to adapt to worst-case adaptive attacks contributes to its robustness
- Transferability of RPO suffixes across models and attack types suggests a generalizable defense mechanism

Limitations:
- Potential for stronger attacks to be developed in response to this defense
- Does not cover multimodal models or other failure modes like deception and malicious code generation
- Small performance reduction observed on benign use cases (MT-Bench)

Conclusion and Future Work:
The paper demonstrates the effectiveness of RPO in defending LLMs against jailbreaking attacks, setting a new state-of-the-art. Future directions include optimizing defenses on a greater variety of attacks, combining various defenses into comprehensive guardrails, and red-teaming to discover new security risks.

Relevant Figures/Tables:
Table 1: Attack success rate of RPO and baseline defenses on JailbreakBench
Table 2: Transfer attack success rate of RPO on HarmBench attacks

New Tools:
Robust Prompt Optimization (RPO) algorithm
GitHub repository: https://github.com/lapisrocks/rpo

## Repository Token Information
Total tokens in repository: 22217

Tokens per file:
- setup.py: 295 tokens
- demo.py: 2275 tokens
- experiments/__init__.py: 0 tokens
- experiments/test.py: 1596 tokens
- experiments/main.py: 712 tokens
- experiments/configs/transfer_starling.py: 167 tokens
- experiments/configs/transfer_llama.py: 169 tokens
- experiments/configs/template.py: 444 tokens
- experiments/configs/__init__.py: 0 tokens
- rpo/string_utils.py: 1465 tokens
- rpo/opt_utils.py: 2342 tokens
- rpo/gcg.py: 1626 tokens
- rpo/__init__.py: 109 tokens
- rpo/suffix_manager.py: 11017 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: Robust Prompt Optimization (RPO) for Defending Language Models

## 1. Project Overview

The Robust Prompt Optimization (RPO) project implements a novel defense mechanism to protect large language models (LLMs) against jailbreaking attacks. The main goal is to create robust system-level defenses that can adapt to worst-case adaptive attacks by optimizing a lightweight and transferable suffix to enforce safe outputs.

### Project Structure

The repository is organized as follows:

- `rpo/`: Core implementation of the RPO algorithm and related utilities
- `experiments/`: Scripts and configurations for running experiments
- `demo.py`: A demonstration script showing how to use RPO with LLaMA-2
- `setup.py`: Package installation script

## 2. Key Components

### 2.1 RPO Core (`rpo/`)

#### `suffix_manager.py`

This file contains the core classes for managing prompts and attacks:

- `AttackPrompt`: Represents an individual attack prompt
- `PromptManager`: Manages multiple attack prompts
- `MultiPromptAttack`: Coordinates attacks across multiple prompts and models
- `ProgressiveMultiPromptAttack`: Implements a progressive attack strategy

#### `opt_utils.py`

Provides utility functions for optimization, including:

- `token_gradients`: Computes gradients of the loss with respect to input tokens
- `sample_control`: Samples new control tokens based on gradients
- `get_filtered_cands`: Filters and processes candidate control tokens

#### `gcg.py`

Implements the Greedy Coordinate Gradient (GCG) attack method:

- `GCGAttackPrompt`: Extends `AttackPrompt` with GCG-specific functionality
- `GCGPromptManager`: Manages GCG-specific prompt operations
- `GCGMultiPromptAttack`: Coordinates GCG attacks across multiple prompts

### 2.2 Experiments (`experiments/`)

- `main.py`: Entry point for running experiments
- `test.py`: Contains test scripts for evaluating the RPO defense
- `configs/`: Configuration files for different experimental setups

### 2.3 Demo (`demo.py`)

Provides a step-by-step demonstration of using RPO to defend LLaMA-2 against jailbreaking attacks.

## 3. Key Concepts and Algorithms

### 3.1 Robust Prompt Optimization (RPO)

The core idea of RPO is to optimize a defensive suffix that can be appended to user prompts to prevent jailbreaking attacks. This is achieved through a minimax optimization process:

1. An adversary tries to find the worst-case attack prompt
2. The defender optimizes the suffix to minimize the effectiveness of this attack

### 3.2 Progressive Multi-Prompt Attack

The `ProgressiveMultiPromptAttack` class implements a strategy where:

1. The attack starts with a single goal and model
2. Gradually increases the number of goals and models
3. Adjusts the control weight over time

This approach allows the defense to adapt to increasingly complex attack scenarios.

### 3.3 Greedy Coordinate Gradient (GCG) Attack

The GCG attack method, implemented in `gcg.py`, works by:

1. Computing token gradients for the current prompt
2. Sampling new tokens based on these gradients
3. Selecting the best candidates to form a new attack prompt

### 3.4 Token Gradient Computation

The `token_gradients` function in `opt_utils.py` is crucial for the optimization process:

1. Creates a one-hot encoding of input tokens
2. Computes embeddings and forward pass through the model
3. Calculates loss and backpropagates to get gradients
4. Returns gradients with respect to input tokens

## 4. Using the Code

To use RPO for defending a language model:

1. Initialize a `PromptManager` with your goals, targets, and model details
2. Create a `ProgressiveMultiPromptAttack` instance
3. Run the attack using the `run` method, which will optimize the defensive suffix
4. Use the resulting suffix to augment user prompts for improved safety

Example (simplified):

```python
prompt_manager = PromptManager(jailbreaks, goals, targets, adv_targets, tokenizer, conv_template)
attack = ProgressiveMultiPromptAttack(jailbreaks, goals, targets, adv_targets, workers, prompt_manager)
optimized_suffix, steps = attack.run(n_steps=1000, batch_size=512)
```

## 5. Relation to the Research Paper

The implementation closely follows the methodology described in the paper:

- The minimax defensive objective is realized through the progressive attack and optimization process
- The RPO algorithm is implemented using discrete optimization techniques (e.g., GCG)
- The code supports evaluation on various benchmarks (JailbreakBench, HarmBench) as mentioned in the paper
- The transferability of RPO suffixes is facilitated by the `ProgressiveMultiPromptAttack` class

# Potential Enhancements

1. **Multimodal Model Support**
   - Extend RPO to work with multimodal models (e.g., image-text models)
   - Implement gradient computation and optimization for non-text modalities
   - Develop benchmarks for multimodal jailbreaking attacks

2. **Dynamic Suffix Adaptation**
   - Implement real-time adaptation of the defensive suffix based on ongoing interactions
   - Develop a feedback loop that continuously updates the suffix using live data
   - Explore reinforcement learning techniques for dynamic suffix optimization

3. **Improved Efficiency and Scalability**
   - Optimize the token gradient computation for larger models and longer sequences
   - Implement distributed computing support for running RPO on multiple GPUs or machines
   - Develop pruning techniques to reduce the computational cost of the optimization process

4. **Integration with Other Defense Mechanisms**
   - Combine RPO with other defense techniques (e.g., constitutional AI, RLHF)
   - Develop a comprehensive defense framework that leverages multiple strategies
   - Implement adaptive selection of defense mechanisms based on the detected attack type

5. **Explainable RPO**
   - Develop visualization tools to understand how the optimized suffix affects model behavior
   - Implement techniques to interpret the importance of different tokens in the defensive suffix
   - Create a human-readable explanation of why certain prompts are classified as attacks

These enhancements would address limitations mentioned in the paper, extend the functionality to new domains, and incorporate recent advancements in AI safety and robustness.