#Tags
[[Research/Research Papers/2410.16222v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection

**Title:** A Realistic Threat Model for Large Language Model Jailbreaks
**Authors:** Valentyn Boreiko, Alexander Panfilov, Vaclav Voracek, Matthias Hein, Jonas Geiping
**Affiliations:** University of Tübingen, Tübingen AI Center, Max Planck Institute for Intelligent Systems, ELLIS Institute Tübingen
**Publication Date:** October 21, 2024

Summary:
This paper proposes a unified threat model for comparing jailbreaking attacks on large language models (LLMs). The model combines perplexity constraints to measure deviation from natural text and computational budget limitations. The authors use an N-gram model built on 1T tokens for LLM-agnostic and interpretable evaluation, adapting popular attacks to this new threat model.

Key Contributions:
- Introduction of a realistic and interpretable threat model for LLM jailbreaks
- Development of an N-gram language model perplexity filter for evaluating attack naturalness
- Adaptation of popular jailbreaking attacks to the proposed threat model
- Comprehensive benchmarking and analysis of jailbreaking attacks under the new threat model

Problem Statement:
Existing jailbreaking attacks on LLMs lack a standardized threat model, making it difficult to compare their effectiveness and practicality. The paper aims to address this issue by proposing a unified framework for evaluating and comparing these attacks.

Methodology:
1. Construction of an N-gram language model on 1T tokens from diverse datasets
2. Development of a perplexity filter based on the N-gram model
3. Adaptation of popular jailbreaking attacks (GCG, PRS, PAIR, AutoDan, BEAST) to the new threat model
4. Evaluation of attacks on various LLMs, including Llama2, Llama3, Gemma, and others
5. Analysis of attack performance, computational cost, and perplexity constraints

Main Results:
1. Attack success rates against safety-tuned models are lower than previously reported when constrained by the realistic threat model
2. Discrete optimization-based attacks (e.g., PRS, GCG) outperform recent LLM-based attacks in terms of success rate and computational efficiency
3. Effective attacks exploit infrequent N-grams, either selecting N-grams absent from real-world text or rare ones specific to certain datasets (e.g., code)
4. The proposed threat model allows for comprehensive analysis and comparison of jailbreak attacks across different LLMs

Qualitative Analysis:
- The paper demonstrates that many existing jailbreaking attacks rely on unnatural text patterns, which may not be practical in real-world scenarios
- The proposed threat model provides a more realistic assessment of attack effectiveness by considering both text naturalness and computational constraints
- The superior performance of discrete optimization-based attacks suggests that simpler, more targeted approaches may be more effective than complex LLM-based methods for jailbreaking

Limitations:
- The study focuses on a specific set of LLMs and may not generalize to all models or future architectures
- The N-gram model, while interpretable, may not capture all aspects of text naturalness that more advanced language models can represent
- The computational budget constraint may not accurately reflect all real-world attack scenarios

Conclusion and Future Work:
The paper concludes that the proposed threat model provides a more realistic and interpretable framework for evaluating jailbreaking attacks on LLMs. Future work suggestions include:
1. Exploring more advanced naturalness metrics while maintaining interpretability
2. Investigating the transferability of attacks across different LLM architectures
3. Developing more robust defense mechanisms based on insights from the threat model

Relevant Figures:
- Figure 1: Visualization of the threat model, showing the trade-off between perplexity and attack success rate
- Figure 2: Comparison of N-gram model perplexity with LLM-based perplexity
- Figure 4: Performance comparison of adaptive attacks under the proposed threat model

New Tools:
The authors mention that their code is available at https://github.com/valentyn1boreiko/llm-threat-model, which likely includes implementations of the N-gram model perplexity filter and adapted jailbreaking attacks.