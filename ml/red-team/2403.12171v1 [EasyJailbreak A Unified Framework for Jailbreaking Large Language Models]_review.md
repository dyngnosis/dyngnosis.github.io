#Tags
[[Research/Research Papers/2403.12171v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0016/ObtainCapabilities
#AMLT0017/DevelopCapabilities

**Title:** EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models
**Authors:** Weikang Zhou, Xiao Wang, Limao Xiong, Han Xia, Yingshuang Gu, et al.
**Affiliation:** School of Computer Science, Fudan University; Institute of Modern Languages and Linguistics, Fudan University; Shanghai AI Laboratory
**Publication Date:** March 18, 2024

Key Contributions:
- Introduction of EasyJailbreak, a unified framework for constructing and evaluating jailbreak attacks against LLMs
- Implementation of 11 distinct jailbreak methods within the framework
- Comprehensive security evaluation of 10 different LLMs using EasyJailbreak
- Release of resources including a web platform, PyPI package, and experimental outputs

Problem Statement:
The lack of a standardized implementation framework for jailbreak methods limits comprehensive security evaluations of Large Language Models (LLMs).

Methodology:
1. Framework Design:
   - Four components: Selector, Mutator, Constraint, and Evaluator
   - Modular architecture allowing easy combination of novel and existing components

2. Jailbreak Methods:
   - Implementation of 11 distinct jailbreak methods
   - Categories: Human Design, Long-tail Encoding, Prompt Optimization

3. Evaluation:
   - Testing on 10 LLMs from 7 different institutions
   - Use of GenerativeJudge with GPT-4-turbo-1106 as the scoring model
   - Evaluation prompts from GPTFUZZER

Main Results:
1. Average breach probability of 60% across all tested models
2. GPT-3.5-Turbo and GPT-4 exhibited ASRs of 57% and 33% respectively
3. Closed-source models (GPT-3.5-Turbo and GPT-4) showed lower average ASR (45%) compared to open-source models (66%)
4. Increased model size did not necessarily lead to improved security

Qualitative Analysis:
- The study reveals significant vulnerabilities in contemporary LLMs, emphasizing the need for enhanced security measures
- The modular framework of EasyJailbreak allows for easy construction and evaluation of various jailbreak methods, potentially accelerating research in this area
- The performance difference between closed-source and open-source models suggests that proprietary training techniques may offer some security advantages

Limitations:
- The study focuses on a limited set of models and jailbreak methods
- The effectiveness of different evaluation methods varies, potentially affecting the accuracy of results

Conclusion and Future Work:
- EasyJailbreak provides a valuable tool for researchers to improve LLM security
- The framework's modular design encourages innovation in both attack and defense strategies
- Future work may include security validations for larger-scale models and development of more robust defense mechanisms

Tools Introduced:
1. EasyJailbreak Framework
   - Web platform: http://easyjailbreak.org/
   - PyPI package: https://pypi.org/project/easyjailbreak/
   - GitHub repository: https://github.com/EasyJailbreak/EasyJailbreak

## Repository Token Information
Total tokens in repository: 131015

Tokens per file:
- setup.py: 303 tokens
- test/settings.py: 52 tokens
- test/__init__.py: 5 tokens
- test/seed/test_seed_random.py: 376 tokens
- test/seed/test_seed_template.py: 452 tokens
- test/seed/test_seed_llm.py: 366 tokens
- test/seed/__init__.py: 0 tokens
- test/selector/test_randomselector.py: 190 tokens
- test/selector/__init__.py: 0 tokens
- test/selector/test_SelectorOnScore.py: 331 tokens
- test/selector/test_EXP3Selector.py: 343 tokens
- test/metrics/__init__.py: 0 tokens
- test/metrics/Metric/test_ASR.py: 219 tokens
- test/metrics/Metric/test_Perplexity.py: 179 tokens
- test/metrics/evaluator/test_EvaluatorMatch.py: 258 tokens
- test/metrics/evaluator/test_EvaluatorClassificationJudge.py: 262 tokens
- test/constraint/__init__.py: 0 tokens
- test/models/test_openaimodel.py: 368 tokens
- test/datasets/test_jailbreakdataset.py: 253 tokens
- test/datasets/test_instance.py: 556 tokens
- test/mutation/test_Disemvowel.py: 325 tokens
- test/mutation/test_Auto_payload_splitting.py: 356 tokens
- test/mutation/test_CaesarExpert.py: 1058 tokens
- test/mutation/test_Base64_input_only.py: 378 tokens
- test/mutation/test_Combination_3.py: 503 tokens
- test/mutation/test_Auto_obfuscation.py: 353 tokens
- test/mutation/test_Base64.py: 321 tokens
- test/mutation/test_Base64_raw.py: 319 tokens
- test/mutation/test_ReplaceWordsWithSynonyms.py: 327 tokens
- test/mutation/test_Combination_2.py: 493 tokens
- test/mutation/test_rule_Crossover.py: 457 tokens
- test/mutation/test_AsciiExpert.py: 2142 tokens
- test/mutation/test_Leetspeak.py: 321 tokens
- test/mutation/test_Rot13.py: 339 tokens
- test/mutation/test_MorseExpert.py: 1899 tokens
- test/mutation/__init__.py: 156 tokens
- test/mutation/test_Combination_1.py: 460 tokens
- test/mutation/test_SelfDefineCipher.py: 829 tokens
- easyjailbreak/__init__.py: 50 tokens
- easyjailbreak/seed/seed_llm.py: 456 tokens
- easyjailbreak/seed/seed_template.py: 567 tokens
- easyjailbreak/seed/__init__.py: 34 tokens
- easyjailbreak/seed/seed_base.py: 161 tokens
- easyjailbreak/seed/seed_random.py: 369 tokens
- easyjailbreak/selector/RandomSelector.py: 225 tokens
- easyjailbreak/selector/RoundRobinSelectPolicy.py: 330 tokens
- easyjailbreak/selector/ReferenceLossSelector.py: 837 tokens
- easyjailbreak/selector/UCBSelectPolicy.py: 541 tokens
- easyjailbreak/selector/selector.py: 338 tokens
- easyjailbreak/selector/__init__.py: 16 tokens
- easyjailbreak/selector/MCTSExploreSelectPolicy.py: 813 tokens
- easyjailbreak/selector/SelectBasedOnScores.py: 642 tokens
- easyjailbreak/selector/EXP3SelectPolicy.py: 675 tokens
- easyjailbreak/metrics/__init__.py: 31 tokens
- easyjailbreak/metrics/Metric/metric.py: 298 tokens
- easyjailbreak/metrics/Metric/metric_perplexit.py: 698 tokens
- easyjailbreak/metrics/Metric/metric_ASR.py: 358 tokens
- easyjailbreak/metrics/Metric/__init__.py: 27 tokens
- easyjailbreak/metrics/Evaluator/Evaluator_PrefixExactMatch.py: 273 tokens
- easyjailbreak/metrics/Evaluator/Evaluator.py: 318 tokens
- easyjailbreak/metrics/Evaluator/Evaluator_ClassificationJudge.py: 930 tokens
- easyjailbreak/metrics/Evaluator/Evaluator_GenerativeJudge.py: 1279 tokens
- easyjailbreak/metrics/Evaluator/__init__.py: 99 tokens
- easyjailbreak/metrics/Evaluator/Evaluator_GenerativeGetScore.py: 1390 tokens
- easyjailbreak/metrics/Evaluator/Evaluator_Match.py: 317 tokens
- easyjailbreak/metrics/Evaluator/Evaluator_ClassificationGetScore.py: 2032 tokens
- easyjailbreak/metrics/Evaluator/Evaluator_PatternJudge.py: 879 tokens
- easyjailbreak/constraint/PerplexityConstraint.py: 1141 tokens
- easyjailbreak/constraint/ConstraintBase.py: 272 tokens
- easyjailbreak/constraint/DeleteHarmLess.py: 724 tokens
- easyjailbreak/constraint/__init__.py: 20 tokens
- easyjailbreak/constraint/DeleteOffTopic.py: 1207 tokens
- easyjailbreak/loggers/logger.py: 380 tokens
- easyjailbreak/loggers/__init__.py: 0 tokens
- easyjailbreak/attacker/ICA_wei_2023.py: 1124 tokens
- easyjailbreak/attacker/PAIR_chao_2023.py: 3075 tokens
- easyjailbreak/attacker/Jailbroken_wei_2023.py: 884 tokens
- easyjailbreak/attacker/DeepInception_Li_2023.py: 963 tokens
- easyjailbreak/attacker/attacker_base.py: 516 tokens
- easyjailbreak/attacker/TAP_Mehrotra_2023.py: 2675 tokens
- easyjailbreak/attacker/CodeChameleon_2024.py: 818 tokens
- easyjailbreak/attacker/__init__.py: 190 tokens
- easyjailbreak/attacker/MJP_Li_2023.py: 1536 tokens
- easyjailbreak/attacker/Cipher_Yuan_2023.py: 1157 tokens
- easyjailbreak/attacker/GCG_Zou_2023.py: 1203 tokens
- easyjailbreak/attacker/Multilingual_Deng_2023.py: 1282 tokens
- easyjailbreak/attacker/AutoDAN_Liu_2023.py: 6421 tokens
- easyjailbreak/attacker/Gptfuzzer_yu_2023.py: 1960 tokens
- easyjailbreak/attacker/ReNeLLM_ding_2023.py: 1437 tokens
- easyjailbreak/utils/model_utils.py: 5613 tokens
- easyjailbreak/utils/__init__.py: 0 tokens
- easyjailbreak/models/openai_model.py: 577 tokens
- easyjailbreak/models/huggingface_model.py: 2638 tokens
- easyjailbreak/models/model_base.py: 768 tokens
- easyjailbreak/models/__init__.py: 105 tokens
- easyjailbreak/models/wenxinyiyan_model.py: 895 tokens
- easyjailbreak/datasets/instance.py: 1204 tokens
- easyjailbreak/datasets/__init__.py: 17 tokens
- easyjailbreak/datasets/jailbreak_datasets.py: 1849 tokens
- easyjailbreak/mutation/__init__.py: 17 tokens
- easyjailbreak/mutation/mutation_base.py: 478 tokens
- easyjailbreak/mutation/gradient/token_gradient.py: 1761 tokens
- easyjailbreak/mutation/gradient/__init__.py: 0 tokens
- easyjailbreak/mutation/generation/ApplyGPTMutation.py: 726 tokens
- easyjailbreak/mutation/generation/Translation.py: 486 tokens
- easyjailbreak/mutation/generation/IntrospectGeneration.py: 2403 tokens
- easyjailbreak/mutation/generation/GenerateSimilar.py: 626 tokens
- easyjailbreak/mutation/generation/InsertMeaninglessCharacters.py: 472 tokens
- easyjailbreak/mutation/generation/AlterSentenceStructure.py: 485 tokens
- easyjailbreak/mutation/generation/__init__.py: 100 tokens
- easyjailbreak/mutation/generation/ChangeStyle.py: 376 tokens
- easyjailbreak/mutation/generation/Rephrase.py: 634 tokens
- easyjailbreak/mutation/generation/historical_insight.py: 571 tokens
- easyjailbreak/mutation/generation/Expand.py: 524 tokens
- easyjailbreak/mutation/generation/Crossover.py: 653 tokens
- easyjailbreak/mutation/generation/MisspellSensitiveWords.py: 524 tokens
- easyjailbreak/mutation/generation/Shorten.py: 628 tokens
- easyjailbreak/mutation/rule/Translate.py: 829 tokens
- easyjailbreak/mutation/rule/Combination_1.py: 383 tokens
- easyjailbreak/mutation/rule/OddEven.py: 1113 tokens
- easyjailbreak/mutation/rule/Base64_input_only.py: 254 tokens
- easyjailbreak/mutation/rule/Combination_3.py: 425 tokens
- easyjailbreak/mutation/rule/Artificial.py: 231 tokens
- easyjailbreak/mutation/rule/ReplaceWordsWithSynonyms.py: 539 tokens
- easyjailbreak/mutation/rule/MorseExpert.py: 7743 tokens
- easyjailbreak/mutation/rule/Base64.py: 251 tokens
- easyjailbreak/mutation/rule/AsciiExpert.py: 7069 tokens
- easyjailbreak/mutation/rule/CaserExpert.py: 7179 tokens
- easyjailbreak/mutation/rule/Reverse.py: 966 tokens
- easyjailbreak/mutation/rule/SelfDefineCipher.py: 7196 tokens
- easyjailbreak/mutation/rule/Inception.py: 220 tokens
- easyjailbreak/mutation/rule/__init__.py: 375 tokens
- easyjailbreak/mutation/rule/Auto_payload_splitting.py: 258 tokens
- easyjailbreak/mutation/rule/Combination_2.py: 416 tokens
- easyjailbreak/mutation/rule/Length.py: 1121 tokens
- easyjailbreak/mutation/rule/MJPChoices.py: 1957 tokens
- easyjailbreak/mutation/rule/BinaryTree.py: 1413 tokens
- easyjailbreak/mutation/rule/Base64_raw.py: 241 tokens
- easyjailbreak/mutation/rule/Auto_obfuscation.py: 264 tokens
- easyjailbreak/mutation/rule/Leetspeak.py: 315 tokens
- easyjailbreak/mutation/rule/Disemvowel.py: 256 tokens
- easyjailbreak/mutation/rule/Crossover.py: 843 tokens
- easyjailbreak/mutation/rule/Rot13.py: 281 tokens
- examples/run_autodan.py: 390 tokens
- examples/run_gcg.py: 270 tokens
- examples/run_gptfuzzer.py: 348 tokens
- examples/run_PAIR.py: 234 tokens
- examples/run_mjp.py: 202 tokens
- examples/run_renellm.py: 306 tokens
- examples/run_deepinception.py: 338 tokens
- examples/run_ica.py: 365 tokens
- examples/run_multilingual.py: 318 tokens
- examples/run_cipher.py: 316 tokens
- examples/run_codechameleon.py: 422 tokens
- examples/run_jailbroken.py: 322 tokens
- examples/run_TAP.py: 399 tokens


## Tutorial and Enhancement Suggestions

# EasyJailbreak Tutorial

## Project Overview

EasyJailbreak is a unified framework for constructing and evaluating jailbreak attacks against Large Language Models (LLMs). The project implements 11 distinct jailbreak methods and provides tools for comprehensive security evaluation of LLMs.

### Project Structure

The repository is organized into several key directories:

- `easyjailbreak/`: Core implementation of the framework
  - `attacker/`: Various jailbreak attack implementations
  - `constraint/`: Constraints for filtering generated prompts
  - `datasets/`: Dataset handling and storage
  - `metrics/`: Evaluation metrics and judges
  - `models/`: Interfaces for different LLM backends
  - `mutation/`: Prompt mutation strategies
  - `seed/`: Seed generation for initial prompts
  - `selector/`: Selection policies for choosing prompts
- `examples/`: Example scripts demonstrating usage
- `test/`: Unit tests for components

## Key Components

### 1. Attackers

Attackers implement specific jailbreak strategies. For example, `PAIR_chao_2023.py` implements the PAIR (Prompt Automatic Iterative Refinement) method:

```python
class PAIR(AttackerBase):
    def __init__(self, attack_model, target_model, eval_model, jailbreak_datasets, ...):
        # Initialize attack parameters
    
    def single_attack(self, instance: Instance):
        # Implement PAIR attack logic
    
    def attack(self):
        # Orchestrate attack across dataset
```

### 2. Mutations

Mutations alter prompts to bypass LLM safeguards. The `mutation/` directory contains various strategies:

```python
class Base64(MutationBase):
    def _get_mutated_instance(self, instance) -> List[Instance]:
        # Encode prompt using Base64
```

### 3. Selectors

Selectors choose which prompts to use or refine. For example, `UCBSelectPolicy` implements an Upper Confidence Bound algorithm:

```python
class UCBSelectPolicy(SelectPolicy):
    def select(self) -> JailbreakDataset:
        # Select instance using UCB formula
```

### 4. Evaluators

Evaluators determine if an attack was successful. `EvaluatorGenerativeJudge` uses another LLM to assess outputs:

```python
class EvaluatorGenerativeJudge(Evaluator):
    def judge(self, seed: str) -> bool:
        # Use LLM to determine if output is harmful
```

### 5. Constraints

Constraints filter generated prompts. `DeleteHarmLess` removes non-harmful prompts:

```python
class DeleteHarmLess(ConstraintBase):
    def __call__(self, jailbreak_dataset, *args, **kwargs) -> JailbreakDataset:
        # Filter dataset, keeping only potentially harmful prompts
```

## Key Algorithms

### Monte Carlo Tree Search (MCTS)

The `MCTSExploreSelectPolicy` implements MCTS for exploring the prompt space:

```python
class MCTSExploreSelectPolicy(SelectPolicy):
    def select(self) -> JailbreakDataset:
        # MCTS selection logic
    
    def update(self, Dataset: JailbreakDataset):
        # Update MCTS tree based on results
```

### Gradient-based Optimization

The `GCG` (Greedy Coordinate Gradient) attacker uses token gradients to optimize prompts:

```python
class GCG(AttackerBase):
    def attack(self):
        # Iteratively optimize prompts using gradient information
```

## Usage Example

Here's a basic example of using the framework:

```python
from easyjailbreak.attacker import PAIR
from easyjailbreak.datasets import JailbreakDataset
from easyjailbreak.models import from_pretrained, OpenaiModel

# Load models and dataset
attack_model = from_pretrained('vicuna-13b-v1.5', 'vicuna_v1.1')
target_model = OpenaiModel('gpt-4', api_key='...')
eval_model = OpenaiModel('gpt-4', api_key='...')
dataset = JailbreakDataset('AdvBench')

# Initialize attacker
attacker = PAIR(attack_model, target_model, eval_model, dataset)

# Run attack
attacker.attack()

# Save results
attacker.attack_results.save_to_jsonl('results.jsonl')
```

# Potential Enhancements

1. **Adaptive Attack Strategies**: Implement a meta-learning approach that dynamically selects and combines different attack strategies based on their effectiveness against specific LLMs. This could involve using reinforcement learning to optimize the attack pipeline.

2. **Improved Evaluation Metrics**: Develop more nuanced evaluation metrics that consider factors beyond binary success/failure, such as the subtlety of the jailbreak, the diversity of generated harmful content, or the consistency of the model's harmful outputs.

3. **Multi-modal Jailbreaks**: Extend the framework to support multi-modal inputs, allowing for jailbreak attempts that combine text with images, audio, or other data types. This could uncover new vulnerabilities in multi-modal LLMs.

4. **Defensive Techniques**: Incorporate methods for improving LLM robustness against jailbreak attacks. This could include adversarial training, prompt engineering for safety, or dynamic safety layers that adapt to new attack patterns.

5. **Scalability and Efficiency**: Optimize the framework for large-scale evaluations, potentially leveraging distributed computing or more efficient sampling techniques. This would allow for more comprehensive testing of LLMs and faster iteration on attack strategies.