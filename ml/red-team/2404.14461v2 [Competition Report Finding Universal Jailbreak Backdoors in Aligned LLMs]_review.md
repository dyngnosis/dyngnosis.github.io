#Tags
[[Research/Research Papers/2404.14461v2.pdf]]

#AMLT0018/BackdoorMLModel
#AMLT0020/PoisonTrainingData
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs
**Authors:** Javier Rando, Francesco Croce, Kryštof Mitka, Stepan Shabalin, Maksym Andriushchenko, Nicolas Flammarion, Florian Tramèr
**Affiliations:** ETH Zurich, EPFL, University of Twente, Georgia Institute of Technology
**Publication Date:** April 22, 2024

Summary:
This paper reports on a competition to find universal jailbreak backdoors in aligned large language models (LLMs). The competition challenged participants to discover backdoors that, when added to any prompt, enable harmful responses from otherwise safe models.

Key Contributions:
- Organized a competition to find universal jailbreak backdoors in aligned LLMs
- Released the first suite of universally backdoored models and datasets for future research
- Summarized key findings and promising ideas for future research in this area

Problem Statement:
How can universal jailbreak backdoors be detected in aligned large language models that have been poisoned during the safety training process?

Methodology:
1. Created 5 instances of LLaMA-2 (7B) fine-tuned and poisoned to become aligned chatbots
2. Used RLHF on poisoned harmless datasets with a high poisoning rate (25%)
3. Provided participants with:
   - Poisoned models
   - Harmless Anthropic dataset (training, validation, test splits)
   - Reward model to measure harmfulness of generations
   - Starting codebase with basic functionalities
4. Participants had to find backdoor strings that, when appended to questions, elicit harmful responses

Main Results:
1. Submissions did not outperform the inserted trojans, suggesting injected backdoors are a strong upperbound for undesired behavior
2. Some teams found backdoors very close to the inserted ones, despite the large search space
3. Various methods were successful, including:
   - Embedding difference analysis across models
   - Genetic algorithms
   - Gradient-based optimization

Qualitative Analysis:
1. The competition demonstrated that different approaches can be effective in finding backdoors
2. The ability to find backdoors close to the inserted ones suggests there may be detectable properties of these backdoors
3. The competition highlights the potential vulnerability of the alignment process to poisoning attacks

Limitations:
1. High poisoning rate (25%) used, which may not be realistic in practice
2. Limited to LLaMA-2 (7B) models
3. Focused on a specific type of backdoor (universal jailbreak)

Conclusion and Future Work:
1. Released the first suite of universally backdoored LLMs for future research
2. Suggested promising research directions:
   - Developing methods that don't assume access to equivalent models trained without triggers
   - Exploring mechanistic interpretability for backdoor detection
   - Using poisoning to better localize harmful capabilities
   - Enhancing "unlearning" techniques with competition findings
   - Studying the effect of poisoning rate on backdoor detectability

New Tools:
- The authors released a suite of universally backdoored models and datasets, available at: https://github.com/ethz-spylab/rlhf_trojan_competition

## Repository Token Information
Total tokens in repository: 16382

Tokens per file:
- generate_evaluate_completions.py: 1174 tokens
- conda_recipe.yaml: 188 tokens
- DETAILED_RESULTS.md: 4339 tokens
- main.py: 519 tokens
- README.md: 6311 tokens
- src/__init__.py: 0 tokens
- src/README.md: 782 tokens
- src/models/__init__.py: 8 tokens
- src/models/reward_model.py: 1165 tokens
- src/datasets/prompt_only.py: 701 tokens
- src/datasets/base.py: 1062 tokens
- src/datasets/__init__.py: 9 tokens
- src/datasets/constants.py: 124 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: Finding Universal Jailbreak Backdoors in Aligned LLMs

## Project Overview

This repository contains the code and resources for a competition aimed at detecting universal jailbreak backdoors in aligned large language models (LLMs). The project implements a framework for evaluating and testing LLMs for potential backdoors that could be exploited to generate harmful content.

### Project Structure

```
├── generate_evaluate_completions.py
├── conda_recipe.yaml
├── DETAILED_RESULTS.md
├── main.py
├── README.md
└── src/
    ├── __init__.py
    ├── README.md
    ├── models/
    │   ├── __init__.py
    │   └── reward_model.py
    └── datasets/
        ├── __init__.py
        ├── base.py
        ├── constants.py
        └── prompt_only.py
```

## Key Components and Functionality

### 1. Generation and Evaluation Script (`generate_evaluate_completions.py`)

This script is the core of the project, responsible for:
- Loading the generation model and reward model
- Creating a dataset for evaluation
- Generating completions using the LLM
- Evaluating the generated completions using the reward model

Key functions:
- Model loading and initialization
- Batch processing of prompts
- Reward calculation for generated completions

### 2. Dataset Handling (`src/datasets/`)

The `PromptOnlyDataset` class in `prompt_only.py` is crucial for preparing and formatting the input data:
- Preprocesses raw samples into a format suitable for the models
- Handles the insertion of trigger strings for testing backdoors
- Implements collation for batching samples

### 3. Reward Model (`src/models/reward_model.py`)

The `RewardModel` class is an extension of the LLaMA model architecture, designed to:
- Compute safety scores for generated text
- Provide both token-level and end-of-sequence rewards

### 4. Main Execution Script (`main.py`)

This script serves as a template for participants to implement their backdoor detection methods:
- Loads necessary models and datasets
- Provides a structure for implementing custom detection algorithms
- Handles output formatting for submission

## Relation to Research Concepts

The code implements several key concepts discussed in the research paper:

1. **Universal Jailbreak Backdoors**: The framework allows for testing LLMs with potential backdoor triggers appended to prompts.

2. **RLHF and Poisoning**: While not directly implemented, the models provided are the result of RLHF training with poisoned datasets.

3. **Reward Modeling**: The `RewardModel` class implements the concept of using a separate model to evaluate the safety of generated content.

4. **Prompt Engineering**: The dataset handling includes mechanisms for inserting potential backdoor triggers into prompts.

## Notable Techniques

1. **Batched Generation and Evaluation**: The code efficiently processes multiple prompts in batches for both generation and reward calculation.

2. **Flexible Dataset Preprocessing**: The `PromptOnlyDataset` class allows for easy modification of prompt structures and trigger insertion.

3. **Model-Agnostic Framework**: While using LLaMA as a base, the code structure allows for easy adaptation to other model architectures.

# Potential Enhancements

1. **Automated Backdoor Search**

Implement a genetic algorithm or gradient-based optimization method to automatically search for effective backdoor triggers. This could involve:
- Defining a fitness function based on the reward model output
- Implementing mutation and crossover operations for trigger candidates
- Parallelizing the search process for efficiency

2. **Interpretability Tools Integration**

Incorporate advanced interpretability techniques to analyze model behavior:
- Implement attention visualization tools to identify patterns in model focus when triggered
- Add integrated gradients or SHAP value calculations to understand feature importance
- Create a dashboard for visualizing and comparing model behaviors with and without triggers

3. **Multi-Model Comparison Framework**

Extend the codebase to support simultaneous analysis of multiple models:
- Implement a framework for loading and comparing different model architectures
- Add metrics for cross-model analysis of backdoor effectiveness
- Create visualization tools for comparing embedding spaces across models

4. **Adaptive Trigger Generation**

Develop a system that dynamically adapts trigger generation based on model responses:
- Implement a reinforcement learning approach to optimize trigger effectiveness
- Create a feedback loop that incorporates reward model outputs into trigger refinement
- Add support for context-aware trigger generation that considers prompt content

5. **Robustness Testing Suite**

Expand the evaluation framework to include comprehensive robustness tests:
- Implement adversarial attack techniques to stress-test model safety
- Add support for testing model behavior across different domains and topics
- Create a module for testing model consistency by rephrasing prompts and comparing outputs

These enhancements would address limitations mentioned in the paper, such as the need for methods that don't assume access to clean models, and would push the research forward by providing more sophisticated tools for backdoor detection and analysis.