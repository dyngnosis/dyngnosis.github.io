#Tags
[[Research/Research Papers/2403.02691v3.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0040/MLModelInferenceAPIAccess
#AMLT0051/LLMPromptInjection
#AMLT0053/LLMPluginCompromise

**Title:** INJEC AGENT : Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents

**Authors:** Qiusi Zhan, Zhixiang Liang, Zifan Ying, Daniel Kang

**Affiliation:** University of Illinois Urbana-Champaign

**Publication Date:** March 5, 2024

Summary:
This paper introduces INJEC AGENT, a benchmark for assessing the vulnerability of tool-integrated LLM agents to indirect prompt injection (IPI) attacks. The benchmark comprises 1,054 test cases covering 17 user tools and 62 attacker tools, categorizing attacks into direct harm and data exfiltration. The study evaluates 30 different LLM agents and demonstrates their vulnerability to IPI attacks.

Key Contributions:
- Introduction of INJEC AGENT benchmark for assessing IPI vulnerabilities in LLM agents
- Evaluation of 30 different LLM agents using the benchmark
- Categorization of attack intentions into direct harm and data exfiltration
- Analysis of attack success rates in base and enhanced settings

Problem Statement:
The paper addresses the security risks associated with LLM agents that can access external tools and content, particularly the vulnerability to indirect prompt injection attacks that can manipulate agents into executing harmful actions against users.

Methodology:
1. Creation of INJEC AGENT benchmark with 1,054 test cases
2. Evaluation of 30 LLM agents using the benchmark
3. Testing in base and enhanced settings (with hacking prompt)
4. Analysis of attack success rates and vulnerabilities

Main Results:
1. ReAct-prompted GPT-4 vulnerable to attacks 24% of the time in base setting
2. Enhanced setting (with hacking prompt) nearly doubles attack success rate on ReAct-prompted GPT-4
3. Fine-tuned agents show greater resilience compared to prompted agents
4. User cases with high content freedom yield higher attack success rates

Qualitative Analysis:
- The study reveals significant vulnerabilities in LLM agents, raising concerns about their widespread deployment
- The effectiveness of hacking prompts in enhancing attack success rates highlights the need for improved security measures
- The difference in vulnerability between prompted and fine-tuned agents suggests potential avenues for improving agent security

Limitations:
- The study focuses on single-turn scenarios and limits attacker instructions to a maximum of two steps
- The enhanced setting uses a fixed hacking prompt, which may not represent all possible attack variations
- Limited examination of fine-tuned agents due to availability constraints

Conclusion and Future Work:
The paper concludes that tool-integrated LLM agents are vulnerable to IPI attacks, with significant implications for their deployment. Future work should focus on developing more robust defense mechanisms and expanding the benchmark to cover more complex scenarios.

New Tool Introduced:
INJEC AGENT - A benchmark for assessing the vulnerability of tool-integrated LLM agents to indirect prompt injection attacks.
GitHub Repository: https://github.com/uiuc-kang-lab/InjecAgent