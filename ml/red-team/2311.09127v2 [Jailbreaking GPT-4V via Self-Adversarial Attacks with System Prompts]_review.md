#Tags
[[Research/Research Papers/2311.09127v2.pdf]]

#AMLT0051/LLMPromptInjection
#AMLT0054/LLMJailbreak
#AMLT0056/LLMMetaPromptExtraction
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData

**Title:** Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts
**Authors:** Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, Lichao Sun
**Affiliations:** Huazhong University of Science and Technology, Lehigh University
**Publication date:** 15 Nov 2023 (updated 20 Jan 2024)

Key Contributions:
- Discovery of a system prompt leakage vulnerability in GPT-4V
- Development of SASP (Self-Adversarial Attack via System Prompt) for jailbreaking MLLMs
- Evaluation of system prompt modifications for defending against jailbreak attacks

Problem Statement:
The research addresses the lack of attention to vulnerabilities in MLLM APIs, particularly focusing on system prompt exploitation for jailbreaking attacks.

Methodology:
1. System Prompt Theft:
   - Designed a meta-theft prompt to extract GPT-4V's internal system prompt
   - Used a simulated incomplete conversation and image trigger
   
2. Self-Adversarial Iteration:
   - Employed GPT-4 as a red-teaming tool against itself
   - Iteratively refined jailbreak prompts based on target model feedback
   
3. Jailbreak Prompt Enhancement:
   - Applied techniques like prefix injection, refusal suppression, hypothetical scenarios, and emotional appeals
   
4. Evaluation:
   - Tested on facial recognition tasks using GPT-4V
   - Measured Attack Success Rate (ASR), Recognition Success Rate (RSR), and Defense Success Rate (DSR)

Main Results:
- SASP achieved 59% ASR in English jailbreak prompts
- Manual enhancement improved ASR to 99% and RSR to 95%
- Demonstrated effectiveness across multiple languages (English, Chinese, Thai)

Qualitative Analysis:
- The study reveals significant vulnerabilities in MLLM safety mechanisms
- System prompts play a crucial role in both facilitating and potentially defending against jailbreak attacks
- The success of SASP highlights the need for improved security measures in MLLMs

Limitations:
- Focus primarily on GPT-4V, may not generalize to all MLLMs
- Ethical considerations of exploiting vulnerabilities in AI systems

Conclusion and Future Work:
- The research provides new insights into MLLM security
- Demonstrates the dual role of system prompts in jailbreaking and defense
- Suggests further exploration of robust protective measures for MLLMs

Tools Introduced:
SASP (Self-Adversarial Attack via System Prompt) - A novel method for jailbreaking MLLMs using stolen system prompts