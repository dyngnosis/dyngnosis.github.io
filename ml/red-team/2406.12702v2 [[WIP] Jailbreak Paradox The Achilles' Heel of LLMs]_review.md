#Tags
[[Research/Research Papers/2406.12702v2.pdf]]

#AMLT0051/LLMPromptInjection
#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel

**Title:** [WIP] Jailbreak Paradox: The Achilles' Heel of LLMs
**Authors:** Abhinav Rao, Monojit Choudhury, Somak Aditya
**Affiliations:** Carnegie Mellon University, MBZUAI, IIT Kharagpur
**Publication date:** June 21, 2024 (last updated)

Key Contributions:
- Introduction of two jailbreak paradoxes for foundation models
- Formal proofs for the impossibility of perfect jailbreak classifiers
- Demonstration that weaker models cannot consistently detect jailbreaks in stronger models
- Case study on Llama and GPT4-o to support the theoretical findings

Problem Statement:
The paper addresses the challenge of preventing and detecting jailbreaks in increasingly powerful language models, questioning whether it becomes easier or harder to jailbreak more advanced models.

Methodology:
1. Theoretical framework:
   - Utilizes undecidability results and Cantor's diagonalization
   - Defines alignment and jailbreaking for LLMs
2. Case study:
   - Compares Llama-2, Tamil-Llama, and GPT-4o using Tamil language jailbreaks
   - Evaluates models on Tamil-Llama-Eval v2 dataset
   - Tests three black-box user-jailbreaks: Albert, Pliny, and CodeJB

Main Results:
1. Theorem 3.1: Impossibility of constructing a perfect jailbreak classifier
2. Theorem 4.1: Weaker models cannot detect all jailbreaks of pareto-dominant models
3. Case study findings:
   - GPT-4o (strongest model) could detect jailbreaks in weaker models but not consistently in itself
   - Tamil-Llama (intermediate model) occasionally detected jailbreaks in Llama-2 but failed for its own outputs
   - Llama-2 (weakest model) unable to evaluate jailbreaks in any model

Qualitative Analysis:
- The paradoxes suggest that as models become more powerful and aligned, preventing jailbreaks becomes increasingly challenging
- Automatic benchmarking of jailbreak resistance may be futile for highly capable models
- Research on jailbreak prevention should focus on discovering new attack strategies rather than solely on prevention

Limitations:
- The study focuses on a specific language (Tamil) and a limited set of models
- The jailbreaks used in the case study may not represent all possible attack vectors

Conclusion and Future Work:
- The jailbreak paradoxes highlight fundamental challenges in securing powerful language models
- Future research should explore creative jailbreaking strategies for powerful models
- The findings may extend to other hard problems in AI, such as detecting AI-generated content and hallucinations

Relevant Figures:
- Tables 1 and 4: Resultant scores of models on Tamil-Llama-Eval v2 dataset
- Tables 2 and 3: Jailbreak success and detection rates for different models

New Tools:
No specific new tools were introduced in this paper.