#Tags
[[Research/Research Papers/2407.01376v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0018/BackdoorMLModel
#AMLT0031/ErodeMLModelIntegrity
#AMLT0043/CraftAdversarialData
#AMLT0044/FullMLModelAccess

**Title:** Badllama 3: removing safety finetuning from Llama 3 in minutes
**Authors:** Dmitrii Volkov
**Affiliation:** Palisade Research
**Publication Date:** July 1, 2024

Abstract Summary:
The paper demonstrates that extensive LLM safety fine-tuning can be easily subverted when an attacker has access to model weights. It evaluates three state-of-the-art fine-tuning methods (QLoRA, ReFT, and Ortho) and shows how algorithmic advances enable constant jailbreaking performance with reduced FLOPs and optimization power. The authors successfully strip safety fine-tuning from Llama 3 8B in one minute and Llama 3 70B in 30 minutes on a single GPU.

Key Contributions:
- Demonstration of rapid removal of safety fine-tuning from Llama 3 models
- Evaluation of three state-of-the-art fine-tuning methods for jailbreaking
- Reduction of computation time and resources required for jailbreaking
- Proof of concept for distributing small "jailbreak adapters" for instant guardrail removal

Problem Statement:
The research addresses the vulnerability of safety fine-tuning in large language models when attackers have access to model weights. It aims to minimize the rate at which models refuse to answer unsafe queries without degrading other performance aspects.

Methodology:
1. Evaluation of three fine-tuning methods:
   - QLoRA: Optimized industry-standard method
   - ReFT: Representation Finetuning with 10-30x fewer parameters
   - Ortho: Optimization-free method using activation addition and directional ablation
2. Use of proprietary datasets for fine-tuning:
   - BadLlama (18k low-quality QA pairs)
   - BadLlama-ICLR24 (5k higher-quality QA pairs)
3. Evaluation metrics:
   - Attack Success Rate (ASR) on HarmBench dataset
   - Performance on Open LLM Leaderboard-like benchmarks

Main Results and Findings:
1. Successful jailbreaking of Llama 3 8B in 5 minutes and Llama 3 70B in 45 minutes using QLoRA
2. Further reduction in computation time by 3-5x using ReFT and Ortho methods
3. Minimal performance degradation across all fine-tuning methods on helpfulness benchmarks
4. Significant increase in ASR scores on HarmBench, comparable to top jailbreaks

Qualitative Analysis:
- The ease and speed of removing safety fine-tuning raise concerns about the effectiveness of current safety measures in open-weight models
- The ability to distribute small "jailbreak adapters" poses a significant risk for widespread misuse of language models
- The trade-off between model accessibility and safety becomes more pronounced with these findings

Limitations and Considerations:
- The study focuses on open-weight models, which may not represent the security of closed API-based models
- The evaluation of generation quality is qualitative and requires further quantification
- The impact on more complex safety measures or multi-modal models is not addressed

Conclusion and Future Work:
The paper concludes that current industrial fine-tuning methods can effectively remove safety guardrails from frontier open-weight models in minutes of GPU-time and at minimal cost. The authors predict further reductions in safety removal costs in the near future. Future work includes:
- Publishing open-source, reproducible evaluations
- Improving ReFT benchmarking
- Evaluating on additional benchmarks (AdvBench and RefusalBench)
- Quantifying generation quality with Elo comparisons

Tools Introduced:
- No specific new tools are introduced, but the paper mentions using existing frameworks like HuggingFace Transformers, bitsandbytes, and unsloth kernels for implementation.