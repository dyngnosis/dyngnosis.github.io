#Tags
[[Research/Research Papers/2401.09002v5.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0042/VerifyAttack

**Title:** AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models
**Authors:** Dong Shu, Mingyu Jin, Chong Zhang, Lingyao Li, Zihao Zhou, Yongfeng Zhang
**Affiliations:** Northwestern University, Rutgers University, University of Liverpool, University of Michigan
**Publication Date:** August 2024 (preprint)

Summary:
This paper introduces a novel framework for evaluating the effectiveness of jailbreak attacks on large language models (LLMs). The authors propose coarse-grained and fine-grained evaluation metrics, along with a comprehensive ground truth dataset for jailbreak prompts. The framework aims to provide a more nuanced assessment of attack effectiveness compared to traditional binary evaluation methods.

Key Contributions:
- Introduction of coarse-grained and fine-grained evaluation frameworks for jailbreak attacks
- Development of a comprehensive ground truth dataset for jailbreak prompts
- Comparison of the proposed evaluation method with traditional baseline metrics
- Identification of potentially harmful attack prompts that may be overlooked by binary evaluations

Problem Statement:
Current evaluation methods for jailbreak attacks on LLMs often rely on binary metrics and may underestimate the harmful potential of certain attack prompts. There is a need for more sophisticated and comprehensive evaluation methodologies to assess the effectiveness of jailbreak attacks.

Methodology:
1. Coarse-grained evaluation:
   - Assesses overall effectiveness of attack prompts across multiple LLMs
   - Uses weighted scoring based on each LLM's defense robustness
   - Calculates effectiveness score using Equation 4

2. Fine-grained evaluation:
   a. With ground truth:
      - Compares LLM responses to a set of authoritative ground truth answers
      - Uses BERT embeddings to calculate similarity scores
   b. Without ground truth:
      - Employs a four-category classification system: Full Rejection, Partial Rejection, Partial Obedience, and Full Obedience
      - Scores range from 0 to 1 based on the level of compliance with the attack prompt

3. Dataset:
   - Jailbreak_llms dataset with 666 prompts and 390 harmful questions across 13 scenarios

4. Baseline comparison:
   - Attack Success Rate (ASR) used as the baseline metric

Main Results:
1. The proposed evaluation framework aligns with baseline metrics while offering more nuanced assessments.
2. Coarse-grained evaluation identifies potentially harmful attack prompts that may appear harmless in traditional evaluations.
3. Fine-grained evaluation provides detailed insights into the effectiveness of individual attack prompts on specific LLMs.
4. The "Political Lobbying" scenario consistently produces the highest effectiveness scores for attack prompts across most LLMs.

Qualitative Analysis:
- The proposed framework offers a more comprehensive understanding of jailbreak attack effectiveness by considering multiple LLMs and providing granular scoring.
- The ground truth dataset serves as a valuable benchmark for future research in this area.
- The identification of scenarios with consistently high effectiveness scores (e.g., "Political Lobbying") highlights potential vulnerabilities in LLM defenses.

Limitations:
- The effectiveness of the ground truth dataset may depend on the quality and diversity of the selected answers.
- The fine-grained evaluation without ground truth still relies on GPT-4 as a judgment model, which may introduce potential biases.

Conclusion and Future Work:
The paper establishes a solid foundation for assessing a broader range of attack prompts in prompt injection scenarios. Future work could focus on:
1. Expanding the ground truth dataset to cover more scenarios and edge cases
2. Developing automated methods for generating and evaluating jailbreak prompts
3. Investigating defensive strategies based on the insights gained from the evaluation framework

Relevant Figures:
- Figure 1: Sample effectiveness evaluation of attack prompts based on LLM responses
- Figure 2: Visualization of a coarse-grained evaluation process
- Figure 3-5: Comparison between proposed metrics and baseline binary metric

New Tools:
The paper introduces the AttackEval framework for evaluating jailbreak attacks on LLMs. While no specific GitHub repository is mentioned, the methodology and equations provided could be implemented as a tool for researchers and practitioners in the field of LLM security.