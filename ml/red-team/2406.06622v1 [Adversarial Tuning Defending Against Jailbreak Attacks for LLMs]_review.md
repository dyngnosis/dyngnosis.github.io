#Tags
[[Research/Research Papers/2406.06622v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0031/ErodeMLModelIntegrity

**Title:** Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs
**Authors:** Fan Liu, Zhao Xu, Hao Liu
**Affiliations:** AI Thrust, The Hong Kong University of Science and Technology (Guangzhou)
**Publication Date:** June 7, 2024 (preprint)

Summary:
This paper proposes a two-stage adversarial tuning framework to enhance the defense capabilities of Large Language Models (LLMs) against jailbreak attacks, particularly unknown jailbreak attacks. The framework generates adversarial prompts to explore worst-case scenarios and optimizes datasets containing pairs of adversarial prompts and their safe responses.

Key Contributions:
- Introduction of a two-stage adversarial tuning framework for LLM defense
- Development of hierarchical meta-universal adversarial prompt learning for efficient token-level adversarial prompt generation
- Proposal of automatic adversarial prompt learning for semantic-level adversarial prompt refinement
- Demonstration of the framework's effectiveness and generalizability across various attack strategies and target LLMs

Problem Statement:
LLMs are vulnerable to jailbreak attacks, which can manipulate prompts to bypass the model's alignment and produce harmful responses. Existing defense methods struggle to effectively defend against unknown jailbreak attacks, highlighting the need for improved generalized defense capabilities.

Methodology:
1. Stage One: Hierarchical Meta-Universal Adversarial Tuning
   - Outer Universal Adversarial Prompt Learning: Learns task-based universal adversarial suffixes
   - Inner Individual Adversarial Prompt Learning: Personalizes universal adversarial prompts for specific instances
   - Token-level Adversarial Tuning Optimization: Fine-tunes the LLM using adversarial prompts and safe responses

2. Stage Two: Prompt-Level Adversarial Refinement Learning
   - Automatic Adversarial Prompt Refinement (AAPR): Iteratively refines semantic-level adversarial prompts
   - Adversarial Fine-tuning: Further enhances the LLM's defense capabilities using refined prompts

Datasets:
- AdvBench: 520 malicious prompts (90% training, 10% testing)
- MaliciousInstruct: 100 instances of harmful behavior across 10 categories
- Forbidden Question Set: Jailbreak prompts from various internet platforms

Target Models:
- Llama-2 (7B-chat-hf)
- Vicuna (13B-v1.5)

Main Results:
1. Superior performance compared to six defense baselines under five representative attack scenarios
2. Significant reduction in Attack Success Rate (ASR) for both known and unknown jailbreak attacks
3. Improved generalizability across various attack strategies and target LLMs
4. Effective transferability of adversarial examples across different model sizes and types

Qualitative Analysis:
- The proposed framework demonstrates the ability to generate effective adversarial prompts that explore worst-case scenarios for LLMs
- The two-stage approach allows for both efficient token-level optimization and semantic-level refinement, addressing different aspects of jailbreak attacks
- The framework's transferability suggests its potential as a universal defense mechanism for various LLM architectures

Limitations:
- Slight reduction in model utility after adversarial tuning
- Computational overhead for generating token-level adversarial prompts

Future Work:
- Exploration of hybrid fine-tuning strategies to balance model utility and adversarial robustness
- Investigation of more efficient methods for generating adversarial prompts
- Further analysis of the framework's effectiveness against emerging jailbreak attack techniques

Tools Introduced:
- Hierarchical Meta-Universal Adversarial Prompt Learning algorithm
- Automatic Adversarial Prompt Refinement (AAPR) method