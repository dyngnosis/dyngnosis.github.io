#Tags
[[Research/Research Papers/2408.15221v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0051/LLMPromptInjection
#AMLT0031/ErodeMLModelIntegrity
#AMLT0043/CraftAdversarialData

**Title:** LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet
**Authors:** Nathaniel Li, Ziwen Han, Ian Steneker, Willow Primack, Riley Goodside, Hugh Zhang, Zifan Wang, Cristina Menghini, Summer Yue
**Affiliations:** Scale AI, UC Berkeley
**Publication Date:** August 27, 2024

Key Contributions:
- Demonstrated that multi-turn human jailbreaks significantly outperform automated attacks against LLM defenses
- Revealed vulnerabilities in machine unlearning defenses through human jailbreaking
- Compiled and released the Multi-Turn Human Jailbreaks (MHJ) dataset
- Developed a taxonomy of jailbreak tactics from commercial red teaming engagements

Problem Statement:
Current LLM defenses are primarily evaluated against single-turn automated adversarial attacks, which is an insufficient threat model for real-world malicious use. The paper investigates the effectiveness of multi-turn human jailbreaks against these defenses.

Methodology:
1. Human red teaming pipeline:
   - Up to three independent human attackers
   - Multi-turn conversations with LLMs
   - Two human reviewers and a language model classifier for verification
2. Evaluation on HarmBench dataset (240 harmful behaviors)
3. Comparison with six automated attacks against four LLM defenses
4. Testing on machine unlearning defenses using WMDP-Bio benchmark

Main Results:
1. Multi-turn human jailbreaks achieved significantly higher attack success rates (ASR) than automated attacks:
   - 19% to 65% higher ASR than an ensemble of all automated attacks on HarmBench
   - 70.4% ASR against CYGNET defense, compared to 0% for automated attacks
2. Human jailbreaks successfully recovered dual-use biosecurity knowledge from unlearned models
3. Compiled 2,912 prompts across 537 multi-turn jailbreaks into the MHJ dataset

Qualitative Analysis:
- Multi-turn conversations allow for more sophisticated and context-aware attacks
- Human attackers can adapt their strategies based on model responses, unlike most automated attacks
- Jailbreak tactics developed through commercial red teaming provide insights into common vulnerabilities across different LLM defenses

Limitations:
- Cost and time-intensive nature of human red teaming compared to automated attacks
- Potential variations in individual red teamer skills and experience
- Differences in evaluation setups between human and automated attacks

Conclusion and Future Work:
- Current LLM defenses are not robust against multi-turn human jailbreaks
- Need for more realistic threat models and stronger automated adversarial attacks
- Importance of evaluating defenses against multi-turn attacks and human-like reasoning

New Tools:
Multi-Turn Human Jailbreaks (MHJ) dataset
- Available at: https://scale.com/research/mhj
- Contains 2,912 prompts across 537 multi-turn jailbreaks
- Includes metadata, jailbreak tactics, and submission messages

Figures:
1. Figure 1: Comparison of attack success rates (ASR) between human and automated attacks on different LLM defenses
2. Figure 2: Human jailbreak pipeline diagram
3. Figure 3: Detailed breakdown of ASR by attack type and defense
4. Figure 4: ASR against RMU unlearning method on WMDP-Bio questions
5. Figure 5: Distribution of primary tactics for successful human attacks on HarmBench