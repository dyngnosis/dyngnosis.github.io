#Tags
[[Research/Research Papers/2407.02855v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0018/BackdoorMLModel
#AMLT0020/PoisonTrainingData
#AMLT0031/ErodeMLModelIntegrity
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** Safe Unlearning: A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks
**Authors:** Zhexin Zhang, Junxiao Yang, Pei Ke, Shiyao Cui, Chujie Zheng, Hongning Wang, Minlie Huang
**Affiliation:** The Conversational AI (CoAI) group, DCST, Tsinghua University
**Publication Date:** July 3, 2024

Summary:
This paper introduces a novel approach called "Safe Unlearning" to defend against jailbreak attacks on Large Language Models (LLMs). The method focuses on unlearning harmful knowledge rather than identifying harmful queries, showing surprising generalizability and effectiveness even with limited training data.

Key Contributions:
- Proposes unlearning as a more effective principle to defend against jailbreak attacks
- Implements Safe Unlearning, which significantly lowers Attack Success Rate (ASR) while maintaining general performance
- Demonstrates strong generalization ability against out-of-distribution (OOD) harmful questions with jailbreak prompts
- Provides insights into the nature of jailbreak attacks and the effectiveness of unlearning-based defenses

Problem Statement:
LLMs are vulnerable to jailbreak attacks even after safety alignment. Current defense methods, primarily based on Supervised Fine-Tuning (SFT), are ineffective against diverse jailbreak queries.

Methodology:
1. Safe Unlearning approach:
   - Minimizes probability of generating harmful responses
   - Maximizes probability of rejecting harmful queries
   - Maintains general performance on harmless queries
2. Training set: 100 harmful instructions from AdvBench
3. Test set: Over 10,000 jailbreak test queries, including in-distribution and OOD harmful questions
4. Evaluated models: Vicuna-7B-v1.5, Mistral-7B-Instruct-v0.2, Llama-2-7B-Chat
5. Baselines: Vanilla, SFT, DPO, GA, GA W/ threshold

Main Results:
1. Safe Unlearning reduced ASR on OOD harmful questions from 82.6% to 7.7% for Vicuna-7B
2. Outperformed Llama2-7B-Chat (ASR 21.9%) despite using only 20 raw harmful questions without jailbreak prompts
3. Maintained general performance on instruction-following tasks (e.g., AlpacaEval)

Qualitative Analysis:
- Unlearning harmful responses is more effective than learning harmless responses due to the substantial differences in queries between raw harmful questions and jailbreak versions
- Generalization ability stems from the intrinsic relatedness among harmful responses across harmful questions
- Unlearning affects both specific harmful behaviors and general expressions that can become harmful in specific contexts

Limitations:
- Potential over-rejection of safe queries
- Limited exploration of additional jailbreak attack methods and models

Conclusion and Future Work:
Safe Unlearning demonstrates a promising direction for defending against jailbreak attacks. Future work should focus on mitigating over-rejection and exploring the method's effectiveness across a broader range of models and attack types.

Tools Introduced:
- Safe Unlearning method
- GitHub repository: https://github.com/thu-coai/SafeUnlearning