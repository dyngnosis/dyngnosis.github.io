#Tags
[[Research/Research Papers/2309.01446v4.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0017/DevelopCapabilities

**Title:** Open Sesame! Universal Black-Box Jailbreaking of Large Language Models
**Authors:** Raz Lapid, Ron Langberg, Moshe Sipper
**Affiliations:** 
- Dept. of Computer Science, Ben-Gurion University, Beer-Sheva, Israel
- DeepKeep, Tel-Aviv, Israel
Publication: ICLR 2024 Workshop on Secure and Trustworthy Large Language Models

Key Contributions:
- Introduction of a novel black-box approach for LLM jailbreaking using Genetic Algorithms (GA)
- Development of an automated, universal adversarial prompt generation technique
- Demonstration of the technique's effectiveness across multiple LLM architectures

Problem Statement:
How to automatically jailbreak LLMs without relying on the LLMs' internals?

Methodology:
1. Genetic Algorithm (GA) for optimizing universal adversarial prompts
2. Population encoding: Individuals represented as vectors of integers (tokens)
3. Fitness function: Cosine similarity between embeddings of LLM output and target output
4. Selection: Tournament selection with k=2
5. Crossover and mutation: One-point crossover and random token replacement
6. Elitism: Preserving best-performing individuals (Î»=n/5)
7. Fitness approximation: Random subset sampling (c=50) for efficiency

Datasets:
- Harmful Behavior dataset (Zou et al., 2023)
- Split into 70% training and 30% test sets

Models Tested:
1. LLaMA2-7b-chat (Touvron et al., 2023)
2. Vicuna-7b (Chiang et al., 2023)

Text Embedders:
1. BGE (bge-large-en)
2. MPNet (all-mpnet-base-v2)
3. MiniLM (all-MiniLM-L6-v2)

Main Results:
1. MPNet embedder consistently achieved highest average Attack Success Rate (ASR)
2. Vicuna-7b: 95.5% ASR (vs. 0.6% without attack)
3. LLaMA-7b-chat: 98.7% ASR (vs. 16.3% without attack)
4. Successful jailbreaking across various harmful prompts (e.g., cheating, malware creation, terrorist activities)

Qualitative Analysis:
- The GA-based approach effectively bypasses LLM safety measures
- Universal adversarial prompts can be transferred between models
- The technique reveals vulnerabilities in current LLM alignment methods

Limitations:
1. Added prompts contain "garbage" tokens, potentially detectable by other LLMs or perplexity measures
2. The attack adds perceptible perturbations

Conclusion and Future Work:
- The study highlights the need for reassessing security mechanisms in LLMs
- Proposed future work includes:
  1. Exploring interaction between prompt construction and GA parameters
  2. Investigating generalizability to other AI systems
  3. Developing more robust alignment techniques (e.g., adversarial training, RLHF improvements)

New Tool:
While no specific tool is mentioned, the paper introduces a novel GA-based technique for universal black-box jailbreaking of LLMs.