#Tags
[[Research/Research Papers/2406.18725v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0057/LLMDataLeakage
#AMLT0015/EvadeMLModel

**Title:** Jailbreaking LLMs with Arabic Transliteration and Arabizi
**Authors:** Mansour Al Ghanim, Saleh Almohaimeed, Mengxin Zheng, Yan Solihin, Qian Lou
**Affiliation:** Cyber Security and Privacy Cluster, Department of Computer Science, University of Central Florida
**Publication Date:** June 26, 2024

Summary:
This study investigates the vulnerabilities of Large Language Models (LLMs) to jailbreak attacks using Arabic language and its variants, particularly transliteration and chatspeak (Arabizi). The research finds that while standardized Arabic prompts are generally safe, using Arabic transliteration and chatspeak can lead to unsafe content generation in LLMs like OpenAI GPT-4 and Anthropic Claude 3 Sonnet.

Key Contributions:
- Demonstrates vulnerabilities in LLMs when using Arabic transliteration and chatspeak
- Reveals potential hidden vulnerabilities that could be exploited for jailbreak attacks
- Proposes mitigation strategies for safer LLM interactions with Arabic language forms

Problem Statement:
The study addresses the potential security risks in LLMs when interacting with non-conventional forms of Arabic language, which may bypass existing safety measures.

Methodology:
1. Dataset: AdvBench benchmark containing 520 harmful prompts
2. Translation: Prompts translated to Arabic using OpenAI GPT-3.5-turbo
3. Conversion: Arabic prompts converted to chatspeak and transliteration forms
4. Testing: Prompts tested on OpenAI GPT-4 and Anthropic Claude-3 Sonnet
5. Evaluation: Manual investigation of LLM outputs for harmful content

Main Results:
1. Standardized Arabic prompts: Generally safe, with high refusal rates
2. Arabic transliteration and chatspeak:
   - GPT-4: Unsafe content increased from 2.50% to 12.12%
   - Claude-3: Unsafe content increased from 0.19% to 4.62%
3. Prefix injection technique ineffective with standardized Arabic
4. Word-level and sentence-level perturbations in Arabic forms can lead to unsafe content

Qualitative Analysis:
- The study reveals that LLMs may have learned connections to specific words in Arabic transliteration and chatspeak, potentially exposing vulnerabilities
- The research highlights the importance of comprehensive safety training across all language forms
- The findings suggest that adversarial training in cross-lingual settings may be more critical than addressing low-resource language issues

Limitations:
- Focus solely on Arabic language and its variants
- Manual evaluation process, which may introduce subjectivity
- Limited to two specific LLM platforms (GPT-4 and Claude-3)

Conclusion and Future Work:
- The study emphasizes the need for more robust safety measures in LLMs, particularly for non-conventional language forms
- Proposed mitigation strategies include:
  1. Converting non-conventional prompts to standardized form
  2. Incorporating Arabic transliteration and chatspeak into model alignment
  3. Implementing advanced adversarial training in Arabic with phonemic knowledge and word collocations

Relevant Figures:
Figure 1: Demonstration of how Arabic transliteration can lead to unsafe responses from GPT-4
Figure 3: Examples of character-level modifications leading to unsafe responses in GPT-4 and Claude-3

New Tools:
No specific new tools were introduced in this paper. The research primarily used existing LLM platforms (GPT-4 and Claude-3) for experimentation.