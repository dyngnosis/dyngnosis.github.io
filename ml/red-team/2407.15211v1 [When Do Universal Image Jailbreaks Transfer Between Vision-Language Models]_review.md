#Tags
[[Research/Research Papers/2407.15211v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0044/FullMLModelAccess
#AMLT0054/LLMJailbreak

**Title:** When Do Universal Image Jailbreaks Transfer Between Vision-Language Models?
**Authors:** Rylan Schaeffer, Dan Valentine, Luke Bailey, James Chua, Crist√≥bal Eyzaguirre, Zane Durante, Joe Benton, Brando Miranda, Henry Sleight, Tony Tong Wang, John Hughes, Rajashree Agrawal, Mrinank Sharma, Scott Emmons, Sanmi Koyejo, Ethan Perez
**Publication Date:** 21 Jul 2024

Key Contributions:
- Conducted a large-scale empirical study on the transferability of gradient-based universal image jailbreaks across 40+ open-parameter Vision-Language Models (VLMs)
- Developed and publicly released 18 new VLMs based on recent language models
- Demonstrated that transferable gradient-based image jailbreaks are extremely difficult to obtain
- Identified two settings with partially successful transfer: between identically-pretrained VLMs with slightly different training data, and between different training checkpoints of a single VLM
- Showed that transfer can be improved against a specific target VLM by attacking larger ensembles of "highly-similar" VLMs

Problem Statement:
The study investigates the transferability of gradient-based universal image jailbreaks between Vision-Language Models (VLMs), addressing the potential vulnerability of multimodal AI systems to adversarial manipulation.

Methodology:
1. Created three datasets of harmful prompts and harmful-yet-helpful responses:
   - AdvBench: 416 training and 104 test prompt-response pairs
   - Anthropic HHH: 416 training and 104 test prompt-response pairs
   - Generated: 48k training and 12k test prompt-response pairs
2. Optimized jailbreak images using a loss function based on negative log likelihood
3. Tested various VLMs with different vision backbones (CLIP, SiGLIP, DINOv2) and language models (Vicuna, Llama 2, Llama 3, Gemma, Mistral, Phi)
4. Evaluated jailbreak success using cross-entropy loss and Claude 3 Opus Harmful-Yet-Helpful Score

Main Results:
1. Image jailbreaks optimized against a single VLM or ensemble of VLMs successfully jailbreak the attacked VLM(s) but exhibit little-to-no transfer to other VLMs
2. Transfer is not affected by matching vision backbones, language models, or safety-alignment training
3. Partial transfer observed between identically-initialized VLMs with overlapping training data
4. Partial transfer observed between different training checkpoints of a single VLM
5. Transfer can be improved by attacking larger ensembles of "highly-similar" VLMs

Qualitative Analysis:
- The study reveals that VLMs may be more robust to gradient-based transfer attacks compared to unimodal language models or image classifiers
- The difficulty in obtaining transferable jailbreaks suggests that the integration of visual and textual features in VLMs creates a more complex attack surface
- The partial success in transfer between similar VLMs indicates that small differences in training data or optimization can significantly impact vulnerability to jailbreaks

Limitations:
- The study focused on gradient-based attacks and may not generalize to other types of attacks
- The evaluation relied heavily on Claude 3 Opus for scoring harmful-yet-helpful responses, which may introduce bias
- The study did not explore black-box VLMs like GPT-4V or Gemini Pro

Conclusion and Future Work:
- The paper concludes that transferable gradient-based image jailbreaks against VLMs are extremely difficult to obtain, contrasting with previous findings on language models and image classifiers
- Future research directions include:
  1. Understanding VLM resistance to transfer attacks
  2. Developing more transferable attacks against VLMs
  3. Detecting image jailbreaks
  4. Creating more robust VLMs

New Tools:
- The authors created and publicly released 18 new VLMs based on recent language models, including Llama 3 Instruct, Gemma Instruct, Phi 3 Instruct, and Mistral Instruct v0.2

Figures:
- Figure 1: Overview of the image jailbreak optimization process and transfer experiments
- Figure 2: Results showing lack of transfer when optimizing against single VLMs
- Figure 3: Results showing lack of transfer when optimizing against ensembles of 8 VLMs
- Figure 5: Partial transfer results between identically-initialized VLMs with overlapping training data
- Figure 8: Improved transfer results when attacking larger ensembles of highly similar VLMs