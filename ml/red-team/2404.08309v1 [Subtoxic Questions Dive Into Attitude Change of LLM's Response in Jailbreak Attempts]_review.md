#Tags
[[Research/Research Papers/2404.08309v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection

**Title:** Subtoxic Questions: Dive Into Attitude Change of LLM's Response in Jailbreak Attempts
**Authors:** Tianyu Zhang, Zixuan Zhao, Jiaqi Huang, Jingyu Hua, Sheng Zhong
**Affiliation:** Department of Computer Science and Technology, Nanjing University
**Publication Date:** April 12, 2024

Key Contributions:
- Introduction of "subtoxic questions" as a novel approach to evaluate LLM vulnerabilities
- Development of the Gradual Attitude Change (GAC) Model for analyzing LLM responses
- Proposal of a quantitative assessment method for evaluating jailbreaking techniques

Problem Statement:
The paper addresses the challenge of evaluating the effectiveness of jailbreak attempts on Large Language Models (LLMs) as their security measures improve, making traditional jailbreaking methods less impactful.

Methodology:
1. Introduction of subtoxic questions: Harmless queries mistakenly identified as harmful by LLMs
2. Development of the GAC Model to analyze LLM response patterns
3. Formulation of two key observations (GAC-1 and GAC-2) based on the GAC Model
4. Experimental validation using GPT-3.5 and other LLMs

Main Results and Findings:
1. Subtoxic questions are more susceptible to jailbreaking interventions than truly toxic questions
2. The GAC Model reveals a spectrum of LLM responses beyond binary outcomes
3. Positive prompts demonstrate consistent effectiveness across different questions and prefixes
4. A method to measure and rank the effectiveness of jailbreaking prompts is proposed

Qualitative Analysis:
- The study provides insights into the mechanics of jailbreaking attempts, suggesting that the effectiveness of prompts can be evaluated on a continuous scale rather than binary outcomes
- The concept of subtoxic questions offers a new perspective on LLM vulnerabilities, potentially leading to more nuanced security measures
- The GAC Model and its observations may have implications beyond jailbreaking, such as improving prompt engineering for general LLM interactions

Limitations:
- The study primarily focuses on subtoxic questions, which may not fully represent all types of jailbreaking scenarios
- The effectiveness of prompts may vary across different LLM architectures and training approaches
- The proposed method for measuring prompt effectiveness requires further validation across a wider range of LLMs and use cases

Conclusion and Future Work:
- The paper introduces a novel framework for evaluating jailbreaking techniques using subtoxic questions and the GAC Model
- Future work includes developing a standardized question set for consistent evaluation across studies
- Further research opportunities include conducting ablation studies and exploring the underlying mechanics of jailbreaking LLMs

Relevant Figures:
Figure 1: Example of a subtoxic question applied to ChatGPT
Figure 2: Attitude distribution of GPT-3.5 responses to subtoxic questions with varying positive prompts

New Tools:
No specific new tools or GitHub repositories were mentioned in the paper.