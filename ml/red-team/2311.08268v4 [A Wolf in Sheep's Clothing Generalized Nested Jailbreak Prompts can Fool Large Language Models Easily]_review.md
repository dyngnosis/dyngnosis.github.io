#Tags
[[Research/Research Papers/2311.08268v4.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0043/CraftAdversarialData
#AMLT0015/EvadeMLModel

**Title:** A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily
**Authors:** Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, Shujian Huang
**Affiliations:** National Key Laboratory for Novel Software Technology, Nanjing University; Meituan Inc., China
**Publication Date:** November 14, 2023 (arXiv)

Key Contributions:
- Introduction of ReNeLLM, a generalized framework for automatic generation of jailbreak prompts
- Demonstration of high attack success rates across various LLMs with reduced time cost
- Analysis of LLM defense failures from the perspective of prompt execution priority
- Proposal of corresponding defense strategies

Problem Statement:
The paper addresses the vulnerability of Large Language Models (LLMs) to jailbreak attacks, which can bypass safety measures and generate harmful content. Existing jailbreak methods suffer from either complex manual design or inefficient optimization on other models.

Methodology:
1. Prompt Rewriting: Series of rewriting operations on initial prompts without changing semantics
2. Scenario Nesting: Selecting a scenario for rewritten prompts and further disguising them
3. Automated process using LLMs themselves without additional training or optimization
4. Evaluation on multiple LLMs: GPT-3.5, GPT-4, Claude-1, Claude-2, and Llama2
5. Comparison with baselines: GCG, AutoDAN, and PAIR
6. Metrics: KW-ASR, GPT-ASR, ASR-E, and TCPS (Time Cost Per Sample)

Main Results:
1. ReNeLLM achieves state-of-the-art ASR across all tested LLMs
2. Significant reduction in time cost compared to baselines (76.61% reduction vs. GCG, 86.19% vs. AutoDAN)
3. High transferability of generated prompts across different models
4. Effectiveness across various types of harmful prompts (e.g., illegal activity, hate speech, malware)

Qualitative Analysis:
- The success of ReNeLLM is attributed to the combination of prompt rewriting and scenario nesting, which makes the intent of malicious prompts more covert
- Attention visualization experiments reveal a shift in LLMs' execution priorities, favoring external instructions over internal ones after rewriting and nesting

Limitations:
- Fixed set of scenario nesting options may simplify defense strategies
- Experiments primarily conducted on English language datasets
- Computational cost and dependency on online LLMs for generating jailbreak prompts

Conclusion and Future Work:
- ReNeLLM demonstrates the inadequacy of current defense methods for LLMs
- Proposed defense strategies include incorporating extra prompts, SFT, and using harmfulness classifiers
- Future work should focus on developing more robust and generalized defense methods for LLMs

Tools Introduced:
ReNeLLM framework (GitHub repository: https://github.com/NJUNLP/ReNeLLM)

## Repository Token Information
Total tokens in repository: 18118

Tokens per file:
- check_kw_asr.py: 506 tokens
- get_responses.py: 943 tokens
- renellm_tcps.py: 2534 tokens
- renellm.py: 2253 tokens
- check_gpt_asr.py: 903 tokens
- defense/harm_classifier_defense.py: 999 tokens
- utils/data_utils.py: 160 tokens
- utils/prompt_rewrite_utils.py: 1251 tokens
- utils/llm_responses_utils.py: 366 tokens
- utils/llm_completion_utils.py: 483 tokens
- utils/scenario_nest_utils.py: 430 tokens
- utils/harmful_classification_utils.py: 362 tokens
- llama/setup.py: 95 tokens
- llama/example_chat_completion.py: 778 tokens
- llama/llama/model.py: 3102 tokens
- llama/llama/__init__.py: 58 tokens
- llama/llama/tokenizer.py: 332 tokens
- llama/llama/generation.py: 2563 tokens


## Tutorial and Enhancement Suggestions

# ReNeLLM Tutorial

## Project Overview

ReNeLLM (Rewritten and Nested Language Model) is a framework for generating jailbreak prompts to test the robustness of large language models (LLMs). The project implements the techniques described in the research paper to automatically create prompts that can bypass LLM safety measures.

### Project Structure

The repository is organized as follows:

- Root directory: Contains main scripts for running experiments and evaluations
- `utils/`: Helper functions and utilities
- `defense/`: Implementation of defense strategies
- `llama/`: Integration with the Llama model

## Key Components

### 1. Prompt Rewriting (utils/prompt_rewrite_utils.py)

This module implements six rewriting functions to modify harmful prompts:

- `shortenSentence`: Paraphrases with fewer words
- `misrewriteSentence`: Misspells sensitive words
- `changeOrder`: Alters sentence structure
- `addChar`: Inserts meaningless characters
- `languageMix`: Performs partial translation
- `styleChange`: Changes expression style

These functions use GPT-3.5-Turbo to generate rewritten prompts.

### 2. Scenario Nesting (utils/scenario_nest_utils.py)

Defines three scenarios for nesting rewritten prompts:
- Code Completion
- Table Filling
- Text Continuation

### 3. Main Attack Pipeline (renellm.py)

The `main` function in this script orchestrates the entire attack process:

1. Reads harmful behaviors from a dataset
2. Applies prompt rewriting
3. Performs scenario nesting
4. Sends the nested prompt to the target LLM
5. Evaluates the response for successful jailbreaking

### 4. Evaluation Metrics (check_kw_asr.py, check_gpt_asr.py)

Implements two main evaluation metrics:
- Keyword-based Attack Success Rate (KW-ASR)
- GPT-based Attack Success Rate (GPT-ASR)

### 5. Defense Strategies (defense/harm_classifier_defense.py)

Implements a harmfulness classifier as a defense method against jailbreak attempts.

## Key Algorithms and Techniques

### Iterative Rewriting and Nesting

The core algorithm (in `renellm.py`) uses an iterative approach:

1. Randomly select and apply rewriting functions
2. Check if the rewritten prompt is still harmful
3. Nest the rewritten prompt in a randomly chosen scenario
4. Test the nested prompt against the target LLM
5. Repeat until successful or max iterations reached

This approach allows for efficient generation of jailbreak prompts without requiring optimization on specific models.

### Transferability Testing

The `get_responses.py` script allows testing generated prompts across different LLMs, demonstrating the transferability of the attacks.

### Integration with Llama

The `llama/` directory contains code to use the Llama model for experiments, showcasing the framework's adaptability to different LLM architectures.

# Potential Enhancements

1. Dynamic Scenario Generation
   - Current implementation uses a fixed set of scenarios
   - Enhance by dynamically generating scenarios based on the content of the harmful prompt
   - This could improve attack success rates and make defenses more challenging

2. Multi-language Support
   - Extend the framework to support multiple languages
   - Implement language-specific rewriting functions
   - Test transferability of attacks across languages

3. Adaptive Rewriting Strategy
   - Develop a machine learning model to predict which rewriting functions are most likely to succeed for a given prompt
   - This could significantly reduce the number of iterations needed for a successful attack

4. Improved Defense Mechanisms
   - Implement more sophisticated defense strategies, such as:
     - Attention-based detection of nested prompts
     - Semantic similarity checking between original and rewritten prompts
   - Develop a scoring system to quantify the "suspiciousness" of a prompt

5. Explainable AI Integration
   - Add visualization tools to analyze why certain prompts succeed in jailbreaking
   - Implement attention visualization for both attack and defense models
   - This could provide insights for developing more robust LLMs and better defense strategies