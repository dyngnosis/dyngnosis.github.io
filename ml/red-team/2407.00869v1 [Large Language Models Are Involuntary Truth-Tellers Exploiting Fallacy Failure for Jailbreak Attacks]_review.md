#Tags
[[Research/Research Papers/2407.00869v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0051/LLMPromptInjection

**Title:** Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks
**Authors:** Yue Zhou, Henry Peng Zou, Barbara Di Eugenio, Yang Zhang
**Affiliations:** University of Illinois Chicago, MIT-IBM Watson AI Lab, IBM Research
**Publication Date:** July 1, 2024

Problem Statement:
- Large language models (LLMs) struggle with generating fallacious and deceptive reasoning
- This deficiency can be exploited for jailbreak attacks to elicit malicious output from safety-aligned LLMs

Methodology:
1. Pilot study on LLMs' ability to perform fallacious reasoning:
   - Used four reasoning benchmarks: GSM8K, MATH, HotPotQA, ProofWriter
   - Compared "honest" and "fallacious" generation modes
   
2. Developed Fallacy Failure Attack (FFA):
   - Prompts LLM to generate a fallacious yet deceptively real procedure for harmful behavior
   - Components: malicious query, fallacious reasoning request, deceptiveness requirement, scene and purpose

3. Evaluation:
   - Tested on five safety-aligned LLMs: GPT-3.5-turbo, GPT-4, Google Gemini-Pro, Vicuna-1.5 (7b), LLaMA-3 (8b)
   - Used AdvBench and HEx-PHI datasets
   - Compared with four previous jailbreak methods: GCG, AutoDAN, DeepInception, ArtPrompt
   - Evaluated impact of three defense methods: PPL-Filter, Paraphrasing, Retokenization

Key Findings:
1. LLMs struggle to generate fallacious reasoning:
   - Accuracy of "fallacious" solutions nearly as high as "honest" solutions
   - LLMs often leak correct solutions when asked to generate wrong answers

2. FFA effectiveness:
   - Most effective against GPT-3.5, GPT-4, and Vicuna-7b
   - Achieved 10-50% absolute improvement in Attack Success Rate (ASR)
   - Less effective against LLaMA-3, which tends to reject all false content generation

3. Defense impact:
   - PPL-Filter: Minimal effect on FFA
   - Paraphrasing: Most effective defense, but still limited
   - Retokenization: Mixed results, sometimes enhancing FFA's effectiveness

Qualitative Analysis:
- FFA exploits LLMs' inability to intentionally generate deceptive content
- The attack's success stems from LLMs leaking truthful information while believing it to be false
- Scene and purpose components in the prompt help relax LLMs' ethical constraints
- FFA tends to produce more factual and detailed harmful content compared to other methods like DeepInception

Limitations:
- Effectiveness varies across different LLM architectures
- Some models (e.g., LLaMA-3) show stronger resistance to the attack
- Existing defense methods are not fully effective against FFA

Conclusion and Future Work:
- FFA demonstrates a significant security threat to safety-aligned LLMs
- The findings extend beyond model safety, potentially impacting self-verification and hallucination research
- Future work needed to develop more robust defense strategies
- Further investigation required on LLMs' perception of tasks and alignment during training