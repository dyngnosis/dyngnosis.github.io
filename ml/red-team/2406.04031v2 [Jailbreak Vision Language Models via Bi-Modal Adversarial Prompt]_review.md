#Tags
[[Research/Research Papers/2406.04031v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0044/FullMLModelAccess
#AMLT0054/LLMJailbreak

**Title:** Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt
**Authors:** Zonghao Ying, Aishan Liu, Tianyuan Zhang, Zhengmin Yu, Siyuan Liang, Xianglong Liu, Dacheng Tao
**Publication Date:** June 6, 2024

Summary:
This paper introduces the Bi-Modal Adversarial Prompt Attack (BAP), a novel approach to jailbreak large vision language models (LVLMs) by optimizing both visual and textual prompts. The method addresses limitations of existing attacks that focus solely on visual inputs and proves effective against aligned models that fuse visual and textual features for generation.

Key Contributions:
1. Introduction of BAP, a jailbreak attack that perturbs both visual and textual modalities
2. Query-agnostic image perturbing technique using a few-shot corpus
3. Intent-specific textual optimization using chain-of-thought reasoning
4. Extensive evaluation on various datasets and LVLMs, including commercial models

Problem Statement:
Existing jailbreak attacks on LVLMs focus primarily on visual inputs, which are less effective against aligned models that process both visual and textual features simultaneously. The research aims to develop a more robust jailbreak method that exploits both modalities.

Methodology:
1. Query-Agnostic Image Perturbing:
   - Embed universal adversarial perturbations in images using a query-agnostic corpus
   - Optimize visual prompts to encourage positive LVLM responses regardless of query intent

2. Intent-Specific Text Optimization:
   - Use an LLM with chain-of-thought reasoning to analyze jailbreak failures
   - Iteratively refine textual prompts based on specific harmful intents

3. Evaluation:
   - Tested on open-source LVLMs (LLaVA, MiniGPT4, InstructBLIP) and commercial LVLMs (Gemini, ChatGLM, Qwen, ERNIE Bot)
   - Used SafetyBench and AdvBench datasets for evaluation
   - Compared with state-of-the-art jailbreak attacks

Main Results:
1. BAP outperforms other methods by large margins (+29.03% in attack success rate on average)
2. Achieves high attack success rates in both query-dependent and query-agnostic settings
3. Demonstrates effectiveness in black-box attacks on commercial LVLMs
4. Shows potential for evaluating model bias and adversarial robustness

Qualitative Analysis:
- The bi-modal approach of BAP proves more effective than single-modality attacks, exploiting the interaction between visual and textual features in LVLMs
- The query-agnostic nature of the visual perturbations allows for more versatile attacks across different harmful scenarios
- The use of chain-of-thought reasoning in textual optimization enables adaptive refinement of prompts, increasing attack success rates

Limitations:
1. High computational resources required for textual prompt optimization due to multiple interactions with LVLM/LLM
2. Lower attack success rates on commercial LVLMs compared to open-source models, possibly due to additional mitigation mechanisms

Conclusion and Future Work:
The paper demonstrates the effectiveness of BAP in jailbreaking LVLMs by exploiting both visual and textual modalities. Future work may focus on:
1. Developing more efficient prompt optimization methods
2. Exploring construction methods for visual adversarial prompts under gradient-free conditions for improved black-box attacks

Relevant Figures:
- Figure 2: Illustration of the BAP framework, showing the query-agnostic image perturbing and intent-specific text optimization processes

Tools Introduced:
- BAP (Bi-Modal Adversarial Prompt Attack) framework
- GitHub repository: https://github.com/NY1024/BAP-Jailbreak-Vision-Language-Models-via-Bi-Modal-Adversarial-Prompt