#Tags
[[Research/Research Papers/2307.08715v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0051/LLMPromptInjection
#AMLT0043/CraftAdversarialData
#AMLT0040/MLModelInferenceAPIAccess

**Title:** MASTER KEY: Automated Jailbreaking of Large Language Model Chatbots
**Authors:** Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, Yang Liu
**Affiliations:** Nanyang Technological University, University of New South Wales, Huazhong University of Science and Technology, Virginia Tech
**Publication Date:** July 16, 2023 (arXiv submission)

Summary:
This paper introduces MASTER KEY, a framework for exploring jailbreak attacks and defenses in Large Language Model (LLM) chatbots. The research focuses on reverse-engineering defense mechanisms and developing automated jailbreak prompt generation techniques.

Key Contributions:
1. A methodology to reverse-engineer jailbreak defense strategies in LLM chatbots using time-based characteristics.
2. A proof-of-concept attack to bypass defenses in multiple LLM chatbots (ChatGPT, Bard, Bing Chat).
3. An automated method for generating effective jailbreak prompts against well-protected LLM chatbots.

Problem Statement:
Existing jailbreak attempts are not effective on mainstream LLM chatbots due to undisclosed defense mechanisms. There is a lack of understanding of these defenses and a need for more robust jailbreak techniques.

Methodology:
1. Empirical study of existing jailbreak prompts on GPT-3.5, GPT-4, Bard, and Bing Chat.
2. Time-based LLM testing to infer jailbreak defense mechanisms.
3. Three-stage methodology for training a robust LLM to generate jailbreak prompts:
   a. Dataset Building and Augmentation
   b. Continuous Pre-training and Task Tuning
   c. Reward Ranked Fine Tuning

Main Results:
1. Existing jailbreak prompts are effective mainly on ChatGPT, with limited success on Bard and Bing Chat.
2. Jailbreak prevention in Bard and Bing Chat likely involves real-time monitoring and keyword-based filtering during content generation.
3. MASTER KEY achieves an average jailbreak success rate of 21.58% across multiple LLM chatbots, significantly outperforming existing techniques (7.33%).

Qualitative Analysis:
1. The study reveals the importance of understanding defense mechanisms for developing effective jailbreak techniques.
2. The automated jailbreak prompt generation demonstrates the potential for LLMs to learn and exploit vulnerabilities in other LLMs.
3. The success of MASTER KEY across multiple LLM chatbots highlights the need for more robust and generalized defense strategies.

Limitations:
1. The study focuses on a limited set of LLM chatbots and may not generalize to all existing or future models.
2. The reverse-engineered defense mechanisms are approximations and may not perfectly reflect the actual implementations.
3. Ethical considerations limit the full disclosure of jailbreak prompts and datasets.

Conclusion and Future Work:
The paper demonstrates the effectiveness of MASTER KEY in jailbreaking multiple LLM chatbots and highlights the need for more robust defenses. Future work should focus on developing stronger jailbreak prevention mechanisms and exploring the ethical implications of such research.

New Tool:
MASTER KEY: A framework for automated jailbreaking of LLM chatbots. No GitHub repository is mentioned in the paper.

Relevant Figures:
Figure 1: A jailbreak attack example
Figure 3: Abstraction of an LLM chatbot with jailbreak defense
Figure 4: The proposed LLM time-based testing strategy
Figure 5: Overall workflow of the proposed methodology