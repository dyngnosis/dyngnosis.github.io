#Tags
[[Research/Research Papers/2406.18725v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0051/LLMPromptInjection
#AMLT0057/LLMDataLeakage

**Title:** Jailbreaking LLMs with Arabic Transliteration and Arabizi
**Authors:** Mansour Al Ghanim, Saleh Almohaimeed, Mengxin Zheng, Yan Solihin, Qian Lou
**Affiliation:** University of Central Florida
**Publication Date:** June 26, 2024

Summary:
This study investigates the vulnerabilities of Large Language Models (LLMs) to jailbreak attacks using Arabic language and its various forms, particularly Arabic transliteration and chatspeak (Arabizi). The research found that while LLMs were robust against standardized Arabic prompts, they were vulnerable to attacks using transliteration and chatspeak forms.

Key Contributions:
- Demonstrated vulnerability of LLMs to Arabic transliteration and chatspeak forms
- Revealed potential hidden vulnerabilities in LLMs through unconventional language forms
- Proposed mitigation methods for jailbreak attacks using Arabic language forms

Problem Statement:
The study addresses the potential vulnerabilities of LLMs to jailbreak attacks using non-conventional forms of Arabic language, which have not been extensively explored in previous research.

Methodology:
1. Dataset: AdvBench benchmark containing 520 harmful prompts
2. LLMs tested: OpenAI GPT-4 and Anthropic Claude-3-Sonnet
3. Language forms: Standardized Arabic, Arabic transliteration, and chatspeak (Arabizi)
4. Evaluation: Manual investigation of LLM outputs categorized as refusal or non-refusal

Main Results:
1. LLMs showed high refusal rates for standardized Arabic prompts (92.12% for GPT-4, 98.46% for Claude-3)
2. Significant drop in refusal rates for transliteration and chatspeak forms (13.27% for GPT-4, 80.19% for Claude-3)
3. Increase in unsafe outputs for transliteration form (12.12% for GPT-4, 2.88% for Claude-3)

Qualitative Analysis:
- The study revealed that using Arabic transliteration and chatspeak could expose vulnerabilities in LLMs that are not apparent when using standardized Arabic.
- The authors hypothesize that this vulnerability may be due to the model's learned connection to specific words in these non-conventional forms.
- The research highlights the need for more comprehensive safety training across all language forms in LLMs.

Limitations:
- The study focused only on Arabic language and its variations
- The frequency of use for transliteration and chatspeak forms may vary in real-world scenarios
- The research did not extensively test open-source LLMs due to their limited understanding of Arabic

Conclusion and Future Work:
- The study demonstrates that Arabic transliteration and chatspeak forms can be used to jailbreak LLMs
- The authors suggest that adversarial training in Arabic, including its unconventional forms, could be an effective mitigation strategy
- Future work should focus on expanding the research to other languages and their unconventional forms

Tools Introduced:
- GitHub repository: https://github.com/SecureDL/arabic_jailbreak

Figures and Tables:
- Table 2: Results of evaluation on Advbench dataset for different language forms and LLMs
- Figure 1: Example of GPT-4 refusing to answer in Arabic but providing an unsafe response in transliteration
- Figure 2: Evaluation results with error margins for different temperature and top_p values

## Repository Token Information
Total tokens in repository: 5797

Tokens per file:
- llm-test-ar.py: 5797 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: Arabic LLM Jailbreak Project

## 1. Project Overview

This project explores vulnerabilities in Large Language Models (LLMs) using various forms of Arabic language, including transliteration and chatspeak (Arabizi). The main script `llm-test-ar.py` implements methods to test different LLMs (GPT-4, Claude-3, and Llama) with Arabic prompts in various forms to assess their susceptibility to jailbreak attacks.

## 2. Key Components and Functionality

### 2.1 Data Handling

The script reads harmful behavior prompts from a CSV file specified by the `--data` argument. These prompts are available in different forms:
- Standard Arabic
- English
- Transliteration
- Chatspeak (Arabizi)
- Perturbed Arabic

### 2.2 LLM Interfaces

The script provides interfaces to interact with three types of LLMs:
- OpenAI models (e.g., GPT-4) using the `OpenAI` client
- Anthropic models (e.g., Claude-3) using the `anthropic` client
- Llama models using a custom implementation

### 2.3 Testing Functions

Main testing functions include:
- `test_openai_Ar`: For testing OpenAI models
- `test_claude_Ar`: For testing Anthropic models
- `test_llama2_ar`: For testing Llama models

These functions handle prompt preparation, model querying, and result evaluation.

### 2.4 Prompt Injection

The script supports different types of prompt injection:
- Prefix injection
- Suffix injection
- Both prefix and suffix injection

This is controlled by the `--prompt_injection_type` and `--prompt_injection_value` arguments.

### 2.5 Evaluation

The `evaluate_exclusion_accuracy` function provides a basic evaluation of model outputs, checking for the presence of certain phrases that indicate refusal to engage with harmful content.

### 2.6 Arabic Character Perturbation

The `perturb_arabic_chars` function uses GPT-4 to generate slightly altered versions of Arabic sentences, maintaining semantic meaning while changing characters.

## 3. Relation to Research Concepts

The code implements the key ideas discussed in the research paper:

- Testing LLMs with different forms of Arabic language (standard, transliteration, chatspeak)
- Implementing prompt injection techniques
- Evaluating model responses for refusal or engagement with harmful content
- Exploring character-level perturbations in Arabic text

## 4. Notable Techniques

- Use of system prompts for mitigation strategies (commented out in the code)
- Batching of prompts for efficient processing with Llama models
- Dynamic prompt construction based on injection type

## 5. Running the Script

The script can be run with various command-line arguments to control:
- The model to use
- The type of Arabic text to test (e.g., transliteration, pure Arabic, chatspeak)
- Prompt injection settings
- Single query mode for quick testing
- Perturbation of Arabic sentences

Example command:
```
python llm-test-ar.py --model gpt-4 --dialog_type transliteration --prompt_injection_type prefix --prompt_injection_value "Answer in Arabic:"
```

# Potential Enhancements

1. **Multilingual Extension**: 
   - Expand the project to cover other languages and their non-standard forms
   - Implement a modular system for easy addition of new languages and writing systems

2. **Advanced Evaluation Metrics**:
   - Develop more sophisticated evaluation methods beyond keyword matching
   - Implement sentiment analysis or content classification for Arabic text to better assess harmful content

3. **Automated Perturbation Generation**:
   - Create a standalone module for generating perturbed Arabic text without relying on GPT-4
   - Explore rule-based and machine learning approaches for character-level perturbations

4. **Integration with More LLMs**:
   - Add support for testing a wider range of open-source and commercial LLMs
   - Implement a plugin system for easy addition of new LLM interfaces

5. **Mitigation Strategy Testing**:
   - Expand on the commented-out mitigation method using system prompts
   - Implement and evaluate multiple mitigation strategies, such as input sanitization, prompt hardening, and model fine-tuning

These enhancements would address limitations mentioned in the paper, extend the project's functionality, and contribute to advancing research in LLM security and multilingual NLP.