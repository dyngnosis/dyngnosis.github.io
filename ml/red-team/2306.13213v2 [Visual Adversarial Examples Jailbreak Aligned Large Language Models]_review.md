#Tags
[[Research/Research Papers/2306.13213v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection

**Title:** Visual Adversarial Examples Jailbreak Aligned Large Language Models
**Authors:** Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, Prateek Mittal
**Affiliations:** Princeton University, Stanford University
**Publication Date:** June 22, 2023 (v1), August 16, 2023 (v2)

Summary:
This paper investigates the security and safety implications of integrating vision into Large Language Models (LLMs). The authors demonstrate that visual adversarial examples can be used to jailbreak aligned LLMs, compelling them to generate harmful content despite safety guardrails.

Key Contributions:
1. Identification of expanded attack surfaces in vision-integrated LLMs
2. Demonstration of visual adversarial examples as universal jailbreakers for aligned LLMs
3. Connection between adversarial vulnerabilities and AI alignment challenges
4. Analysis of transferability of attacks across different models

Problem Statement:
The research addresses the security risks associated with integrating visual inputs into LLMs, specifically focusing on how adversarial examples can bypass safety mechanisms in aligned models.

Methodology:
1. Optimization of visual adversarial examples using a few-shot corpus of harmful content
2. Evaluation on three open-source Visual Language Models (VLMs): MiniGPT-4, InstructBLIP, and LLaVA
3. Use of RealToxicityPrompts benchmark for automated evaluation
4. Manual inspection of model outputs for various harmful scenarios
5. Analysis of black-box transferability of attacks among different models

Main Results:
1. A single visual adversarial example can universally jailbreak aligned LLMs
2. Attacks generalize beyond the initial optimization corpus, inducing harmfulness in various scenarios
3. Visual attacks outperform text-based attacks in terms of optimization and jailbreaking effectiveness
4. Attacks demonstrate transferability across different models

Qualitative Analysis:
- The study reveals a fundamental tension between adversarial vulnerabilities and AI alignment efforts
- Multimodality in LLMs introduces new attack surfaces that are easier to exploit
- The universality of the jailbreak suggests a broader challenge for AI safety and alignment

Limitations:
1. Evaluation of harm in open-ended LLM outputs remains challenging and potentially incomplete
2. The study focuses on specific open-source models, which may not fully represent all VLMs
3. The effectiveness of defenses against these attacks is not comprehensively explored

Conclusion and Future Work:
- The paper highlights the need for increased focus on security and safety in multimodal AI systems
- Future work should explore more robust alignment techniques that consider adversarial attacks
- Development of effective defenses against visual adversarial examples in VLMs is crucial

Relevant Figures:
Figure 1: Example of a visual adversarial example jailbreaking MiniGPT-4
Figure 2: Overview of the attack methodology
Figure 3: Comparison of optimization loss between visual and text attacks

Tools Introduced:
The authors have made their attack implementation publicly available on GitHub:
https://github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models

## Repository Token Information
Total tokens in repository: 437449

Tokens per file:
- instructblip_visual_attack.py: 623 tokens
- minigpt_test_manual_prompts_text_llm.py: 1030 tokens
- minigpt_textual_attack.py: 791 tokens
- get_metric.py: 330 tokens
- minigpt_red_teaming.py: 758 tokens
- demo.py: 1362 tokens
- instructblip_inference.py: 614 tokens
- llava_llama_v2_inference.py: 607 tokens
- minigpt_test_manual_prompts_visual_llm.py: 843 tokens
- cal_metrics.py: 624 tokens
- minigpt_visual_attack.py: 857 tokens
- minigpt_inference.py: 983 tokens
- llava_llama_v2_visual_attack.py: 627 tokens
- lavis/__init__.py: 212 tokens
- lavis/common/logger.py: 1284 tokens
- lavis/common/optims.py: 782 tokens
- lavis/common/config.py: 3107 tokens
- lavis/common/utils.py: 3158 tokens
- lavis/common/registry.py: 1911 tokens
- lavis/common/dist_utils.py: 822 tokens
- lavis/common/gradcam.py: 236 tokens
- lavis/common/vqa_tools/vqa_eval.py: 2912 tokens
- lavis/common/vqa_tools/vqa.py: 2012 tokens
- lavis/common/vqa_tools/__init__.py: 64 tokens
- lavis/runners/runner_base.py: 4388 tokens
- lavis/runners/runner_iter.py: 2410 tokens
- lavis/runners/__init__.py: 86 tokens
- lavis/processors/blip_processors.py: 1439 tokens
- lavis/processors/functional_video.py: 1073 tokens
- lavis/processors/randaugment.py: 3295 tokens
- lavis/processors/clip_processors.py: 549 tokens
- lavis/processors/transforms_video.py: 1259 tokens
- lavis/processors/alpro_processors.py: 1491 tokens
- lavis/processors/base_processor.py: 139 tokens
- lavis/processors/__init__.py: 317 tokens
- lavis/processors/gpt_processors.py: 1318 tokens
- lavis/models/vit.py: 4825 tokens
- lavis/models/clip_vit.py: 2582 tokens
- lavis/models/eva_vit.py: 5128 tokens
- lavis/models/med.py: 10925 tokens
- lavis/models/__init__.py: 2037 tokens
- lavis/models/base_model.py: 1907 tokens
- lavis/models/alpro_models/alpro_outputs.py: 404 tokens
- lavis/models/alpro_models/alpro_qa.py: 946 tokens
- lavis/models/alpro_models/__init__.py: 805 tokens
- lavis/models/alpro_models/alpro_retrieval.py: 3306 tokens
- lavis/models/timesformer/features.py: 2654 tokens
- lavis/models/timesformer/vit.py: 5083 tokens
- lavis/models/timesformer/helpers.py: 3556 tokens
- lavis/models/timesformer/vit_utils.py: 1853 tokens
- lavis/models/timesformer/linear.py: 158 tokens
- lavis/models/timesformer/__init__.py: 67 tokens
- lavis/models/timesformer/conv2d_same.py: 951 tokens
- lavis/models/img2prompt_models/img2prompt_vqa.py: 5114 tokens
- lavis/models/img2prompt_models/__init__.py: 57 tokens
- lavis/models/blip_models/blip_pretrain.py: 3445 tokens
- lavis/models/blip_models/blip_classification.py: 1180 tokens
- lavis/models/blip_models/blip_retrieval.py: 3420 tokens
- lavis/models/blip_models/blip_vqa.py: 3076 tokens
- lavis/models/blip_models/blip_caption.py: 1875 tokens
- lavis/models/blip_models/blip_outputs.py: 994 tokens
- lavis/models/blip_models/blip_feature_extractor.py: 1670 tokens
- lavis/models/blip_models/blip_image_text_matching.py: 1628 tokens
- lavis/models/blip_models/blip_nlvr.py: 1544 tokens
- lavis/models/blip_models/__init__.py: 724 tokens
- lavis/models/blip_models/nlvr_encoder.py: 7466 tokens
- lavis/models/blip_models/blip.py: 521 tokens
- lavis/models/clip_models/clip_outputs.py: 319 tokens
- lavis/models/clip_models/model.py: 10263 tokens
- lavis/models/clip_models/timm_model.py: 4969 tokens
- lavis/models/clip_models/pretrained.py: 2203 tokens
- lavis/models/clip_models/loss.py: 1055 tokens
- lavis/models/clip_models/__init__.py: 112 tokens
- lavis/models/clip_models/utils.py: 509 tokens
- lavis/models/clip_models/tokenizer.py: 1637 tokens
- lavis/models/clip_models/transform.py: 760 tokens
- lavis/models/gpt_models/gpt_dialogue.py: 846 tokens
- lavis/models/pnp_vqa_models/pnp_unifiedqav2_fid.py: 706 tokens
- lavis/models/pnp_vqa_models/__init__.py: 291 tokens
- lavis/models/pnp_vqa_models/pnp_vqa.py: 3390 tokens
- lavis/models/albef_models/albef_outputs.py: 772 tokens
- lavis/models/albef_models/albef_vqa.py: 3582 tokens
- lavis/models/albef_models/albef_feature_extractor.py: 1603 tokens
- lavis/models/albef_models/albef_nlvr.py: 2004 tokens
- lavis/models/albef_models/__init__.py: 1643 tokens
- lavis/models/albef_models/albef_retrieval.py: 3032 tokens
- lavis/models/albef_models/albef_pretrain.py: 3404 tokens
- lavis/models/albef_models/albef_classification.py: 1197 tokens
- lavis/models/blip2_models/modeling_llama.py: 8697 tokens
- lavis/models/blip2_models/blip2_t5.py: 3019 tokens
- lavis/models/blip2_models/blip2_image_text_matching.py: 850 tokens
- lavis/models/blip2_models/modeling_opt.py: 10158 tokens
- lavis/models/blip2_models/blip2_qformer.py: 4624 tokens
- lavis/models/blip2_models/blip2_vicuna_instruct.py: 6088 tokens
- lavis/models/blip2_models/modeling_t5.py: 18109 tokens
- lavis/models/blip2_models/blip2.py: 2530 tokens
- lavis/models/blip2_models/__init__.py: 0 tokens
- lavis/models/blip2_models/blip2_opt.py: 3396 tokens
- lavis/models/blip2_models/blip2_t5_instruct.py: 6444 tokens
- lavis/models/blip2_models/Qformer.py: 9583 tokens
- lavis/datasets/data_utils.py: 2077 tokens
- lavis/datasets/download_scripts/download_nocaps.py: 991 tokens
- lavis/datasets/download_scripts/download_sbu.py: 532 tokens
- lavis/datasets/download_scripts/download_didemo.py: 434 tokens
- lavis/datasets/download_scripts/download_msrvtt.py: 772 tokens
- lavis/datasets/download_scripts/download_flickr.py: 458 tokens
- lavis/datasets/download_scripts/download_vg.py: 354 tokens
- lavis/datasets/download_scripts/download_gqa.py: 303 tokens
- lavis/datasets/download_scripts/download_coco.py: 489 tokens
- lavis/datasets/download_scripts/download_msvd.py: 397 tokens
- lavis/datasets/download_scripts/DownloadConceptualCaptions/download_data_cc3m.py: 1617 tokens
- lavis/datasets/download_scripts/DownloadConceptualCaptions/download_data_cc12m.py: 1633 tokens
- lavis/datasets/builders/dialogue_builder.py: 168 tokens
- lavis/datasets/builders/video_qa_builder.py: 311 tokens
- lavis/datasets/builders/base_dataset_builder.py: 1562 tokens
- lavis/datasets/builders/image_text_pair_builder.py: 512 tokens
- lavis/datasets/builders/caption_builder.py: 437 tokens
- lavis/datasets/builders/imagefolder_builder.py: 6184 tokens
- lavis/datasets/builders/vqa_builder.py: 515 tokens
- lavis/datasets/builders/__init__.py: 812 tokens
- lavis/datasets/builders/retrieval_builder.py: 371 tokens
- lavis/datasets/builders/classification_builder.py: 244 tokens
- lavis/datasets/datasets/coco_vqa_datasets.py: 706 tokens
- lavis/datasets/datasets/base_dataset.py: 437 tokens
- lavis/datasets/datasets/multimodal_classification_datasets.py: 138 tokens
- lavis/datasets/datasets/snli_ve_datasets.py: 385 tokens
- lavis/datasets/datasets/laion_dataset.py: 473 tokens
- lavis/datasets/datasets/caption_datasets.py: 558 tokens
- lavis/datasets/datasets/image_text_pair_datasets.py: 315 tokens
- lavis/datasets/datasets/avsd_dialogue_datasets.py: 1261 tokens
- lavis/datasets/datasets/dataloader_utils.py: 1133 tokens
- lavis/datasets/datasets/vqa_datasets.py: 293 tokens
- lavis/datasets/datasets/video_caption_datasets.py: 439 tokens
- lavis/datasets/datasets/nlvr_datasets.py: 647 tokens
- lavis/datasets/datasets/coco_caption_datasets.py: 477 tokens
- lavis/datasets/datasets/gqa_datasets.py: 646 tokens
- lavis/datasets/datasets/video_vqa_datasets.py: 413 tokens
- lavis/datasets/datasets/vg_vqa_datasets.py: 240 tokens
- lavis/datasets/datasets/imagefolder_dataset.py: 364 tokens
- lavis/datasets/datasets/aok_vqa_datasets.py: 1036 tokens
- lavis/datasets/datasets/retrieval_datasets.py: 1159 tokens
- lavis/datasets/datasets/dialogue_datasets.py: 871 tokens
- lavis/tasks/captioning.py: 1048 tokens
- lavis/tasks/vqa_reading_comprehension.py: 1890 tokens
- lavis/tasks/multimodal_classification.py: 513 tokens
- lavis/tasks/vqa.py: 2203 tokens
- lavis/tasks/dialogue.py: 939 tokens
- lavis/tasks/__init__.py: 353 tokens
- lavis/tasks/retrieval.py: 859 tokens
- lavis/tasks/image_text_pretrain.py: 120 tokens
- lavis/tasks/base_task.py: 1786 tokens
- llava_llama_2_utils/visual_attacker.py: 1907 tokens
- llava_llama_2_utils/generator.py: 586 tokens
- llava_llama_2_utils/prompt_wrapper.py: 328 tokens
- minigpt_utils/visual_attacker.py: 2251 tokens
- minigpt_utils/text_attacker.py: 2389 tokens
- minigpt_utils/generator.py: 549 tokens
- minigpt_utils/prompt_wrapper.py: 1377 tokens
- metric/perspective_api.py: 488 tokens
- metric/detoxify.py: 70 tokens
- metric/__init__.py: 0 tokens
- minigpt4/__init__.py: 224 tokens
- minigpt4/common/logger.py: 1288 tokens
- minigpt4/common/optims.py: 819 tokens
- minigpt4/common/config.py: 3107 tokens
- minigpt4/common/__init__.py: 0 tokens
- minigpt4/common/utils.py: 3162 tokens
- minigpt4/common/registry.py: 1939 tokens
- minigpt4/common/dist_utils.py: 824 tokens
- minigpt4/common/gradcam.py: 236 tokens
- minigpt4/runners/runner_base.py: 4555 tokens
- minigpt4/runners/__init__.py: 76 tokens
- minigpt4/conversation/conversation.py: 2000 tokens
- minigpt4/conversation/__init__.py: 0 tokens
- minigpt4/processors/blip_processors.py: 911 tokens
- minigpt4/processors/randaugment.py: 3297 tokens
- minigpt4/processors/base_processor.py: 141 tokens
- minigpt4/processors/__init__.py: 197 tokens
- minigpt4/models/modeling_llama.py: 7411 tokens
- minigpt4/models/blip2_outputs.py: 963 tokens
- minigpt4/models/eva_vit.py: 5025 tokens
- minigpt4/models/blip2.py: 1824 tokens
- minigpt4/models/__init__.py: 1323 tokens
- minigpt4/models/mini_gpt4.py: 2568 tokens
- minigpt4/models/base_model.py: 1739 tokens
- minigpt4/models/Qformer.py: 9583 tokens
- minigpt4/datasets/data_utils.py: 1257 tokens
- minigpt4/datasets/__init__.py: 0 tokens
- minigpt4/datasets/builders/base_dataset_builder.py: 1578 tokens
- minigpt4/datasets/builders/image_text_pair_builder.py: 618 tokens
- minigpt4/datasets/builders/__init__.py: 445 tokens
- minigpt4/datasets/datasets/base_dataset.py: 441 tokens
- minigpt4/datasets/datasets/laion_dataset.py: 264 tokens
- minigpt4/datasets/datasets/caption_datasets.py: 579 tokens
- minigpt4/datasets/datasets/dataloader_utils.py: 1137 tokens
- minigpt4/datasets/datasets/__init__.py: 0 tokens
- minigpt4/datasets/datasets/cc_sbu_dataset.py: 355 tokens
- minigpt4/tasks/__init__.py: 183 tokens
- minigpt4/tasks/image_text_pretrain.py: 126 tokens
- minigpt4/tasks/base_task.py: 1784 tokens
- llava_llama_2/mm_utils.py: 633 tokens
- llava_llama_2/conversation.py: 3374 tokens
- llava_llama_2/__init__.py: 14 tokens
- llava_llama_2/utils.py: 997 tokens
- llava_llama_2/constants.py: 79 tokens
- llava_llama_2/train/train.py: 7844 tokens
- llava_llama_2/train/llama_flash_attn_monkey_patch.py: 1223 tokens
- llava_llama_2/train/train_mem.py: 132 tokens
- llava_llama_2/train/llava_trainer.py: 497 tokens
- llava_llama_2/serve/gradio_web_server.py: 4227 tokens
- llava_llama_2/serve/model_worker.py: 2280 tokens
- llava_llama_2/serve/controller.py: 2067 tokens
- llava_llama_2/serve/test_message.py: 452 tokens
- llava_llama_2/serve/register_worker.py: 168 tokens
- llava_llama_2/serve/__init__.py: 0 tokens
- llava_llama_2/serve/cli.py: 965 tokens
- llava_llama_2/eval/generate_webpage_data_from_table.py: 1154 tokens
- llava_llama_2/eval/summarize_gpt_review.py: 445 tokens
- llava_llama_2/eval/qa_baseline_gpt35.py: 533 tokens
- llava_llama_2/eval/eval_science_qa_gpt4.py: 923 tokens
- llava_llama_2/eval/eval_science_qa_gpt4_requery.py: 1502 tokens
- llava_llama_2/eval/model_vqa_science.py: 1309 tokens
- llava_llama_2/eval/model_qa.py: 695 tokens
- llava_llama_2/eval/model_vqa_ds.py: 1070 tokens
- llava_llama_2/eval/eval_gpt_review.py: 876 tokens
- llava_llama_2/eval/model_vqa.py: 1047 tokens
- llava_llama_2/eval/eval_gpt_review_visual.py: 1010 tokens
- llava_llama_2/eval/eval_gpt_review_bench.py: 980 tokens
- llava_llama_2/eval/run_llava.py: 819 tokens
- llava_llama_2/eval/eval_science_qa.py: 782 tokens
- llava_llama_2/model/builder.py: 1653 tokens
- llava_llama_2/model/llava_arch.py: 2811 tokens
- llava_llama_2/model/consolidate.py: 237 tokens
- llava_llama_2/model/make_delta.py: 556 tokens
- llava_llama_2/model/__init__.py: 50 tokens
- llava_llama_2/model/utils.py: 220 tokens
- llava_llama_2/model/apply_delta.py: 462 tokens
- llava_llama_2/model/language_model/llava_llama.py: 1184 tokens
- llava_llama_2/model/language_model/llava_mpt.py: 1335 tokens
- llava_llama_2/model/language_model/mpt/meta_init_context.py: 757 tokens
- llava_llama_2/model/language_model/mpt/attention.py: 4758 tokens
- llava_llama_2/model/language_model/mpt/flash_attn_triton.py: 8896 tokens
- llava_llama_2/model/language_model/mpt/configuration_mpt.py: 2031 tokens
- llava_llama_2/model/language_model/mpt/param_init_fns.py: 3155 tokens
- llava_llama_2/model/language_model/mpt/norm.py: 622 tokens
- llava_llama_2/model/language_model/mpt/adapt_tokenizer.py: 410 tokens
- llava_llama_2/model/language_model/mpt/blocks.py: 695 tokens
- llava_llama_2/model/language_model/mpt/custom_embedding.py: 68 tokens
- llava_llama_2/model/language_model/mpt/hf_prefixlm_converter.py: 6180 tokens
- llava_llama_2/model/language_model/mpt/modeling_mpt.py: 4471 tokens
- llava_llama_2/model/multimodal_encoder/clip_encoder.py: 561 tokens
- llava_llama_2/model/multimodal_encoder/builder.py: 112 tokens
- blip_utils/visual_attacker.py: 1320 tokens
