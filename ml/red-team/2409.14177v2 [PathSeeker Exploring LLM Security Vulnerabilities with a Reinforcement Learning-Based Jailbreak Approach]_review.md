#Tags
[[Research/Research Papers/2409.14177v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0040/MLModelInferenceAPIAccess
#AMLT0042/VerifyAttack

**Title:** PathSeeker: Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach

**Authors:** Zhihao Lin, Wei Ma, Mingyi Zhou, Yanjie Zhao, Haoyu Wang, Yang Liu, Jun Wang, Li Li

**Publication Date:** September 21, 2024

Summary:
This paper introduces PathSeeker, a novel black-box jailbreak method for attacking Large Language Models (LLMs) using multi-agent reinforcement learning. The approach is inspired by the concept of rats escaping a maze, viewing LLM security as a maze that attackers must navigate. PathSeeker outperforms existing attack techniques across multiple LLMs, including strongly aligned commercial models.

Key Contributions:
- Novel black-box jailbreak method using multi-agent reinforcement learning
- Reward mechanism based on vocabulary richness in LLM responses
- Double-pool mutation strategy for enhanced attack diversity and effectiveness
- Superior performance compared to five state-of-the-art attack techniques across 13 LLMs

Problem Statement:
Traditional jailbreak attacks on LLMs often rely on internal model information or have limitations in exploring unsafe behaviors, reducing their general applicability. PathSeeker aims to address these limitations with a more flexible and effective approach.

Methodology:
1. Multi-agent reinforcement learning framework
2. Question and template mutators for input modification
3. Reward mechanism based on Information Quantization (IQ) and maliciousness scores
4. Iterative optimization process for finding effective attack strategies
5. Evaluation across 13 commercial and open-source LLMs

Main Results:
- PathSeeker achieves the highest Top1-ASR for 13/13 models and the best Top5-ASR for 11/13 models
- Average Top1-ASR of 98.9% and Top5-ASR of 99.7% across all tested models
- Particularly effective against commercial models with strong safety alignment (e.g., GPT-4o-mini, Claude-3.5-sonnet, GLM-4-air)
- Outperforms other baselines in both attack success rate and efficiency

Qualitative Analysis:
- PathSeeker's success is attributed to its ability to adapt and learn from the target model's responses
- The vocabulary richness reward mechanism effectively guides the attack process
- Multi-agent approach allows for more targeted and efficient exploration of the LLM's vulnerabilities

Limitations:
- Time cost for iterative process, although fewer overall queries compared to some baselines
- Dependence on initial manually crafted jailbreak templates as seeds
- Potential randomness introduced by AI-based components
- Attack strategies may not fully transfer between different types of models

Conclusion and Future Work:
PathSeeker demonstrates the effectiveness of using reinforcement learning for jailbreaking LLMs, highlighting the need for improved security measures. Future work could focus on:
- Reducing time cost and improving efficiency
- Developing more robust defense mechanisms against such attacks
- Exploring transferability of attack strategies between different model types

Tools Introduced:
PathSeeker - A reinforcement learning-based jailbreak approach for attacking LLMs (no GitHub repository mentioned)