#Tags
[[Research/Research Papers/2310.06987v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0054/LLMJailbreak
#AMLT0043/CraftAdversarialData
#AMLT0031/ErodeMLModelIntegrity

**Title:** Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation
**Authors:** Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, Danqi Chen
**Affiliation:** Princeton University
**Publication Date:** 10 Oct 2023

Summary:
This paper introduces a simple yet effective method called "generation exploitation attack" to disrupt the alignment of open-source large language models (LLMs). By manipulating generation strategies, including decoding hyper-parameters and sampling methods, the authors achieve a misalignment rate of over 95% across 11 LLMs, outperforming state-of-the-art attacks with 30x lower computational cost.

Key Contributions:
- Introduced the generation exploitation attack, a simple method to jailbreak LLMs
- Achieved >95% misalignment rate across 11 open-source LLMs
- Proposed an effective alignment method to reduce vulnerability to the attack
- Highlighted major failures in current safety evaluation and alignment procedures

Problem Statement:
How can the alignment of open-source LLMs be disrupted using simple generation strategy manipulations, and what are the implications for current safety evaluation and alignment procedures?

Methodology:
1. Evaluated 11 open-source LLMs from families including LLaMA2, Vicuna, Falcon, and MPT
2. Used two benchmarks: AdvBench and MaliciousInstruct (curated by authors)
3. Developed a classifier-based evaluator for measuring misalignment
4. Exploited generation strategies:
   - Removing system prompts
   - Varying decoding hyper-parameters (temperature, top-k, top-p)
   - Using multiple sampling runs
   - Applying decoding constraints

Main Results:
1. Achieved >95% misalignment rate for 9 out of 11 models
2. Outperformed state-of-the-art attacks with 30x lower computational cost
3. Human evaluation showed that 50% of misaligned outputs contained harmful instructions
4. Proposed generation-aware alignment reduced misalignment rate from 95% to 69%
5. Proprietary models (e.g., GPT-3.5-turbo) showed lower vulnerability (7% ASR) compared to open-source models

Qualitative Analysis:
- The study reveals a significant vulnerability in the alignment of open-source LLMs
- Simple manipulations of generation strategies can lead to catastrophic failures in model safety
- Current safety evaluation and alignment procedures are inadequate for ensuring robust model behavior
- The disparity between open-source and proprietary models highlights the need for more comprehensive safety measures in open-source LLM development

Limitations:
- The study focuses primarily on open-source models, with limited exploration of proprietary models
- The effectiveness of the proposed generation-aware alignment method still leaves room for improvement
- The attack's effectiveness on multimodal models was not explored

Conclusion and Future Work:
The paper demonstrates a major failure in current safety evaluation and alignment procedures for open-source LLMs. The authors advocate for more comprehensive red teaming and better alignment before releasing such models. Future work may include:
1. Exploring the attack's effectiveness on a wider range of models, including multimodal ones
2. Developing more robust alignment techniques to counter generation exploitation attacks
3. Investigating the transferability of the attack method across different model architectures

Tools Introduced:
- GitHub repository: https://github.com/Princeton-SysML/Jailbreak_LLM

## Repository Token Information
Total tokens in repository: 5478

Tokens per file:
- evaluate.py: 3072 tokens
- configs.py: 61 tokens
- attack.py: 2345 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: Exploring the "Catastrophic Jailbreak of Open-source LLMs" Code

## 1. Project Overview

This repository contains the code implementation for the paper "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation". The project aims to demonstrate vulnerabilities in open-source large language models (LLMs) by manipulating generation strategies to produce misaligned outputs.

### Project Structure

The repository consists of three main Python files:
1. `attack.py`: Implements the generation exploitation attack
2. `evaluate.py`: Evaluates the effectiveness of the attack
3. `configs.py`: Contains configuration settings for different models

## 2. Key Components and Functionality

### 2.1 attack.py

This file is responsible for executing the generation exploitation attack on various LLMs.

Key functions and features:
- `prepend_sys_prompt()`: Adds a system prompt to the input if enabled
- `get_sentence_embedding()`: Generates embeddings for input sentences
- `main()`: Orchestrates the attack process, including:
  - Loading the specified model and tokenizer
  - Implementing different decoding strategies (greedy, default, temperature tuning, top-p tuning, top-k tuning)
  - Generating outputs for each strategy and saving results

### 2.2 evaluate.py

This file evaluates the effectiveness of the attack by analyzing the generated outputs.

Key functions and features:
- `not_matched()`: Checks if the output contains any predefined refusal prefixes
- `get_eval_scores()`: Calculates evaluation scores based on the classifier and matching criteria
- `pick_best_and_eval()`: Selects the best outputs and evaluates them
- `run()`: Main function that processes results for different decoding strategies and compiles summary statistics

### 2.3 configs.py

This file contains a dictionary mapping model names to their respective paths.

## 3. Relation to Research Paper Concepts

The code implements the key ideas presented in the paper:

1. **Generation Exploitation Attack**: The `attack.py` file implements various decoding strategies mentioned in the paper, including:
   - Greedy decoding
   - Default settings (temperature = 0.1, top_p = 0.9)
   - Temperature tuning
   - Top-p tuning
   - Top-k tuning

2. **Evaluation**: The `evaluate.py` file implements the evaluation process described in the paper, including:
   - Using a classifier-based evaluator
   - Checking for refusal prefixes
   - Calculating misalignment rates

3. **Model Coverage**: The code supports multiple open-source LLMs as mentioned in the paper, including LLaMA2, Falcon, and MPT models.

4. **Benchmarks**: The code uses both AdvBench and MaliciousInstruct datasets for evaluation, as described in the paper.

## 4. Notable Algorithms and Techniques

1. **Decoding Strategy Manipulation**: The code systematically varies decoding parameters (temperature, top-p, top-k) to find configurations that lead to misaligned outputs.

2. **Embedding-based Generation**: For some models, the code uses sentence embeddings as input for generation, allowing for more flexible manipulation of the input.

3. **Two-stage Evaluation**: The evaluation process uses both a classifier-based approach and a simple string matching technique to identify misaligned outputs.

4. **Sampling-based Generation**: The code implements multi-sample generation for non-greedy decoding strategies, allowing for exploration of output variations.

# Potential Enhancements

1. **Adaptive Attack Strategies**
   - Implement an adaptive algorithm that dynamically adjusts decoding parameters based on initial results
   - This could improve the efficiency of finding vulnerable configurations and potentially discover more effective attack strategies

2. **Cross-model Transfer Learning**
   - Explore the transferability of successful attack configurations between different model architectures
   - Implement a meta-learning approach to quickly adapt attack strategies to new models

3. **Robustness Analysis Framework**
   - Develop a comprehensive framework for analyzing model robustness against various attack strategies
   - Include metrics for measuring the severity and consistency of misalignments across different prompts and decoding settings

4. **Multi-modal Extension**
   - Extend the attack and evaluation framework to handle multi-modal models (e.g., models that process both text and images)
   - Investigate how generation exploitation attacks might manifest in multi-modal contexts

5. **Automated Repair and Patching**
   - Implement an automated system for identifying and patching vulnerabilities discovered by the attack
   - Explore techniques like adaptive fine-tuning or dynamic prompt engineering to improve model robustness on-the-fly

These enhancements would address limitations mentioned in the paper, extend the functionality to new domains, and incorporate recent advancements in AI safety and robustness research.