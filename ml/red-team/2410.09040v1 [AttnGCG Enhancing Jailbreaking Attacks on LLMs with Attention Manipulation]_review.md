#Tags
[[Research/Research Papers/2410.09040v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection

**Title:** AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation
**Authors:** Zijun Wang, Haoqin Tu, Jieru Mei, Bingchen Zhao, Yisen Wang, Cihang Xie
**Publication Date:** October 11, 2024

Summary:
This paper introduces AttnGCG, an enhanced method for jailbreaking attacks on Large Language Models (LLMs) by manipulating attention scores. The authors build upon the Greedy Coordinate Gradient (GCG) strategy, incorporating attention manipulation to improve attack efficacy across various LLMs.

Key Contributions:
- Introduction of AttnGCG, an attention-based enhancement to GCG for jailbreaking LLMs
- Demonstration of consistent improvements in attack efficacy across diverse LLMs
- Robust attack transferability against unseen harmful goals and black-box LLMs
- More interpretable attention-score visualization for insights into effective jailbreaking

Problem Statement:
The research addresses the vulnerability of transformer-based LLMs to jailbreaking attacks, specifically focusing on improving the optimization-based GCG strategy through attention manipulation.

Methodology:
1. Observation of correlation between attack effectiveness and internal model behaviors
2. Development of AttnGCG, incorporating attention loss into the GCG objective
3. Evaluation on various LLMs, including Llama-2, Gemma, Mistral, and Mixtral series
4. Transfer attack experiments across different goals and black-box models
5. Visualization and analysis of attention scores

Main Results:
1. AttnGCG consistently outperforms GCG across various models:
   - Average improvement of 6.3% in ASR_GPT and 3.9% in ASR_KW
   - 7% increase in ASR for Llama-2 series
   - 10% increase in ASR for Gemma series
2. Enhanced transferability:
   - 11.4% improvement in ASR for unseen harmful goals
   - 2.8% improvement in ASR for black-box LLMs like GPT-3.5 and GPT-4
3. Reduction in 'false jailbreak' instances, particularly in Gemma models

Qualitative Analysis:
- Attention score manipulation is key to successful jailbreaking
- Shifting attention from the goal and system prompts to the adversarial suffix increases attack success
- Visualization of attention heatmaps provides insights into the jailbreaking process

Limitations:
- Limited effectiveness on the latest closed-weight models (e.g., GPT-4o, Gemini-1.5-Flash)
- Potential ethical concerns regarding the development of more effective jailbreaking techniques

Conclusion:
AttnGCG demonstrates significant improvements in jailbreaking LLMs by manipulating attention scores. The method shows consistent enhancements across various models and exhibits strong transferability. The attention-based approach also provides more interpretable insights into the jailbreaking process.

Future Work:
- Further research on defending against attention-based jailbreaking attacks
- Exploration of AttnGCG's applicability to other jailbreaking frameworks
- Investigation of methods to improve transferability to the latest closed-weight models

Tools Introduced:
- AttnGCG: An enhanced jailbreaking method incorporating attention manipulation
- GitHub repository: https://github.com/UCSC-VLAA/AttnGCG-attack

## Repository Token Information
Total tokens in repository: 28378

Tokens per file:
- AttnGCG/base/attack_manager.py: 15011 tokens
- AttnGCG/attngcg/attngcg_attack.py: 2444 tokens
- eval/gene_response/eval_hf.py: 1399 tokens
- eval/gpt4_judge/gpt_classify.py: 915 tokens
- attack_closed_model/attack_gemini_pro/attack_gemini.py: 861 tokens
- experiments/main.py: 733 tokens
- data/sample_train_data.py: 689 tokens
- attack_closed_model/attack_gpt35_0125/attack_gpt.py: 590 tokens
- attack_closed_model/attack_gpt35_0613/attack_gpt.py: 590 tokens
- attack_closed_model/attack_gpt35_instruct/attack_gpt.py: 590 tokens
- attack_closed_model/attack_gpt35_1106/attack_gpt.py: 590 tokens
- attack_closed_model/attack_gpt4_1106/attack_gpt.py: 588 tokens
- eval/keyword_detection/keyword_detection.py: 584 tokens
- experiments/configs/template.py: 457 tokens
- eval/transfer_across_goals/transfer_suffix.py: 415 tokens
- setup.py: 249 tokens
- experiments/configs/transfer_llama3_8b_it.py: 144 tokens
- experiments/configs/individual_llama3_8b_it.py: 127 tokens
- experiments/configs/transfer_mixtral_8x7b.py: 121 tokens
- experiments/configs/transfer_mistral_7b.py: 115 tokens
- experiments/configs/transfer_llama2_chat_13b.py: 115 tokens
- experiments/configs/transfer_llama2_chat_7b.py: 115 tokens
- experiments/configs/transfer_gemma_2b.py: 114 tokens
- experiments/configs/transfer_gemma_7b.py: 114 tokens
- experiments/configs/individual_llama2_chat_13b.py: 107 tokens
- experiments/configs/individual_mixtral_8x7b.py: 103 tokens
- experiments/configs/individual_llama2_chat_7b.py: 98 tokens
- experiments/configs/individual_mistral_7b.py: 97 tokens
- experiments/configs/individual_gemma_2b.py: 81 tokens
- experiments/configs/individual_gemma_7b.py: 81 tokens
- AttnGCG/__init__.py: 74 tokens
- AttnGCG/attngcg/__init__.py: 67 tokens
- experiments/__init__.py: 0 tokens
- AttnGCG/base/__init__.py: 0 tokens


## Tutorial and Enhancement Suggestions

# AttnGCG Tutorial

## Project Overview

AttnGCG is a novel approach to jailbreaking attacks on Large Language Models (LLMs) that enhances the Greedy Coordinate Gradient (GCG) method by incorporating attention manipulation. The project aims to improve attack efficacy and transferability across various LLMs.

### Project Structure

The repository is organized as follows:

- `AttnGCG/`: Main package
  - `base/`: Base classes and utilities
  - `attngcg/`: Implementation of AttnGCG attack
- `experiments/`: Experiment configurations and main script
- `eval/`: Evaluation scripts
- `attack_closed_model/`: Scripts for attacking closed-source models
- `data/`: Data processing scripts

## Key Components

### 1. Attack Manager (`AttnGCG/base/attack_manager.py`)

This file contains the core classes for managing the attack:

- `AttackPrompt`: Represents a single attack prompt
- `PromptManager`: Manages multiple attack prompts
- `MultiPromptAttack`: Coordinates attacks across multiple prompts and models
- `IndividualPromptAttack` and `ProgressiveMultiPromptAttack`: Variations of the attack strategy

Key functions:
- `token_gradients`: Computes gradients for token optimization
- `get_embedding_matrix` and `get_embeddings`: Utility functions for working with model embeddings

### 2. AttnGCG Attack (`AttnGCG/attngcg/attngcg_attack.py`)

This file implements the AttnGCG-specific attack logic:

- `AttnGCGAttackPrompt`: Extends `AttackPrompt` with attention manipulation
- `AttnGCGPromptManager`: Manages AttnGCG-specific prompt operations
- `AttnGCGMultiPromptAttack`: Coordinates AttnGCG attacks

Key methods:
- `grad`: Computes gradients incorporating attention loss
- `attention_loss`: Calculates the attention-based loss

### 3. Main Experiment Script (`experiments/main.py`)

This script orchestrates the entire attack process:

- Loads configuration
- Sets up workers and attack managers
- Runs the attack loop
- Logs results

### 4. Evaluation Scripts

- `eval/gene_response/eval_hf.py`: Generates responses using Hugging Face models
- `eval/gpt4_judge/gpt_classify.py`: Uses GPT-4 to classify attack success
- `eval/keyword_detection/keyword_detection.py`: Detects keywords in generated responses

## Key Concepts and Implementations

### Attention Manipulation

The core innovation of AttnGCG is the manipulation of attention scores. This is implemented in the `attention_loss` method of `AttnGCGAttackPrompt`:

```python
def attention_loss(self, attentions, offset=0, attention_pooling_method=None, attention_weight_dict=None):
    # ... (code to calculate attention-based loss)
```

This function calculates a loss based on the attention scores, encouraging the model to focus on specific parts of the input (e.g., the adversarial suffix) while ignoring others (e.g., the system prompt).

### Gradient-based Optimization

The attack uses gradient-based optimization to find effective adversarial suffixes. This is implemented in the `step` method of `AttnGCGMultiPromptAttack`:

```python
def step(self, batch_size=1024, topk=256, temp=1, allow_non_ascii=True, ...):
    # ... (code for optimization step)
```

This method performs one step of the optimization process, using gradients to update the control tokens (adversarial suffix).

### Transfer Attacks

The project implements transfer attacks to test the generalization of the method. This is handled by the `ProgressiveMultiPromptAttack` class, which allows for attacking multiple goals and models progressively.

## Notable Algorithms

1. **Greedy Coordinate Gradient (GCG)**: The base algorithm, enhanced with attention manipulation.
2. **Attention pooling**: Methods like 'mean' and 'sum' are used to aggregate attention scores.
3. **Token sampling**: The `sample_control` method in `AttnGCGPromptManager` implements token sampling based on gradients.

# Potential Enhancements

1. **Adaptive Attention Weighting**
   - Implement a dynamic scheme to adjust attention weights during the attack
   - This could improve attack efficiency and effectiveness across different models

2. **Multi-model Ensemble Attacks**
   - Extend the framework to simultaneously attack multiple models
   - Use ensemble gradients to find more robust and transferable adversarial suffixes

3. **Reinforcement Learning Integration**
   - Incorporate RL techniques to learn attack strategies over time
   - This could lead to more adaptive and sophisticated attacks

4. **Defense Mechanism Exploration**
   - Implement and evaluate potential defenses against AttnGCG attacks
   - This could involve attention score regularization or detection mechanisms

5. **Scalability Improvements**
   - Optimize the code for distributed computing environments
   - Implement more efficient gradient computation and token sampling methods for larger models