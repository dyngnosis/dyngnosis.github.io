#Tags
[[Research/Research Papers/2305.14965v4.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0056/LLMMetaPromptExtraction
#AMLT0057/LLMDataLeakage
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData

**Title:** Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks
**Authors:** Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, Monojit Choudhury
**Publication Date:** May 24, 2023 (updated March 27, 2024)

Key Contributions:
- Proposes a formalism and taxonomy for known and possible jailbreaks on LLMs
- Surveys existing jailbreak methods and their effectiveness on various LLMs
- Discusses challenges in jailbreak detection
- Releases a dataset of model outputs across 3700 jailbreak prompts over 4 tasks

Problem Statement:
The paper addresses the vulnerability of Large Language Models (LLMs) to jailbreaks or prompt injection attacks, where users can manipulate prompts to cause misalignment, privacy breaches, and offensive outputs.

Methodology:
1. Formalization of jailbreaks and prompt injection attacks
2. Development of a taxonomy for jailbreak techniques and intents
3. Survey of existing jailbreak methods
4. Evaluation of jailbreak effectiveness on various LLMs (GPT-based, OPT, BLOOM, FLAN-T5-XXL)
5. Analysis of jailbreak detection challenges
6. Creation of a dataset with 3700 jailbreak prompts across 4 tasks

Main Results and Findings:
1. Jailbreak techniques are categorized into orthographic, lexical, morpho-syntactic, semantic, and pragmatic types
2. Jailbreak intents are classified as information leakage, misaligned content generation, and performance degradation
3. Cognitive hacking (COG) appears to be the most successful form of jailbreak
4. Instruction-tuned models (e.g., GPT-3.5-turbo) show increased vulnerability to jailbreaks
5. Performance degradation intent has the highest success rate across most models
6. Jailbreak detection is challenging due to the vast space of possible outputs and instruction-following capabilities of LLMs

Qualitative Analysis:
- The study reveals a "jailbreak paradox" where it becomes increasingly difficult to detect and mitigate jailbreaks due to the vast output space and instruction-following capabilities of LLMs
- The effectiveness of jailbreaks varies across different model sizes and training paradigms, suggesting a complex relationship between model capabilities and vulnerabilities
- The paper highlights the need for more robust evaluation metrics for attack success, as current methods like ASR and GPT-4 based evaluations show limitations

Limitations:
- The study focuses primarily on English language jailbreaks
- Manual annotations may introduce bias due to annotators' backgrounds in NLP
- The instruction-tuned models evaluated are mainly from OpenAI, which may limit generalizability

Conclusion and Future Work:
- The paper provides a comprehensive framework for understanding and addressing jailbreaks in LLMs
- Future work should focus on developing more robust detection and mitigation strategies
- There is a need for large-scale studies on user-level understanding of misalignment and explaining model misalignment to end-users

Tools Introduced:
- A dataset of model outputs across 3700 jailbreak prompts over 4 tasks (available at https://github.com/AetherPrior/TrickLLM)

Relevant Figures:
- Figure 1: Jailbreaking pipeline example
- Figure 2: Property-test results for all models with respect to jailbreak type
- Figure 3: Property-test results for all models with respect to jailbreak intent
- Figure 6: Jailbreak success over different tasks

## Repository Token Information
Total tokens in repository: 40966

Tokens per file:
- requirements.txt: 40 tokens
- environment.yml: 3562 tokens
- README.md: 1979 tokens
- Observations/Batch&Checksum Prompt Guard/README.md: 605 tokens
- Observations/Information Leakage/README.md: 1 tokens
- outputs/processed/README.md: 72 tokens
- src/datautils.py: 2225 tokens
- src/code_generation_collection.py: 447 tokens
- src/__init__.py: 13 tokens
- src/main.py: 425 tokens
- src/eval/__init__.py: 41 tokens
- src/eval/attackmetrics/confusion_matrix.py: 2245 tokens
- src/eval/attackmetrics/GPT4_test.py: 943 tokens
- src/eval/attackmetrics/get_prop_test_stats.py: 3317 tokens
- src/eval/attackmetrics/__init__.py: 24 tokens
- src/eval/attackmetrics/GPT4_test_analysis.py: 1853 tokens
- src/eval/attackmetrics/intent_test_results/get_attack_outputs_with_intents.py: 509 tokens
- src/eval/attackmetrics/intent_test_results/intent_test_stats.py: 2555 tokens
- src/eval/attackmetrics/intent_test_results/intent_to_prop.py: 606 tokens
- src/eval/attackmetrics/propertyprompts/summarize.md: 1790 tokens
- src/eval/attackmetrics/propertyprompts/MT.md: 894 tokens
- src/eval/attackmetrics/propertyprompts/codegenerate.md: 975 tokens
- src/eval/attackmetrics/propertyprompts/classify.md: 842 tokens
- src/eval/attackmetrics/manual/manual_sample.py: 668 tokens
- src/eval/attackmetrics/manual/map_manual.py: 729 tokens
- src/eval/attackmetrics/manual/V2/getgpt4.py: 947 tokens
- src/eval/attackmetrics/DL_outputs/README.md: 77 tokens
- src/t-sne/flan_embeddings_generator.py: 544 tokens
- src/t-sne/temps/attack_success_failure.py: 257 tokens
- src/model/flan.py: 644 tokens
- src/model/bloom.py: 718 tokens
- src/model/base.py: 561 tokens
- src/model/opt.py: 551 tokens
- src/model/__init__.py: 0 tokens
- src/model/openai.py: 884 tokens
- Scraping/requirements.txt: 13 tokens
- Scraping/YoutubeScraping/analyse_youtube.py: 1948 tokens
- Scraping/YoutubeScraping/config.py: 62 tokens
- Scraping/YoutubeScraping/video_comments.py: 1790 tokens
- Scraping/YoutubeScraping/main.py: 208 tokens
- Scraping/YoutubeScraping/output/README.md: 293 tokens
- Scraping/YoutubeScraping/utils/helper.py: 33 tokens
- Scraping/RedditScraping/scrape_posts_only.py: 570 tokens
- Scraping/RedditScraping/scrape_submissions.py: 810 tokens
- Scraping/RedditScraping/secrets.json: 64 tokens
- Scraping/RedditScraping/analyze_posts.py: 2041 tokens
- Scraping/RedditScraping/scrape_subreddit.py: 591 tokens


## Tutorial and Enhancement Suggestions

# TrickLLM Tutorial

## Project Overview

TrickLLM is a research project aimed at formalizing, analyzing, and detecting jailbreaks in Large Language Models (LLMs). The repository contains code for evaluating various jailbreak techniques across different LLMs and tasks, as well as analyzing the results.

## Project Structure

The project is organized into several key directories:

- `src/`: Contains the main source code
- `Scraping/`: Scripts for data collection from Reddit and YouTube
- `attacks/`: CSV files with attack prompts for different tasks
- `outputs/`: Stores model outputs and processed results
- `Observations/`: Additional notes and observations
- `plots/`: Visualization outputs

## Key Components

### 1. Data Preparation (`src/datautils.py`)

The `FinalPromptBuilder` class is responsible for constructing prompts by combining base prompts, attack prompts, and user inputs. It reads data from various CSV files and an Excel sheet to create a comprehensive set of prompts for evaluation.

```python
class FinalPromptBuilder:
    def __init__(self, base_prompts_path: str=BASE_PROMPTS_PATH, 
                 attack_prompts_dir: str=ATTACKS_FOLDER_DIR,
                 inputs_dir: str=INPUTS_DIR):
        # Initialize and load data
        ...

    def get_final_list_of_prompts(self, model_name: str) -> List[dict]:
        # Generate final prompts for a given model
        ...
```

### 2. Model Inferencing (`src/model/`)

The project implements inferencing code for various LLMs:

- FLAN-T5 (`flan.py`)
- BLOOM (`bloom.py`)
- OPT (`opt.py`)
- GPT models (`openai.py`)

Each model has a dedicated class (e.g., `FlanInferencer`, `BLOOMInferencer`) that handles tokenization, generation, and output processing.

### 3. Evaluation (`src/eval/attackmetrics/`)

The evaluation pipeline consists of several components:

- `get_prop_test_stats.py`: Implements property tests to evaluate attack success
- `GPT4_test.py` and `GPT4_test_analysis.py`: Use GPT-4 to assess attack success
- `confusion_matrix.py`: Generates confusion matrices for comparing different evaluation methods

### 4. Visualization (`src/t-sne/`)

The project includes code for generating t-SNE visualizations of embeddings, which can be useful for analyzing the distribution of successful and failed attacks in the embedding space.

### 5. Data Collection (`Scraping/`)

Scripts for collecting data from Reddit and YouTube are provided, allowing for the expansion of the dataset with real-world examples of jailbreak attempts.

## Key Concepts and Implementations

### Jailbreak Taxonomy

The code implements the jailbreak taxonomy described in the paper through the organization of attack prompts in the `attacks/` directory. Each CSV file corresponds to a specific task (e.g., code generation, hate speech detection) and contains various attack prompts categorized by type and intent.

### Attack Success Evaluation

The project uses two main methods to evaluate attack success:

1. Property Tests: Implemented in `get_prop_test_stats.py`, these tests check if the model output violates expected properties for each task.

2. GPT-4 Based Evaluation: `GPT4_test.py` and `GPT4_test_analysis.py` use GPT-4 to assess whether an attack was successful based on the model's output.

### Cross-Model Comparison

The project allows for comparing jailbreak effectiveness across different models by running the same set of prompts through various LLMs and analyzing the results.

## Running the Code

To evaluate a model:

1. Set up the environment using `environment.yml` or `requirements.txt`
2. Prepare the attack prompts and user inputs in the appropriate directories
3. Run the main script for the desired model, e.g.:

```bash
python -m src.model.flan
```

4. Analyze the results using the evaluation scripts in `src/eval/attackmetrics/`

# Potential Enhancements

1. Multi-lingual Jailbreak Analysis
   - Extend the project to analyze jailbreaks in languages other than English
   - Implement language-specific property tests and evaluation metrics
   - Compare jailbreak effectiveness across languages and cultural contexts

2. Advanced Detection Techniques
   - Implement machine learning models for jailbreak detection
   - Explore anomaly detection techniques to identify potential jailbreaks
   - Develop a real-time jailbreak detection system that can be integrated with LLM APIs

3. Adaptive Jailbreak Generation
   - Create a system that can automatically generate and test new jailbreak techniques
   - Implement reinforcement learning algorithms to optimize jailbreak prompts
   - Develop a framework for continuous evaluation of LLM robustness against evolving jailbreak techniques

4. Human-in-the-Loop Evaluation
   - Integrate a human evaluation component to complement automated metrics
   - Develop a user interface for manual annotation and analysis of model outputs
   - Implement active learning techniques to efficiently use human feedback for improving detection models

5. Mitigation Strategies Evaluation
   - Extend the codebase to implement and evaluate various jailbreak mitigation techniques
   - Develop a benchmark for comparing the effectiveness of different mitigation strategies
   - Explore the trade-offs between model performance and robustness against jailbreaks

These enhancements would address limitations mentioned in the paper, extend the project's functionality, and incorporate recent advancements in the field of LLM security and robustness.