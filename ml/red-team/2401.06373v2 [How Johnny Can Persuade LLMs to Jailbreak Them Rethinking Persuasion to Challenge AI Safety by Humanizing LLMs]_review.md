#Tags
[[Research/Research Papers/2401.06373v2.pdf]]

#AMLT0051/LLMPromptInjection
#AMLT0054/LLMJailbreak
#AMLT0057/LLMDataLeakage
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData

**Title:** How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs
**Authors:** Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, Weiyan Shi
**Publication Date:** January 12, 2024

Summary:
This paper introduces a novel approach to jailbreaking large language models (LLMs) by treating them as human-like communicators susceptible to persuasion techniques. The authors develop a persuasion taxonomy based on social science research and use it to generate persuasive adversarial prompts (PAPs) that effectively jailbreak LLMs across various risk categories.

Key Contributions:
- Development of a persuasion taxonomy for LLM jailbreaking
- Creation of a method to generate interpretable persuasive adversarial prompts (PAPs)
- Demonstration of high jailbreak success rates across multiple LLMs
- Analysis of existing defenses against PAPs and proposal of new adaptive defenses

Problem Statement:
Traditional AI safety research focuses on algorithm-based attacks, overlooking the risks posed by everyday users interacting with LLMs through natural language. This paper addresses the gap by exploring how persuasion techniques can be used to jailbreak LLMs.

Methodology:
1. Created a persuasion taxonomy with 40 techniques across 13 strategies
2. Developed a Persuasive Paraphraser to generate PAPs based on the taxonomy
3. Conducted broad scan experiments across 14 risk categories
4. Performed in-depth iterative probing on multiple LLMs
5. Evaluated existing defenses and proposed new adaptive defenses

Main Results:
- PAPs achieved over 92% attack success rate on Llama-2 7b Chat, GPT-3.5, and GPT-4
- PAPs outperformed existing algorithm-focused jailbreak methods
- More advanced models (e.g., GPT-4) showed higher susceptibility to PAPs than less advanced models
- Existing defenses were found to be inadequate against PAPs

Qualitative Analysis:
- The study reveals that LLMs are vulnerable to human-like persuasion techniques, highlighting a significant gap in current AI safety measures
- The effectiveness of PAPs across different risk categories and LLMs suggests that the problem is widespread and not limited to specific models or domains
- The research demonstrates that everyday users without technical expertise could potentially pose security risks to LLMs through natural language interactions

Limitations:
- The study primarily focuses on single-turn persuasive attempts, while real-world scenarios may involve multi-turn interactions
- The research does not extensively explore the potential real-world harm of jailbroken LLM outputs
- The study does not fully address the ethical implications of using persuasion techniques for jailbreaking

Conclusion and Future Work:
The paper concludes that persuasion-based jailbreaking poses a significant risk to LLM safety and calls for more fundamental solutions to address these vulnerabilities. Future work suggestions include:
- Investigating multi-turn persuasive conversations
- Exploring the connection between persuasion and other jailbreak methods
- Developing more robust defenses against persuasion-based attacks
- Studying the cognitive and anthropological aspects of how advanced models interact with persuasion

Tools Introduced:
- Persuasive Paraphraser: A fine-tuned LLM for generating PAPs based on the persuasion taxonomy
- Adaptive defenses: 
  1. Adaptive System Prompt
  2. Base Summarizer
  3. Tuned Summarizer

(No GitHub repositories were mentioned in the paper)