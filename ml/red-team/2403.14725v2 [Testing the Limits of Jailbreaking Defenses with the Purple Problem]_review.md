#Tags
[[Research/Research Papers/2403.14725v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection

**Title:** Testing the Limits of Jailbreaking Defenses with the Purple Problem
**Authors:** Taeyoun Kim, Suhas Kotha, Aditi Raghunathan
**Affiliations:** Carnegie Mellon University
**Publication Date:** March 20, 2024 (preprint)

Key Contributions:
- Introduces the "Purple Problem" as a simple, well-specified definition for testing jailbreaking defenses
- Demonstrates that existing fine-tuning and input defenses fail on this simple problem
- Highlights the importance of separating definition quality from enforcement quality in jailbreaking defenses
- Shows that current safety benchmarks primarily test enforcement rather than definition quality

Problem Statement:
The paper addresses the challenge of creating robust defenses against jailbreaking attacks on language models, focusing on the efficacy of existing enforcement mechanisms for preventing undesirable outputs.

Methodology:
1. Conceptual decomposition of jailbreaking defenses into definition and enforcement stages
2. Introduction of the "Purple Problem": preventing a language model from generating the string "purple"
3. Testing various defense strategies:
   - Fine-tuning via DPO and PPO
   - Adversarial training
   - Input preprocessing (paraphrasing, in-context learning, perplexity-based filtering)
4. Evaluation of real-world safety benchmarks

Main Results:
1. Fine-tuning defenses:
   - Achieve 100% Defense Success Rate (DSR) on natural prompts
   - Fail against adversarial suffixes (DSR drops to 0-3.5%)
   - Adversarial training improves robustness but still vulnerable to adaptive attacks

2. Input preprocessing defenses:
   - Paraphrasing: Initially effective but fails against paraphrase-aware attacks
   - In-context learning: Vulnerable to adaptive suffix optimization
   - Perplexity-based filtering: Bypassed by prepending low-perplexity text

3. Real-world safety benchmarks:
   - Simple output filtering achieves perfect safety scores without utility loss
   - Highlights the focus on enforcement rather than definition quality in current benchmarks

Qualitative Analysis:
- The paper demonstrates that even for a simple, well-specified safety definition, current defenses are inadequate against adaptive attacks
- The results suggest that improving enforcement alone may not be sufficient for robust jailbreaking defenses
- The study highlights the need for better definitions of safety and more comprehensive evaluation methods

Limitations:
- Focus on a single, simple definition (the Purple Problem) may not fully represent the complexity of real-world safety concerns
- The study primarily uses smaller language models (7B parameters) and may not fully capture the behavior of larger, more advanced models

Conclusion and Future Work:
- Current jailbreaking defenses are vulnerable to adaptive attacks, even for simple safety definitions
- Future research should focus on:
  1. Developing more robust enforcement mechanisms
  2. Improving the quality of safety definitions
  3. Creating evaluation methods that test both definition and enforcement quality

Relevant Figures:
- Figure 1: Define and Enforce Framework
- Figure 2: Enforcement Strategies for Purple Problem

New Tools:
The paper introduces the "Purple Problem" as a simple benchmark for testing jailbreaking defenses. Code is available at https://github.com/kothasuhas/purple-problem

## Repository Token Information
Total tokens in repository: 31363

Tokens per file:
- evaluate.py: 2624 tokens
- train_dpo.py: 1672 tokens
- create_dataset.py: 1152 tokens
- llm-attacks-clone/setup.py: 321 tokens
- llm-attacks-clone/llm_attacks/__init__.py: 78 tokens
- llm-attacks-clone/llm_attacks/base/__init__.py: 0 tokens
- llm-attacks-clone/llm_attacks/base/attack_manager.py: 14152 tokens
- llm-attacks-clone/llm_attacks/minimal_gcg/string_utils.py: 1295 tokens
- llm-attacks-clone/llm_attacks/minimal_gcg/opt_utils.py: 1731 tokens
- llm-attacks-clone/llm_attacks/minimal_gcg/__init__.py: 0 tokens
- llm-attacks-clone/llm_attacks/gcg/__init__.py: 64 tokens
- llm-attacks-clone/llm_attacks/gcg/gcg_attack.py: 1628 tokens
- llm-attacks-clone/experiments/evaluate.py: 1168 tokens
- llm-attacks-clone/experiments/evaluate_individual.py: 917 tokens
- llm-attacks-clone/experiments/__init__.py: 0 tokens
- llm-attacks-clone/experiments/main.py: 814 tokens
- llm-attacks-clone/experiments/launch_scripts/submit_multiple.py: 351 tokens
- llm-attacks-clone/experiments/configs/individual_vicuna.py: 33 tokens
- llm-attacks-clone/experiments/configs/individual_llama2.py: 112 tokens
- llm-attacks-clone/experiments/configs/transfer_general.py: 167 tokens
- llm-attacks-clone/experiments/configs/template.py: 390 tokens
- llm-attacks-clone/experiments/configs/__init__.py: 0 tokens
- llm-attacks-clone/experiments/configs/transfer_vicuna.py: 174 tokens
- llm-attacks-clone/experiments/configs/transfer_vicuna_guanaco.py: 349 tokens
- llm-attacks-clone/experiments/configs/transfer_llama2.py: 169 tokens
- llm-attacks-clone/api_experiments/evaluate_api_models.py: 2002 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: The Purple Problem Repository

## 1. Project Overview

The Purple Problem repository implements the concepts and experiments described in the paper "Testing the Limits of Jailbreaking Defenses with the Purple Problem". It focuses on evaluating the effectiveness of various jailbreaking defenses for language models, specifically targeting the simple task of preventing a model from outputting the word "purple".

### Project Structure

The repository is organized into several key directories:

- Root directory: Contains main scripts for evaluation, training, and dataset creation
- `llm-attacks-clone/`: A modified version of the llm-attacks library
- `experiments/`: Configuration files and scripts for running experiments
- `api_experiments/`: Scripts for evaluating API-based models

## 2. Key Components

### 2.1 Main Scripts

#### evaluate.py

This script is responsible for evaluating the effectiveness of jailbreaking defenses. Key functions include:

- `get_embedding_layer()`, `get_embedding_matrix()`, `get_embeddings()`: Utility functions for working with model embeddings
- `AttackPrompt`: Class representing an attack prompt, including methods for generating and testing outputs
- `PromptManager`: Manages multiple attack prompts
- `MultiPromptAttack`: Coordinates attacks using multiple prompts
- `EvaluateAttack`: Evaluates the success of attacks across different models and prompts

#### train_dpo.py

This script implements the Direct Preference Optimization (DPO) training method for fine-tuning language models. Notable components:

- `dpo_map_jailbreak()`: Prepares data for DPO training
- `train_dpo()`: Main function for DPO training
- `get_model()`, `get_dataset()`: Utility functions for loading models and datasets

#### create_dataset.py

This script generates datasets for training and testing jailbreaking defenses. Key functions:

- `get_prompts()`: Generates diverse prompts using GPT-3.5-turbo
- `create_responses()`: Creates responses for each prompt
- `create_dataset()`: Combines prompts and responses into a dataset

### 2.2 llm-attacks-clone

This directory contains a modified version of the llm-attacks library, adapted for the Purple Problem. Key components include:

- `base/attack_manager.py`: Implements core attack logic
- `gcg/gcg_attack.py`: Implements the Greedy Coordinate Gradient (GCG) attack method
- `minimal_gcg/`: Contains utilities for a minimal implementation of GCG

### 2.3 Experiments

The `experiments/` directory contains configuration files and scripts for running various experiments:

- `configs/`: Configuration files for different experimental setups
- `main.py`: Main script for running experiments
- `evaluate.py` and `evaluate_individual.py`: Scripts for evaluating experiment results

## 3. Relation to Research Paper Concepts

The code implements several key concepts discussed in the paper:

1. **Purple Problem Definition**: The repository focuses on the task of preventing a model from outputting "purple", as defined in the paper.

2. **Defense Strategies**: 
   - Fine-tuning defenses (DPO, PPO) are implemented in `train_dpo.py`
   - Input preprocessing defenses (paraphrasing, in-context learning, perplexity-based filtering) are implemented in various parts of the codebase

3. **Attack Methods**: 
   - The GCG attack method is implemented in `gcg_attack.py`
   - Adversarial suffix generation is part of the attack implementation

4. **Evaluation**: 
   - The `EvaluateAttack` class in `evaluate.py` implements the evaluation methodology described in the paper
   - Defense Success Rate (DSR) calculation is part of the evaluation process

## 4. Notable Algorithms and Techniques

1. **Greedy Coordinate Gradient (GCG) Attack**: Implemented in `gcg_attack.py`, this method iteratively optimizes adversarial suffixes to bypass defenses.

2. **Direct Preference Optimization (DPO)**: Implemented in `train_dpo.py`, this fine-tuning method aims to improve model behavior based on preferred outputs.

3. **Token Gradient Calculation**: The `token_gradients()` function in `gcg_attack.py` efficiently computes gradients with respect to input tokens.

4. **Progressive Multi-Prompt Attack**: Implemented in `attack_manager.py`, this technique progressively increases the complexity of attacks.

# Potential Enhancements

1. **Scaling to Larger Models**: 
   - Implement efficient methods for applying the Purple Problem techniques to larger language models (e.g., models with 100B+ parameters)
   - Optimize memory usage and computation to handle increased model sizes

2. **Improved Adversarial Training**:
   - Develop more sophisticated adversarial training techniques that can better anticipate and defend against adaptive attacks
   - Implement a curriculum learning approach for adversarial examples, gradually increasing their complexity during training

3. **Multi-Objective Defense Optimization**:
   - Extend the defense strategies to optimize for multiple objectives simultaneously (e.g., preventing "purple" output while maintaining overall model performance)
   - Implement Pareto optimization techniques to find optimal trade-offs between different defense objectives

4. **Generalization to Complex Safety Definitions**:
   - Extend the Purple Problem framework to handle more complex safety definitions beyond single-word restrictions
   - Develop techniques for automatically generating comprehensive safety definitions from high-level descriptions

5. **Integration with Interpretability Tools**:
   - Incorporate model interpretability techniques to gain insights into why certain defenses succeed or fail
   - Develop visualization tools to help researchers understand the internal dynamics of models during jailbreaking attempts

These enhancements would address limitations mentioned in the paper and push the research forward by improving the robustness, scalability, and generalizability of jailbreaking defenses.