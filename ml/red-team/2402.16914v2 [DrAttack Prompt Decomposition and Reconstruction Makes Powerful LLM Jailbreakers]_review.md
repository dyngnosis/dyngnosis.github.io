#Tags
[[Research/Research Papers/2402.16914v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection

**Title:** DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers
**Authors:** Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh
**Publication Date:** February 25, 2024

Key Contributions:
- Introduction of DrAttack, a novel jailbreaking framework for LLMs
- Demonstration of prompt decomposition and reconstruction to conceal malicious intent
- Significant improvement in attack success rates and query efficiency compared to existing methods
- Effective jailbreaking of both open-source and closed-source LLMs

Problem Statement:
Current jailbreaking methods for LLMs, which use entire harmful prompts, are ineffective at concealing malicious intent and can be easily detected by well-aligned LLMs.

Methodology:
1. Prompt Decomposition:
   - Use semantic parsing to break down malicious prompts into sub-prompts
   - Categorize sub-prompts into [instruction], [structure], [noun], and [verb]

2. Implicit Reconstruction:
   - Utilize In-Context Learning (ICL) with benign demos to guide LLMs in reconstructing the original query
   - Automatically generate semantically similar but harmless demos

3. Synonym Search:
   - Perform level-wise random search on sub-prompts' synonyms
   - Use cosine similarity to maintain faithfulness to original prompts and responses

4. Evaluation:
   - Test on multiple open-source and closed-source LLMs (GPT-3.5, GPT-4, Gemini-Pro, Vicuna, Llama-2)
   - Compare with existing white-box and black-box jailbreaking methods
   - Assess attack success rate (ASR) using string matching, GPT evaluation, and human inspection

Main Results:
- DrAttack achieves significantly higher ASR compared to existing methods:
  - 84.6% on GPT-4 (human evaluation)
  - 62.0% on GPT-4 (LLM evaluation)
- Substantial improvement in query efficiency, requiring only 15 queries on average
- Effective across various LLMs, including closed-source models

Qualitative Analysis:
- Prompt decomposition effectively conceals malicious intent by fragmenting harmful phrases
- ICL reconstruction with benign demos helps bypass LLM safety measures
- The approach maintains faithfulness to original prompts while evading detection

Limitations:
- Potential for misuse by malicious actors
- Effectiveness may vary depending on the specific LLM and its safety measures
- Ethical considerations in developing and sharing jailbreaking techniques

Conclusion and Future Work:
- DrAttack demonstrates a novel vulnerability in LLMs through prompt decomposition and reconstruction
- The study highlights the need for more robust defense mechanisms against advanced jailbreaking techniques
- Future work may focus on developing countermeasures and improving LLM safety alignment

Tools Introduced:
- DrAttack framework (GitHub repository: https://github.com/xirui-li/DrAttack)

Relevant Figures:
1. Figure 1: Illustration of DrAttack framework
2. Figure 2: Taxonomy of prompt-based jailbreak attacks
3. Figure 3: Parsing tree example for attack sentence decomposition
4. Figure 4: ICL demo template example