#Tags
[[Research/Research Papers/2405.07932v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** PARDEN, Can You Repeat That? Defending against Jailbreaks via Repetition
**Authors:** Ziyang Zhang, Qizhen Zhang, Jakob Foerster
**Publication Date:** May 14, 2024

Summary:
This paper introduces PARDEN, a novel defense method against jailbreak attacks on large language models (LLMs). PARDEN uses the LLM itself as a safeguard by asking it to repeat its own outputs, avoiding domain shift issues present in other defense methods. The approach significantly outperforms existing jailbreak detection baselines for Llama-2 and Claude-2.

Key Contributions:
- Introduction of PARDEN, a jailbreak defense method using output repetition
- Demonstration of PARDEN's effectiveness in reducing false positive rates
- Analysis of the method's performance across different LLMs and datasets
- Open-source release of code and datasets for benchmarking

Problem Statement:
How to defend safety-aligned LLMs against jailbreak attacks without introducing domain shift or requiring model fine-tuning?

Methodology:
1. PARDEN Approach:
   - Prompt the LLM to repeat its own output
   - Use BLEU score to measure similarity between original and repeated outputs
   - Classify outputs as harmful if BLEU score falls below a threshold

2. Experimental Setup:
   - Dataset: Combination of benign examples from open-instruct-v1 and harmful examples from AdvBench
   - Models tested: Llama2-7B, Claude-2.1, Mistral-7B
   - Comparison with baseline methods: binary classification and perplexity filters

3. Evaluation Metrics:
   - ROC curves
   - Area Under Curve (AUC)
   - True Positive Rate (TPR) and False Positive Rate (FPR)

Main Results:
1. PARDEN outperforms baseline methods:
   - For Llama2-7B: AUC improved from 0.92 to 0.96
   - At 90% TPR, FPR reduced from 24.8% to 2.0% for harmful behaviors dataset

2. Generalization across models:
   - PARDEN shows consistent improvement across Llama2-7B, Claude-2.1, and Mistral-7B
   - Particularly effective for Claude-2.1, reaching an AUC of 0.9875

3. Robustness to different attack types:
   - Effective against both GCG (high perplexity) and AutoDan (low perplexity) attacks

Qualitative Analysis:
- PARDEN avoids the "auto-regressive trap" by processing the entire output at once
- The method leverages the LLM's inherent safety mechanisms, reducing false positives
- PARDEN's effectiveness stems from its alignment with the self-censoring format used in LLM training

Limitations:
- May not detect harmful intentions in benign-looking outputs (e.g., fake reviews)
- Assumes a safely aligned base model
- Potential for increased computational cost due to repetition process

Conclusion and Future Work:
- PARDEN demonstrates significant improvement in jailbreak defense without requiring model fine-tuning
- The method's success highlights the importance of developing safe base models
- Future work could explore combining PARDEN with input-space filtering methods for comprehensive defense

Tools Introduced:
- PARDEN: Available at https://github.com/Ed-Zh/PARDEN
- Dataset for jailbreak defense evaluation: Available at the same repository

## Repository Token Information
Total tokens in repository: 158663

Tokens per file:
- utils.py: 6880 tokens
- PARDEN_notebook_minimal.html: 151607 tokens
- README.md: 176 tokens
