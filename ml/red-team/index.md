# AI Security Research Hub

Welcome to the AI Security Research Hub. Here you'll find links to our latest research papers.

## Research Papers

| Title | Tags | Last Updated |
|-------|------|---------------|
| [.09177v2  review](./2402.09177v2 [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks]_review.md) |  | 2024-10-15 |
| [.00292v2  review](./2403.00292v2 [Enhancing Jailbreak Attacks with Diversity Guidance]_review.md) |  | 2024-10-15 |
| [.08424v2  review](./2403.08424v2 [Distract Large Language Models for Automatic Jailbreak Attack]_review.md) |  | 2024-10-15 |
| [.17336v2  review](./2403.17336v2 [Don't Listen To Me Understanding and Exploring Jailbreak Prompts of Large Language Models]_review.md) |  | 2024-10-15 |
| [.01833v2  review](./2404.01833v2 [Great, Now Write an Article About That The Crescendo Multi-Turn LLM Jailbreak Attack]_review.md) |  | 2024-10-15 |
| [.02151v3  review](./2404.02151v3 [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks]_review.md) |  | 2024-10-15 |
| [.16369v2  review](./2404.16369v2 [Don't Say No Jailbreaking LLM by Suppressing Refusal]_review.md) |  | 2024-10-15 |
| [.17894v2  review](./2405.17894v2 [White-box Multimodal Jailbreaks Against Large Vision-Language Models]_review.md) |  | 2024-10-15 |
| [.09289v2  review](./2406.09289v2 [Understanding Jailbreak Success A Study of Latent Space Dynamics in Large Language Models]_review.md) |  | 2024-10-15 |
| [.09324v2  review](./2406.09324v2 [Bag of Tricks Benchmarking of Jailbreak Attacks on LLMs]_review.md) |  | 2024-10-15 |
| [.14393v3  review](./2406.14393v3 [Jailbreaking as a Reward Misspecification Problem]_review.md) |  | 2024-10-15 |
| [.18725v2  review](./2406.18725v2 [Jailbreaking LLMs with Arabic Transliteration and Arabizi]_review.md) |  | 2024-10-15 |
| [.00869v2  review](./2407.00869v2 [Large Language Models Are Involuntary Truth-Tellers Exploiting Fallacy Failure for Jailbreak Attacks]_review.md) |  | 2024-10-15 |
| [.21659v3  review](./2407.21659v3 [Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models]_review.md) |  | 2024-10-15 |
| [.14177v2  review](./2409.14177v2 [PathSeeker Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based Jailbreak Approach]_review.md) |  | 2024-10-15 |
| [.14866v2  review](./2409.14866v2 [Effective and Evasive Fuzz Testing-Driven Jailbreaking Attacks against LLMs]_review.md) |  | 2024-10-15 |
| [.16727v1  review](./2409.16727v1 [RoleBreak Character Hallucination as a Jailbreak Attack in Role-Playing Systems]_review.md) |  | 2024-10-15 |
| [.17458v1  review](./2409.17458v1 [RED QUEEN Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking]_review.md) |  | 2024-10-15 |
| [.17699v3  review](./2409.17699v3 [MoJE Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt Attacks]_review.md) |  | 2024-10-15 |
| [.19149v1  review](./2409.19149v1 [Multimodal Pragmatic Jailbreak on Text-to-image Models]_review.md) |  | 2024-10-15 |
| [.01294v1  review](./2410.01294v1 [Endless Jailbreaks with Bijection Learning]_review.md) |  | 2024-10-15 |
| [.02832v1  review](./2410.02832v1 [FlipAttack Jailbreak LLMs via Flipping]_review.md) |  | 2024-10-15 |
| [.01438v1  review](./2410.01438v1 [Information-Theoretical Principled Trade-off between Jailbreakability and Stealthiness on Vision Language Models]_review.md) |  | 2024-10-15 |
| [.02298v2  review](./2410.02298v2 [Jailbreak Antidote Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models]_review.md) |  | 2024-10-15 |
| [.05295v2  review](./2410.05295v2 [AutoDAN-Turbo A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs]_review.md) |  | 2024-10-15 |
| [.03489v1  review](./2410.03489v1 [Gradient-based Jailbreak Images for Multimodal Fusion Models]_review.md) |  | 2024-10-15 |
| [.03857v2  review](./2410.03857v2 [You Know What I'm Saying Jailbreak Attack via Implicit Reference]_review.md) |  | 2024-10-15 |
| [.03869v1  review](./2410.03869v1 [Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step]_review.md) |  | 2024-10-15 |
| [.04234v1  review](./2410.04234v1 [Functional Homotopy Smoothing Discrete Optimization via Continuous Parameters for LLM Jailbreak Attacks]_review.md) |  | 2024-10-15 |
| [.08660v1  review](./2410.08660v1 [RePD Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition Process]_review.md) |  | 2024-10-15 |
| [.09040v1  review](./2410.09040v1 [AttnGCG Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation]_review.md) |  | 2024-10-15 |
| [.09804v1  review](./2410.09804v1 [BlackDAN A Black-Box Multi-Objective Approach for Effective and Contextual Jailbreaking of Large Language Models]_review.md) |  | 2024-10-15 |
| [.10150v1  review](./2410.10150v1 [Jailbreak Instruction-Tuned LLMs via end-of-sentence MLP Re-weighting]_review.md) |  | 2024-10-15 |
| [.04811v2  review](./2408.04811v2 [h4rm3l A Dynamic Benchmark of Composable Jailbreak Attacks for LLM Safety Assessment]_review.md) |  | 2024-09-22 |
| [.07503v1  review](./2409.07503v1 [AdaPPA Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs]_review.md) |  | 2024-09-22 |
| [.08045v1  review](./2409.08045v1 [Unleashing Worms and Extracting Data Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking]_review.md) |  | 2024-09-22 |
| [.11445v1  review](./2409.11445v1 [Jailbreaking Large Language Models with Symbolic Mathematics]_review.md) |  | 2024-09-22 |
| [.07353v1  review](./2409.07353v1 [Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial Attacks]_review.md) |  | 2024-09-11 |
| [.00523v2  review](./2408.00523v2 [Jailbreaking Text-to-Image Models with LLM-Based Agents]_review.md) |  | 2024-09-09 |
| [.05498v2  review](./2406.05498v2 [SelfDefend LLMs Can Defend Themselves against Jailbreaking in a Practical Manner]_review.md) |  | 2024-09-05 |
| [.00137v1  review](./2409.00137v1 [Emerging Vulnerabilities in Frontier Models Multi-Turn Jailbreak Attacks]_review.md) |  | 2024-09-04 |
| [.08464v2  review](./2408.08464v2 [$textit{MMJ-Bench}$ A Comprehensive Study on Jailbreak Attacks and Defenses for Vision Language Models]_review.md) |  | 2024-09-04 |
| [.02928v3  review](./2404.02928v3 [Jailbreaking Prompt Attack A Controllable Adversarial Attack against Diffusion Models]_review.md) |  | 2024-09-04 |
| [.15221v2  review](./2408.15221v2 [LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet]_review.md) |  | 2024-09-04 |
| [.04295v2  review](./2407.04295v2 [Jailbreak Attacks and Defenses Against Large Language Models A Survey]_review.md) |  | 2024-09-01 |
| [.10260v2  review](./2402.10260v2 [A StrongREJECT for Empty Jailbreaks]_review.md) |  | 2024-08-30 |
| [.02534v2  review](./2407.02534v2 [Image-to-Text Logic Jailbreak Your Imagination can Help You Do Anything]_review.md) |  | 2024-08-30 |
| [.16686v2  review](./2407.16686v2 [Can Large Language Models Automatically Jailbreak GPT-4V]_review.md) |  | 2024-08-30 |
| [.17915v3  review](./2407.17915v3 [The Dark Side of Function Calling Pathways to Jailbreaking Large Language Models]_review.md) |  | 2024-08-30 |
| [.10848v2  review](./2408.10848v2 [Perception-guided Jailbreak against Text-to-Image Models]_review.md) |  | 2024-08-30 |
| [.13896v2  review](./2408.13896v2 [RT-Attack Jailbreaking Text-to-Image Models via Random Token]_review.md) |  | 2024-08-30 |
| [.15207v1  review](./2408.15207v1 [Investigating Coverage Criteria in Large Language Models An In-Depth Study Through Jailbreak Attacks]_review.md) |  | 2024-08-30 |
| [.15221v1  review](./2408.15221v1 [LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet]_review.md) |  | 2024-08-30 |
| [.11313v1  review](./2408.11313v1 [Unlocking Adversarial Suffix Optimization Without Affirmative Phrases Efficient Black-box Jailbreaking via LLM as Optimizer]_review.md) |  | 2024-08-23 |
| [.08419v4  review](./2310.08419v4 [Jailbreaking Black Box Large Language Models in Twenty Queries]_review.md) |  | 2024-08-23 |
| [.06255v3  review](./2402.06255v3 [Fight Back Against Jailbreaking via Prompt Adversarial Tuning]_review.md) |  | 2024-08-22 |
| [.20775v2  review](./2405.20775v2 [Medical MLLM is Vulnerable Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models]_review.md) |  | 2024-08-22 |
| [.17915v2  review](./2407.17915v2 [The Dark Side of Function Calling Pathways to Jailbreaking Large Language Models]_review.md) |  | 2024-08-22 |
| [.08924v2  review](./2408.08924v2 [Prefix Guidance A Steering Wheel for Large Language Models to Defend Against Jailbreak Attacks]_review.md) |  | 2024-08-22 |
| [.00869v1  review](./2407.00869v1 [Large Language Models Are Involuntary Truth-Tellers Exploiting Fallacy Failure for Jailbreak Attacks]_review.md) |  | 2024-08-22 |
| [.03805v1  review](./2406.03805v1 [AutoJailbreak Exploring Jailbreak Attacks and Defenses through a Dependency Lens]_review.md) |  | 2024-08-22 |
| [.09321v1  review](./2406.09321v1 [JailbreakEval An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models]_review.md) |  | 2024-08-22 |
| [.15690v1  review](./2402.15690v1 [Foot In The Door Understanding Large Language Model Jailbreaking via Cognitive Psychology]_review.md) |  | 2024-08-22 |
| [.15043v2  review](./2307.15043v2 [Universal and Transferable Adversarial Attacks on Aligned Language Models]_review.md) |  | 2024-08-22 |
| [.20653v2  review](./2405.20653v2 [Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens]_review.md) |  | 2024-08-22 |
| [.10848v1  review](./2408.10848v1 [Perception-guided Jailbreak against Text-to-Image Models]_review.md) |  | 2024-08-22 |
| [.08754v2  review](./2406.08754v2 [Exploiting Uncommon Text-Encoded Structures for Automated Jailbreaks in LLMs]_review.md) |  | 2024-08-22 |
| [.12082v3  review](./2305.12082v3 [SneakyPrompt Jailbreaking Text-to-image Generative Models]_review.md) |  | 2024-08-22 |
| [.08679v2  review](./2402.08679v2 [COLD-Attack Jailbreaking LLMs with Stealthiness and Controllability]_review.md) |  | 2024-08-22 |
| [.20099v1  review](./2405.20099v1 [Defensive Prompt Patch A Robust and Interpretable Defense of LLMs against Jailbreak Attacks]_review.md) |  | 2024-08-22 |
| [.13213v2  review](./2306.13213v2 [Visual Adversarial Examples Jailbreak Aligned Large Language Models]_review.md) |  | 2024-08-22 |
| [.01902v1  review](./2407.01902v1 [SoP Unlock the Power of Social Facilitation for Automatic Jailbreak Attack]_review.md) |  | 2024-08-22 |
| [.08268v4  review](./2311.08268v4 [A Wolf in Sheep's Clothing Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily]_review.md) |  | 2024-08-22 |
| [.17263v4  review](./2401.17263v4 [Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks]_review.md) |  | 2024-08-22 |
| [.18725v1  review](./2406.18725v1 [Jailbreaking LLMs with Arabic Transliteration and Arabizi]_review.md) |  | 2024-08-22 |
| [.04031v2  review](./2406.04031v2 [Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt]_review.md) |  | 2024-08-22 |
| [.11668v1  review](./2406.11668v1 [Not Aligned is Not Malicious Being Careful about Hallucinations of Large Language Models' Jailbreak]_review.md) |  | 2024-08-22 |
| [.00523v1  review](./2408.00523v1 [Jailbreaking Text-to-Image Models with LLM-Based Agents]_review.md) |  | 2024-08-22 |
| [.06987v1  review](./2310.06987v1 [Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation]_review.md) |  | 2024-08-22 |
| [.14725v2  review](./2403.14725v2 [Testing the Limits of Jailbreaking Defenses with the Purple Problem]_review.md) |  | 2024-08-22 |
| [.12171v1  review](./2403.12171v1 [EasyJailbreak A Unified Framework for Jailbreaking Large Language Models]_review.md) |  | 2024-08-22 |
| [.11757v3  review](./2406.11757v3 [STAR SocioTechnical Approach to Red Teaming Language Models]_review.md) |  | 2024-08-22 |
| [.02309v1  review](./2402.02309v1 [Jailbreaking Attack against Multimodal Large Language Model]_review.md) |  | 2024-08-22 |
| [.18510v1  review](./2406.18510v1 [WildTeaming at Scale From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models]_review.md) |  | 2024-08-22 |
| [.03299v4  review](./2402.03299v4 [GUARD Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models]_review.md) |  | 2024-08-22 |
| [.08487v3  review](./2307.08487v3 [Latent Jailbreak A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models]_review.md) |  | 2024-08-22 |
| [.16686v1  review](./2407.16686v1 [Can Large Language Models Automatically Jailbreak GPT-4V]_review.md) |  | 2024-08-22 |
| [.10794v2  review](./2406.10794v2 [Towards Understanding Jailbreak Attacks in LLMs A Representation Space Analysis]_review.md) |  | 2024-08-22 |
| [.19103v1  review](./2405.19103v1 [Voice Jailbreak Attacks Against GPT-4o]_review.md) |  | 2024-08-22 |
| [.06474v3  review](./2310.06474v3 [Multilingual Jailbreak Challenges in Large Language Models]_review.md) |  | 2024-08-22 |
| [.08705v1  review](./2406.08705v1 [When LLM Meets DRL Advancing Jailbreaking Efficiency via DRL-guided Search]_review.md) |  | 2024-08-22 |
| [.05274v2  review](./2309.05274v2 [FuzzLLM A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak Vulnerabilities in Large Language Models]_review.md) |  | 2024-08-22 |
| [.09177v1  review](./2402.09177v1 [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks]_review.md) |  | 2024-08-22 |
| [.08441v1  review](./2407.08441v1 [Are Large Language Models Really Bias-Free Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation]_review.md) |  | 2024-08-22 |
| [.05608v2  review](./2311.05608v2 [FigStep Jailbreaking Large Vision-language Models via Typographic Visual Prompts]_review.md) |  | 2024-08-22 |
| [.08416v1  review](./2402.08416v1 [Pandora Jailbreak GPTs by Retrieval Augmented Generation Poisoning]_review.md) |  | 2024-08-22 |
| [.11308v1  review](./2408.11308v1 [EEG-Defender Defending against Jailbreak through Early Exit Generation of Large Language Models]_review.md) |  | 2024-08-22 |
| [.05668v1  review](./2402.05668v1 [Comprehensive Assessment of Jailbreak Attacks Against LLMs]_review.md) |  | 2024-08-22 |
| [.02416v1  review](./2408.02416v1 [Why Are My Prompts Leaked Unraveling Prompt Extraction Threats in Customized Large Language Models]_review.md) |  | 2024-08-22 |
| [.08956v2  review](./2407.08956v2 [DeCE Deceptive Cross-Entropy Loss Designed for Defending Backdoor Attacks]_review.md) |  | 2024-08-22 |
| [.09091v2  review](./2402.09091v2 [Play Guessing Game with LLM Indirect Jailbreak Attack with Implicit Clues]_review.md) |  | 2024-08-22 |
| [.03027v3  review](./2404.03027v3 [JailBreakV-28K A Benchmark for Assessing the Robustness of MultiModal Large Language Models against Jailbreak Attacks]_review.md) |  | 2024-08-22 |
| [.03603v1  review](./2408.03603v1 [EnJa Ensemble Jailbreak on Large Language Models]_review.md) |  | 2024-08-22 |
| [.14644v2  review](./2407.14644v2 [Human-Interpretable Adversarial Prompt Attack on Large Language Models with Situational Context]_review.md) |  | 2024-08-22 |
| [.06255v2  review](./2402.06255v2 [Fight Back Against Jailbreaking via Prompt Adversarial Tuning]_review.md) |  | 2024-08-22 |
| [.16567v2  review](./2405.16567v2 [Automatic Jailbreaking of the Text-to-Image Generative AI Systems]_review.md) |  | 2024-08-22 |
| [.16006v2  review](./2402.16006v2 [ASETF A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings]_review.md) |  | 2024-08-22 |
| [.05197v3  review](./2304.05197v3 [Multi-step Jailbreaking Privacy Attacks on ChatGPT]_review.md) |  | 2024-08-22 |
| [.08464v1  review](./2408.08464v1 [textit{MMJ-Bench} A Comprehensive Study on Jailbreak Attacks and Defenses for Vision Language Models]_review.md) |  | 2024-08-22 |
| [.02119v2  review](./2312.02119v2 [Tree of Attacks Jailbreaking Black-Box LLMs Automatically]_review.md) |  | 2024-08-22 |
| [.07188v2  review](./2406.07188v2 [Merging Improves Self-Critique Against Jailbreak Attacks]_review.md) |  | 2024-08-22 |
| [.01446v4  review](./2309.01446v4 [Open Sesame! Universal Black Box Jailbreaking of Large Language Models]_review.md) |  | 2024-08-22 |
| [.12867v4  review](./2301.12867v4 [Red teaming ChatGPT via Jailbreaking Bias, Robustness, Reliability and Toxicity]_review.md) |  | 2024-08-22 |
| [.05941v1  review](./2406.05941v1 [Jailbreaking Quantum Computers]_review.md) |  | 2024-08-22 |
| [.04522v1  review](./2408.04522v1 [Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language Models]_review.md) |  | 2024-08-22 |
| [.13148v2  review](./2402.13148v2 [Defending Jailbreak Prompts via In-Context Adversarial Game]_review.md) |  | 2024-08-22 |
| [.03191v4  review](./2311.03191v4 [DeepInception Hypnotize Large Language Model to Be Jailbreaker]_review.md) |  | 2024-08-22 |
| [.05147v2  review](./2408.05147v2 [Gemma Scope Open Sparse Autoencoders Everywhere All At Once on Gemma 2]_review.md) |  | 2024-08-22 |
| [.05467v1  review](./2402.05467v1 [Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia]_review.md) |  | 2024-08-22 |
| [.02928v2  review](./2404.02928v2 [Jailbreaking Prompt Attack A Controllable Adversarial Attack against Diffusion Models]_review.md) |  | 2024-08-22 |
| [.06622v1  review](./2406.06622v1 [Adversarial Tuning Defending Against Jailbreak Attacks for LLMs]_review.md) |  | 2024-08-22 |
| [.14393v2  review](./2406.14393v2 [Jailbreaking as a Reward Misspecification Problem]_review.md) |  | 2024-08-22 |
| [.19845v2  review](./2406.19845v2 [Virtual Context Enhancing Jailbreak Attacks with Special Token Injection]_review.md) |  | 2024-08-22 |
| [.20413v1  review](./2405.20413v1 [Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters]_review.md) |  | 2024-08-22 |
| [.13860v2  review](./2305.13860v2 [Jailbreaking ChatGPT via Prompt Engineering An Empirical Study]_review.md) |  | 2024-08-22 |
| [.11182v1  review](./2408.11182v1 [Hide Your Malicious Goal Into Benign Narratives Jailbreak Large Language Models through Neural Carrier Articles]_review.md) |  | 2024-08-22 |
| [.06387v3  review](./2310.06387v3 [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations]_review.md) |  | 2024-08-22 |
| [.05644v2  review](./2406.05644v2 [How Alignment and Jailbreak Work Explain LLM Safety through Intermediate Hidden States]_review.md) |  | 2024-08-22 |
| [.02151v2  review](./2404.02151v2 [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks]_review.md) |  | 2024-08-22 |
| [.09346v1  review](./2403.09346v1 [AVIBench Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions]_review.md) |  | 2024-08-22 |
| [.08924v1  review](./2408.08924v1 [Prefix Guidance A Steering Wheel for Large Language Models to Defend Against Jailbreak Attacks]_review.md) |  | 2024-08-22 |
| [.14455v4  review](./2311.14455v4 [Universal Jailbreak Backdoors from Poisoned Human Feedback]_review.md) |  | 2024-08-22 |
| [.20242v2  review](./2407.20242v2 [The Threats of Embodied Multimodal LLMs Jailbreaking Robotic Manipulation in the Physical World]_review.md) |  | 2024-08-22 |
| [.13517v1  review](./2402.13517v1 [Round Trip Translation Defence against Large Language Model Jailbreaking Attacks]_review.md) |  | 2024-08-22 |
| [.20859v1  review](./2407.20859v1 [Breaking Agents Compromising Autonomous LLM Agents Through Malfunction Amplification]_review.md) |  | 2024-08-22 |
| [.14965v4  review](./2305.14965v4 [Tricking LLMs into Disobedience Formalizing, Analyzing, and Detecting Jailbreaks]_review.md) |  | 2024-08-22 |
| [.13796v1  review](./2407.13796v1 [Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language Models]_review.md) |  | 2024-08-22 |
| [.04403v1  review](./2405.04403v1 [Learning To See But Forgetting To Follow Visual Instruction Tuning Makes LLMs More Prone To Jailbreak Attacks]_review.md) |  | 2024-08-22 |
| [.08309v1  review](./2404.08309v1 [Subtoxic Questions Dive Into Attitude Change of LLM's Response in Jailbreak Attempts]_review.md) |  | 2024-08-22 |
| [.17915v1  review](./2407.17915v1 [The Dark Side of Function Calling Pathways to Jailbreaking Large Language Models]_review.md) |  | 2024-08-22 |
| [.08424v1  review](./2403.08424v1 [Tastle Distract Large Language Models for Automatic Jailbreak Attack]_review.md) |  | 2024-08-22 |
| [.02855v1  review](./2407.02855v1 [Safe Unlearning A Surprisingly Effective and Generalizable Solution to Defend Against Jailbreak Attacks]_review.md) |  | 2024-08-22 |
| [.07932v2  review](./2405.07932v2 [PARDEN, Can You Repeat That Defending against Jailbreaks via Repetition]_review.md) |  | 2024-08-22 |
| [.21018v2  review](./2405.21018v2 [Improved Techniques for Optimization-Based Jailbreaking on Large Language Models]_review.md) |  | 2024-08-22 |
| [.04295v1  review](./2407.04295v1 [Jailbreak Attacks and Defenses Against Large Language Models A Survey]_review.md) |  | 2024-08-22 |
| [.01704v2  review](./2402.01704v2 [States as Strings as Strategies Steering Language Models with Game-Theoretic Solvers]_review.md) |  | 2024-08-22 |
| [.18104v2  review](./2402.18104v2 [Making Them Ask and Answer Jailbreaking Large Language Models in Few Queries via Disguise and Reconstruction]_review.md) |  | 2024-08-22 |
| [.02534v1  review](./2407.02534v1 [Image-to-Text Logic Jailbreak Your Imagination can Help You Do Anything]_review.md) |  | 2024-08-22 |
| [.13077v1  review](./2405.13077v1 [GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation]_review.md) |  | 2024-08-22 |
| [.14872v2  review](./2402.14872v2 [Semantic Mirror Jailbreak Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs]_review.md) |  | 2024-08-22 |
| [.02691v3  review](./2403.02691v3 [InjecAgent Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents]_review.md) |  | 2024-08-22 |
| [.03825v2  review](./2308.03825v2 [Do Anything Now Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models]_review.md) |  | 2024-08-22 |
| [.03684v4  review](./2310.03684v4 [SmoothLLM Defending Large Language Models Against Jailbreaking Attacks]_review.md) |  | 2024-08-22 |
| [.15570v1  review](./2402.15570v1 [Fast Adversarial Attacks on Language Models In One GPU Minute]_review.md) |  | 2024-08-22 |
| [.14461v2  review](./2404.14461v2 [Competition Report Finding Universal Jailbreak Backdoors in Aligned LLMs]_review.md) |  | 2024-08-22 |
| [.16205v3  review](./2407.16205v3 [Figure it Out Analyzing-based Jailbreak Attack on Large Language Models]_review.md) |  | 2024-08-22 |
| [.01251v2  review](./2403.01251v2 [Accelerating Greedy Coordinate Gradient via Probe Sampling]_review.md) |  | 2024-08-22 |
| [.13494v2  review](./2402.13494v2 [GradSafe Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis]_review.md) |  | 2024-08-22 |
| [.13662v1  review](./2406.13662v1 [ObscurePrompt Jailbreaking Large Language Models via Obscure Input]_review.md) |  | 2024-08-22 |
| [.03045v1  review](./2407.03045v1 [JailbreakHunter A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational Datasets]_review.md) |  | 2024-08-22 |
| [.15211v1  review](./2407.15211v1 [When Do Universal Image Jailbreaks Transfer Between Vision-Language Models]_review.md) |  | 2024-08-22 |
| [.03348v2  review](./2311.03348v2 [Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation]_review.md) |  | 2024-08-22 |
| [.10260v1  review](./2402.10260v1 [A StrongREJECT for Empty Jailbreaks]_review.md) |  | 2024-08-22 |
| [.10601v1  review](./2402.10601v1 [Jailbreaking Proprietary Large Language Models using Word Substitution Cipher]_review.md) |  | 2024-08-22 |
| [.00399v2  review](./2404.00399v2 [Aurora-M The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order]_review.md) |  | 2024-08-22 |
| [.08676v3  review](./2404.08676v3 [ALERT A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming]_review.md) |  | 2024-08-22 |
| [.04849v2  review](./2404.04849v2 [Hidden You Malicious Goal Into Benign Narratives Jailbreak Large Language Models through Logic Chain Injection]_review.md) |  | 2024-08-22 |
| [.09096v2  review](./2311.09096v2 [Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization]_review.md) |  | 2024-08-22 |
| [.13457v2  review](./2402.13457v2 [A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models]_review.md) |  | 2024-08-22 |
| [.11753v4  review](./2402.11753v4 [ArtPrompt ASCII Art-based Jailbreak Attacks against Aligned LLMs]_review.md) |  | 2024-08-22 |
| [.19668v1  review](./2405.19668v1 [AutoBreach Universal and Adaptive Jailbreaking with Efficient Wordplay-Guided Optimization]_review.md) |  | 2024-08-22 |
| [.06302v2  review](./2406.06302v2 [Unveiling the Safety of GPT-4o An Empirical Study using Jailbreak Attacks]_review.md) |  | 2024-08-22 |
| [.16914v2  review](./2402.16914v2 [DrAttack Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers]_review.md) |  | 2024-08-22 |
| [.02910v2  review](./2403.02910v2 [ImgTrojan Jailbreaking Vision-Language Models with ONE Image]_review.md) |  | 2024-08-22 |
| [.08725v1  review](./2406.08725v1 [RL-JACK Reinforcement Learning-powered Black-box Jailbreaking Attack against LLMs]_review.md) |  | 2024-08-22 |
| [.16765v1  review](./2401.16765v1 [A Cross-Language Investigation into Jailbreak Attacks in Large Language Models]_review.md) |  | 2024-08-22 |
| [.15727v2  review](./2402.15727v2 [LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner A Vision Paper]_review.md) |  | 2024-08-22 |
| [.15902v1  review](./2405.15902v1 [Hacc-Man An Arcade Game for Jailbreaking LLMs]_review.md) |  | 2024-08-22 |
| [.06407v3  review](./2404.06407v3 [Rethinking How to Evaluate Language Model Jailbreak]_review.md) |  | 2024-08-22 |
| [.15180v2  review](./2402.15180v2 [Break the Breakout Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement]_review.md) |  | 2024-08-22 |
| [.09798v3  review](./2401.09798v3 [All in How You Ask for It Simple Black-Box Method for Jailbreak Attacks]_review.md) |  | 2024-08-22 |
| [.02417v1  review](./2310.02417v1 [Jailbreaker in Jail Moving Target Defense for Large Language Models]_review.md) |  | 2024-08-22 |
| [.18166v2  review](./2405.18166v2 [Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing]_review.md) |  | 2024-08-22 |
| [.08793v1  review](./2404.08793v1 [JailbreakLens Visual Analysis of Jailbreak Attacks Against Large Language Models]_review.md) |  | 2024-08-22 |
| [.16459v3  review](./2402.16459v3 [Defending LLMs against Jailbreaking Attacks via Backtranslation]_review.md) |  | 2024-08-22 |
| [.01376v1  review](./2407.01376v1 [Badllama 3 removing safety finetuning from Llama 3 in minutes]_review.md) |  | 2024-08-22 |
| [.09792v2  review](./2403.09792v2 [Images are Achilles' Heel of Alignment Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models]_review.md) |  | 2024-08-22 |
| [.08567v2  review](./2402.08567v2 [Agent Smith A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast]_review.md) |  | 2024-08-22 |
| [.08983v4  review](./2402.08983v4 [SafeDecoding Defending against Jailbreak Attacks via Safety-Aware Decoding]_review.md) |  | 2024-08-22 |
| [.20015v1  review](./2405.20015v1 [Efficient LLM-Jailbreaking by Introducing Visual Modality]_review.md) |  | 2024-08-22 |
| [.00867v2  review](./2403.00867v2 [Gradient Cuff Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes]_review.md) |  | 2024-08-22 |
| [.10253v4  review](./2309.10253v4 [GPTFUZZER Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts]_review.md) |  | 2024-08-22 |
| [.13068v2  review](./2405.13068v2 [Lockpicking LLMs A Logit-Based Jailbreak Using Token-level Manipulation]_review.md) |  | 2024-08-22 |
| [.04811v1  review](./2408.04811v1 [h4rm3l A Dynamic Benchmark of Composable Jailbreak Attacks for LLM Safety Assessment]_review.md) |  | 2024-08-22 |
| [.06561v3  review](./2401.06561v3 [Intention Analysis Makes LLMs A Good Jailbreak Defender]_review.md) |  | 2024-08-22 |
| [.14539v2  review](./2307.14539v2 [Jailbreak in pieces Compositional Adversarial Attacks on Multi-Modal Language Models]_review.md) |  | 2024-08-22 |
| [.17336v1  review](./2403.17336v1 [Don't Listen To Me Understanding and Exploring Jailbreak Prompts of Large Language Models]_review.md) |  | 2024-08-22 |
| [.03654v2  review](./2405.03654v2 [Can LLMs Deeply Detect Complex Malicious Queries A Framework for Jailbreaking via Obfuscating Intent]_review.md) |  | 2024-08-22 |
| [.09289v1  review](./2406.09289v1 [Understanding Jailbreak Success A Study of Latent Space Dynamics in Large Language Models]_review.md) |  | 2024-08-22 |
| [.09326v1  review](./2408.09326v1 [Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks]_review.md) |  | 2024-08-22 |
| [.20775v1  review](./2405.20775v1 [Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models]_review.md) |  | 2024-08-22 |
| [.02446v2  review](./2310.02446v2 [Low-Resource Languages Jailbreak GPT-4]_review.md) |  | 2024-08-22 |
| [.09093v1  review](./2408.09093v1 [BaThe Defense against the Jailbreak Attack in Multimodal Large Language Models by Treating Harmful Instruction as Backdoor Trigger]_review.md) |  | 2024-08-22 |
| [.11717v2  review](./2406.11717v2 [Refusal in Language Models Is Mediated by a Single Direction]_review.md) |  | 2024-08-22 |
| [.05880v2  review](./2404.05880v2 [Eraser Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge]_review.md) |  | 2024-08-22 |
| [.14023v1  review](./2405.14023v1 [WordGame Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation in Query and Response]_review.md) |  | 2024-08-22 |
| [.14968v3  review](./2402.14968v3 [Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment]_review.md) |  | 2024-08-22 |
| [.18122v1  review](./2406.18122v1 [Poisoned LangChain Jailbreak LLMs by LangChain]_review.md) |  | 2024-08-22 |
| [.06824v3  review](./2401.06824v3 [Rethinking Jailbreaking through the Lens of Representation Engineering]_review.md) |  | 2024-08-22 |
| [.04451v2  review](./2310.04451v2 [AutoDAN Generating Stealthy Jailbreak Prompts on Aligned Large Language Models]_review.md) |  | 2024-08-22 |
| [.17447v1  review](./2407.17447v1 [Fluent Student-Teacher Redteaming]_review.md) |  | 2024-08-22 |
| [.09127v2  review](./2311.09127v2 [Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts]_review.md) |  | 2024-08-22 |
| [.12702v2 Jailbreak Paradox The Achilles' Heel of LLMs] review](./2406.12702v2 [[WIP] Jailbreak Paradox The Achilles' Heel of LLMs]_review.md) |  | 2024-08-22 |
| [.03411v1  review](./2404.03411v1 [Red Teaming GPT-4V Are GPT-4V Safe Against UniMulti-Modal Jailbreak Attacks]_review.md) |  | 2024-08-22 |
| [.01229v1  review](./2405.01229v1 [Boosting Jailbreak Attack with Momentum]_review.md) |  | 2024-08-22 |
| [.05498v1  review](./2406.05498v1 [SelfDefend LLMs Can Defend Themselves against Jailbreaking in a Practical Manner]_review.md) |  | 2024-08-22 |
| [.15050v1  review](./2407.15050v1 [Arondight Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts]_review.md) |  | 2024-08-22 |
| [.21659v2  review](./2407.21659v2 [Defending Jailbreak Attack in VLMs via Cross-modality Information Detector]_review.md) |  | 2024-08-22 |
| [.04783v1  review](./2403.04783v1 [AutoDefense Multi-Agent LLM Defense against Jailbreak Attacks]_review.md) |  | 2024-08-22 |
| [.01288v1  review](./2406.01288v1 [Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses]_review.md) |  | 2024-08-22 |
| [.17256v2  review](./2401.17256v2 [Weak-to-Strong Jailbreaking on Large Language Models]_review.md) |  | 2024-08-22 |
| [.16192v2  review](./2402.16192v2 [Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing]_review.md) |  | 2024-08-22 |
| [.06373v2  review](./2401.06373v2 [How Johnny Can Persuade LLMs to Jailbreak Them Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs]_review.md) |  | 2024-08-22 |
| [.04686v1  review](./2408.04686v1 [Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles]_review.md) |  | 2024-08-22 |
| [.08956v1  review](./2407.08956v1 [DeCE Deceptive Cross-Entropy Loss Designed for Defending Backdoor Attacks]_review.md) |  | 2024-08-22 |
| [.18495v2  review](./2406.18495v2 [WildGuard Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs]_review.md) |  | 2024-08-22 |
| [.17894v1  review](./2405.17894v1 [White-box Multimodal Jailbreaks Against Large Vision-Language Models]_review.md) |  | 2024-08-22 |
| [.01599v2  review](./2407.01599v2 [JailbreakZoo Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models]_review.md) |  | 2024-08-22 |
| [.16369v1  review](./2404.16369v1 [Don't Say No Jailbreaking LLM by Suppressing Refusal]_review.md) |  | 2024-08-22 |
| [.14859v1  review](./2406.14859v1 [From LLMs to MLLMs Exploring the Landscape of Multimodal Jailbreaking]_review.md) |  | 2024-08-22 |
| [.16717v1  review](./2402.16717v1 [CodeChameleon Personalized Encryption Framework for Jailbreaking Large Language Models]_review.md) |  | 2024-08-22 |
| [.03391v1  review](./2407.03391v1 [Soft Begging Modular and Efficient Shielding of LLMs against Prompt Injection and Jailbreaking based on Prompt Tuning]_review.md) |  | 2024-08-22 |
| [.03729v3  review](./2401.03729v3 [The Butterfly Effect of Altering Prompts How Small Changes and Jailbreaks Affect Large Language Model Performance]_review.md) |  | 2024-08-22 |
| [.01318v4  review](./2404.01318v4 [JailbreakBench An Open Robustness Benchmark for Jailbreaking Large Language Models]_review.md) |  | 2024-08-22 |
| [.18118v2  review](./2406.18118v2 [SafeAligner Safety Alignment against Jailbreak Attacks via Response Disparity Guidance]_review.md) |  | 2024-08-22 |
| [.09113v1  review](./2405.09113v1 [Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization]_review.md) |  | 2024-08-22 |
| [.10862v2  review](./2401.10862v2 [Pruning for Protection Increasing Jailbreak Resistance in Aligned LLMs Without Fine-Tuning]_review.md) |  | 2024-08-22 |
| [.09002v5  review](./2401.09002v5 [AttackEval How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models]_review.md) |  | 2024-08-22 |
| [.08715v2  review](./2307.08715v2 [MasterKey Automated Jailbreak Across Multiple Large Language Model Chatbots]_review.md) |  | 2024-08-22 |
| [.09827v2  review](./2311.09827v2 [Cognitive Overload Jailbreaking Large Language Models with Overloaded Logical Thinking]_review.md) |  | 2024-08-22 |
| [.17600v5  review](./2311.17600v5 [MM-SafetyBench A Benchmark for Safety Evaluation of Multimodal Large Language Models]_review.md) |  | 2024-08-22 |
| [.09324v1  review](./2406.09324v1 [Bag of Tricks Benchmarking of Jailbreak Attacks on LLMs]_review.md) |  | 2024-08-22 |
| [.14857v2  review](./2402.14857v2 [Is the System Message Really Important to Jailbreaks in Large Language Models]_review.md) |  | 2024-08-22 |
| [.01420v1  review](./2408.01420v1 [Mission Impossible A Statistical Perspective on Jailbreaking LLMs]_review.md) |  | 2024-08-22 |
| [.07921v2  review](./2404.07921v2 [AmpleGCG Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs]_review.md) |  | 2024-08-22 |
| [.13352v2  review](./2406.13352v2 [AgentDojo A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents]_review.md) |  | 2024-08-22 |
