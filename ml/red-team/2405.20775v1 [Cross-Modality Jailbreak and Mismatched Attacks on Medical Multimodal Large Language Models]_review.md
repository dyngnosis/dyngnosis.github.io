#Tags
[[Research/Research Papers/2405.20775v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0031/ErodeMLModelIntegrity
#AMLT0054/LLMJailbreak

**Title:** Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models
**Authors:** Xijie Huang, Xinyuan Wang, Hantao Zhang, Jiawen Xi, Jingkun An, Hao Wang, Chengwei Pan
**Affiliations:** Beihang University, Beijing, China; University of Science and Technology of China, Heifei, China
**Publication Date:** May 26, 2024

Summary:
This paper investigates security vulnerabilities in Medical Multimodal Large Language Models (MedMLLMs) by introducing novel attack methods: mismatched malicious attack (2M-attack) and optimized mismatched malicious attack (O2M-attack). The authors create a comprehensive dataset called 3MAD and propose a Multimodal Cross-optimization Method (MCM) to enhance attack success rates on MedMLLMs.

Key Contributions:
- Definition of 2M-attack and O2M-attack for MedMLLMs
- Creation of the 3MAD dataset for evaluating MedMLLM vulnerabilities
- Development of the MCM optimization method for jailbreaking attacks
- Comprehensive evaluation of attacks on multiple state-of-the-art MedMLLMs

Problem Statement:
The paper addresses the underexplored security vulnerabilities of MedMLLMs in clinical environments, where the accuracy and relevance of question-and-answer interactions are critical for patient safety and medical decision-making.

Methodology:
1. Dataset Creation:
   - 3MAD dataset: 66,609 images across 18 imaging types and 1,080 GPT-4-aided prompts
   - 3MAD-Tiny-1K: 6,480 text-image jailbreak pairs for focused testing

2. Attack Methods:
   - 2M-attack: Combines mismatched clinical data with malicious queries
   - O2M-attack: Applies jailbreak optimization techniques to 2M-attack

3. Multimodal Cross-optimization Method (MCM):
   - Iteratively enhances adversarial strength of both image and text inputs
   - Uses gradient-based approach to modify inputs for minimizing loss function

4. Evaluation:
   - White-box attacks on LLaVA-Med
   - Transfer attacks on four other state-of-the-art models (CheXagent, XrayGLM, Med-flamingo, RadFM)

5. Metrics:
   - Attack Success Rate (ASR)
   - Refusal Rate (RR)
   - Text similarity score (Stext)
   - Image similarity score (Simg)

Main Results:
1. MCM outperforms other attack methods (GCG, PGD) in terms of ASR and RR:
   - Highest ASR in Malicious attacks (0.8157) and 2M-attacks (0.8204)
   - Lowest RR in Mismatched attacks (0.0074)

2. Transfer attacks show vulnerabilities across multiple MedMLLMs:
   - RadFM's ASR reaches 0.985 under O2M-attack(MCM)
   - CheXagent's ASR for Malicious inputs is 0.905

3. MedMLLMs maintain relatively stable semantic alignment (Stext) across various input conditions but show lower image-text matching (Simg) under complex attacks.

Qualitative Analysis:
- The success of 2M-attacks and O2M-attacks reveals that clinical mismatches can be exploited to jailbreak MedMLLMs, posing significant risks in medical settings.
- The high ASR across different models suggests a systemic vulnerability in current MedMLLM architectures.
- The stability of Stext scores indicates that models can generate coherent responses even when compromised, making detection of successful attacks challenging.

Limitations:
- Focus on modalities and anatomy mismatches, not addressing finer-grained clinical challenges like lesion misinterpretations
- Potential oversight of specialized medical subfields in the dataset
- Ethical concerns regarding the development and publication of attack methods on medical AI systems

Conclusion and Future Work:
The paper demonstrates significant vulnerabilities in MedMLLMs and emphasizes the urgent need for robust security measures in medical AI. Future work should focus on:
1. Enhancing defense mechanisms against complex, multi-modal attacks
2. Expanding the dataset to cover more specialized medical areas
3. Developing ethical guidelines for security research in medical AI
4. Improving the robustness of MedMLLMs against mismatched and malicious inputs

Tools Introduced:
- 3MAD dataset: Available at https://huggingface.co/datasets/MedMLLM-attack/3MAD-66K and https://huggingface.co/datasets/MedMLLM-attack/3MAD-Tiny-1K
- MCM optimization method: Code available at https://github.com/dirtycomputer/O2M_attack.git

## Repository Token Information
Total tokens in repository: 13185

Tokens per file:
- metric.py: 666 tokens
- run_jailbreak.py: 1911 tokens
- Readme.md: 871 tokens
- api/gpt.py: 262 tokens
- api/__init__.py: 0 tokens
- api/util.py: 40 tokens
- api/gemini.py: 131 tokens
- llava/__init__.py: 14 tokens
- llava/utils.py: 234 tokens
- llava/attacks/attacks.py: 2117 tokens
- llava/gcg/string_utils.py: 503 tokens
- llava/gcg/conversation.py: 1105 tokens
- llava/gcg/__init__.py: 0 tokens
- llava/gcg/opti_utils.py: 2069 tokens
- llava/model/__init__.py: 15 tokens
- llava/model/utils.py: 183 tokens
- llava/model/llava.py: 3064 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models

## 1. Project Overview

This repository contains the implementation of the research paper "Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models". The project aims to explore and demonstrate security vulnerabilities in Medical Multimodal Large Language Models (MedMLLMs) through novel attack methods.

### Project Structure

The repository is organized as follows:

- `metric.py`: Contains similarity calculation metrics for text and images
- `run_jailbreak.py`: Main script for running jailbreak attacks
- `api/`: Contains API wrappers for GPT and Gemini models
- `llava/`: Core implementation of the LLaVA model and attack methods
  - `attacks/`: Implementation of attack algorithms
  - `gcg/`: Implementation of gradient-based optimization methods
  - `model/`: LLaVA model implementation

## 2. Key Components and Functionality

### 2.1 Similarity Metrics (`metric.py`)

This file implements two main classes:

1. `TxtSimCal`: Calculates text similarity using the BGEM3FlagModel.
2. `ImgSimCal`: Calculates image similarity using OpenCLIP models.

These metrics are crucial for evaluating the effectiveness of attacks by measuring how much the generated content deviates from the original input.

### 2.2 Jailbreak Runner (`run_jailbreak.py`)

This script orchestrates the jailbreak attacks. Key functionalities include:

- Loading the dataset and model
- Implementing different attack modes (MCM, PGD, GCG)
- Logging results to Weights & Biases (wandb)

### 2.3 LLaVA Model (`llava/model/llava.py`)

This file contains the implementation of the LLaVA (Large Language and Vision Assistant) model, which is a multimodal model combining LLAMA and CLIP. Key classes:

- `LlavaLlamaModel`: Extends LlamaModel with vision capabilities
- `LlavaLlamaForCausalLM`: Causal language model using LlavaLlamaModel

### 2.4 Attack Implementations (`llava/attacks/attacks.py`)

This file implements various attack methods:

- `gcg_attack`: Gradient-based attack
- `pgd_attack`: Projected Gradient Descent attack
- `run_mcm_attack`: Multimodal Cross-optimization Method (MCM) attack

## 3. Relation to Research Paper Concepts

### 3.1 2M-attack and O2M-attack

The implemented attacks in `attacks.py` correspond to the 2M-attack (mismatched malicious attack) and O2M-attack (optimized mismatched malicious attack) described in the paper. These attacks combine mismatched clinical data with malicious queries to exploit vulnerabilities in MedMLLMs.

### 3.2 Multimodal Cross-optimization Method (MCM)

The `run_mcm_attack` function in `attacks.py` implements the MCM method proposed in the paper. This method iteratively enhances the adversarial strength of both image and text inputs to maximize the attack success rate.

### 3.3 Evaluation Metrics

The similarity metrics implemented in `metric.py` correspond to the Stext and Simg metrics mentioned in the paper, which are used to evaluate the semantic alignment and image-text matching of the model outputs.

## 4. Notable Algorithms and Techniques

### 4.1 Gradient-based Optimization

The `gcg_attack` function uses gradient information to optimize the adversarial inputs. This technique is central to the paper's approach of finding effective jailbreak prompts.

### 4.2 Projected Gradient Descent (PGD)

The `pgd_attack` function implements the PGD algorithm, which is a powerful method for generating adversarial examples by iteratively applying gradient steps and projecting the result back onto a constrained space.

### 4.3 Token Sampling

In `llava/gcg/opti_utils.py`, the `sample_control` function implements a method for sampling new tokens based on their gradient information. This is a key component of the optimization process for textual adversarial inputs.

# 5. Potential Enhancements

1. **Improved Defense Mechanisms**: 
   - Implement and evaluate various defense strategies against the proposed attacks.
   - Explore techniques like adversarial training, input sanitization, or robust optimization to enhance model resilience.

2. **Expanded Dataset Coverage**:
   - Extend the 3MAD dataset to include more specialized medical areas and rare conditions.
   - Implement data augmentation techniques to increase the diversity of the dataset.

3. **Real-time Attack Detection**:
   - Develop a module for real-time detection of potential jailbreak attempts.
   - Implement anomaly detection algorithms to identify suspicious input patterns.

4. **Multi-model Ensemble Approach**:
   - Implement an ensemble of different MedMLLMs to improve robustness against attacks.
   - Explore voting or consensus mechanisms to reduce the impact of successful attacks on individual models.

5. **Adaptive Attack Strategies**:
   - Implement reinforcement learning techniques to develop adaptive attack strategies.
   - Explore the use of generative models to create more diverse and unpredictable adversarial inputs.

These enhancements would address some of the limitations mentioned in the paper and push the research forward by improving both the attack and defense aspects of MedMLLM security.