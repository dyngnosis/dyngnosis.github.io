#Tags
[[Research/Research Papers/2407.00869v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0057/LLMDataLeakage

**Title:** Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks
**Authors:** Yue Zhou, Henry Peng Zou, Barbara Di Eugenio, Yang Zhang
**Affiliations:** University of Illinois Chicago, MIT-IBM Watson AI Lab, IBM Research
**Publication Date:** September 23, 2024

Key Contributions:
- Discovery of LLMs' inability to generate fallacious reasoning
- Development of a new jailbreak attack method called Fallacy Failure Attack (FFA)
- Evaluation of FFA on five safety-aligned large language models
- Comparison with four previous jailbreak methods
- Analysis of defense methods against FFA

Problem Statement:
The research addresses the vulnerability of large language models (LLMs) to jailbreak attacks, specifically exploiting their inability to generate fallacious reasoning when requested.

Methodology:
1. Pilot study on LLMs' ability to perform fallacious reasoning using four reasoning benchmarks
2. Development of FFA jailbreak attack method
3. Evaluation of FFA on five LLMs: GPT-3.5-turbo, GPT-4, Google Gemini-Pro, Vicuna-1.5 (7b), and LLaMA-3 (8b)
4. Comparison with four previous jailbreak methods: GCG, AutoDAN, DeepInception, and ArtPrompt
5. Testing three defense methods: Perplexity Filter, Paraphrasing, and Retokenization

Main Results:
1. LLMs struggle to generate fallacious reasoning, often leaking correct solutions when asked to provide wrong answers
2. FFA achieves competitive performance in jailbreaking LLMs, particularly effective against GPT-3.5, GPT-4, and Vicuna-7b
3. FFA outperforms previous methods in generating more harmful outputs
4. Existing defense methods are not effective against FFA

Qualitative Analysis:
- The study reveals a fundamental limitation in LLMs' ability to generate deceptive content, which can be exploited for malicious purposes
- The effectiveness of FFA highlights the need for more robust safety measures in LLMs
- The research suggests potential applications beyond security, such as self-verification and hallucination detection

Limitations:
- The study does not provide an ideal defense mechanism against FFA
- The approach may not be equally effective across all LLM architectures (e.g., less effective on LLaMA-3)

Conclusion and Future Work:
- The paper demonstrates a novel jailbreak attack method exploiting LLMs' inability to generate fallacious reasoning
- Future work should focus on developing more robust defense strategies and exploring the implications of this finding for LLM behavior and general intelligence

Figures and Tables:
- Figure 1: Illustration of FFA concept
- Figure 2: Accuracy comparison of fallacious and honest solutions on four reasoning tasks
- Figure 3: Example of LLM failing to provide a fallacious solution
- Table 1: Attack efficacy of FFA against five language models compared to baseline methods
- Table 2: FFA performance under the impact of defense approaches

New Tools:
- Fallacy Failure Attack (FFA): A jailbreak attack method that exploits LLMs' inability to generate fallacious reasoning