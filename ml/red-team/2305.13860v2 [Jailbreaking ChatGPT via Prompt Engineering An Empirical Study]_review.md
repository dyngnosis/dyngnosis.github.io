#Tags
[[Research/Research Papers/2305.13860v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0040/MLModelInferenceAPIAccess
#AMLT0043/CraftAdversarialData

**Title:** Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study
**Authors:** Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, Kailong Wang, Yang Liu
**Affiliations:** Nanyang Technological University (Singapore), University of New South Wales (Australia), Virginia Tech (USA), Huazhong University of Science and Technology (China)
**Publication Date:** May 23, 2023 (updated March 10, 2024)

Key Contributions:
- Developed a comprehensive jailbreak classification model with 10 distinct categories
- Conducted an empirical study on jailbreak prompt effectiveness across various scenarios
- Analyzed the evolution and robustness of jailbreak prompts
- Investigated the protection strength of ChatGPT against jailbreak attempts

Problem Statement:
The study addresses the challenges related to content constraints and potential misuse of Large Language Models (LLMs) like ChatGPT, focusing on the effectiveness of jailbreak prompts in bypassing restrictions and the resilience of ChatGPT against these attempts.

Methodology:
1. Collected 78 real-world jailbreak prompts
2. Developed a classification model for jailbreak prompts
3. Generated 40 prohibited scenarios based on OpenAI's disallowed usage policy
4. Conducted experiments using GPT-3.5-Turbo and GPT-4 models
5. Analyzed jailbreak success rates and robustness across different scenarios

Main Results and Findings:
1. Jailbreak prompts were categorized into 3 types and 10 patterns
2. Pretending (97.44%) was the most prevalent jailbreak strategy
3. Simulate Jailbreaking (SIMU) and Superior Model (SUPER) were the most effective patterns
4. GPT-4 showed improved resistance to jailbreak attempts compared to GPT-3.5-Turbo
5. Jailbreak prompts significantly outperformed non-jailbreak prompts in bypassing restrictions

Qualitative Analysis:
- The study reveals that jailbreak prompts can consistently evade restrictions in various scenarios
- Complex prompts combining multiple techniques are more likely to succeed in jailbreaking
- The effectiveness of jailbreak prompts varies across different prohibited scenarios
- There is a discrepancy between the prohibition strength of content categories and their real-world severity

Limitations:
- The study is limited to ChatGPT and may not generalize to all LLMs
- The prohibited scenarios were manually created and may not cover all possible use cases
- The evaluation of jailbreak success was based on manual analysis, which may introduce subjective factors

Conclusion and Future Work:
- The study highlights the need for improved protection mechanisms in LLMs
- Future research directions include:
  1. Developing a top-down taxonomy of jailbreak prompts
  2. Aligning prompt-based jailbreaking with existing vulnerability categories
  3. Generating new jailbreak prompts for analysis and prevention
  4. Developing jailbreak prevention techniques at various stages of the LLM pipeline
  5. Testing open-source LLMs for vulnerabilities to prompt-based attacks
  6. Analyzing the output boundaries of LLMs under jailbreak scenarios

Tools Introduced:
- A dataset of 78 real-world jailbreak prompts (available at [11])
- A jailbreak prompt classification model

Figures and Tables:
- Figure 1: Motivating example for jailbreaking
- Figure 2: Taxonomy of jailbreak prompt patterns
- Table I: Taxonomy of jailbreak prompts
- Table II: Summarized descriptions and examples of OpenAI's disallowed usages
- Table III: Number of successful jailbreaking attempts for each pattern and scenario
- Table IV: Evolution of DAN jailbreak prompts
- Table V: Successful cases in GPT-3.5-Turbo vs GPT-4
- Table VI: Numbers of successful cases for each pattern and scenario with question details
- Table VII: Comparison of Non-Jailbreak and Jailbreak Outcomes on GPT-4
- Table VIII: Examples of laws and penalties related to the eight content categories