#Tags
[[Research/Research Papers/2307.14539v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models
**Authors:** Erfan Shayegani, Yue Dong, Nael Abu-Ghazaleh
**Affiliation:** Department of Computer Science, University of California, Riverside
**Publication Date:** July 26, 2023 (updated October 10, 2023)

Summary:
This paper introduces novel jailbreak attacks on vision language models (VLMs) using cross-modality attacks on alignment. The authors propose a compositional strategy combining adversarial images with generic prompts to bypass the safety mechanisms of VLMs.

Key Contributions:
- Introduction of cross-modality adversarial vulnerability in VLMs
- Novel embedding-space-based adversarial attack methodology
- Demonstration of the compositional nature of the proposed attacks
- Evaluation of attack effectiveness across different VLMs

Problem Statement:
How to develop effective jailbreak attacks on vision language models that are resilient to text-only jailbreak attempts?

Methodology:
1. Decomposition of malicious prompts into benign textual prompts and malicious triggers
2. Four embedding-based settings for malicious triggers:
   a) Textual trigger
   b) OCR textual trigger
   c) Visual trigger
   d) Combined OCR textual and visual trigger
3. Embedding-space-based adversarial attack to generate benign-looking images
4. Evaluation on LLaVA and LLaMA-Adapter V2 models
5. Human and automatic evaluation of attack success rates

Main Results:
1. High success rates for attacks using image-based triggers (OCR textual, visual, and combined)
2. Low success rates for attacks using textual triggers
3. LLaVA more vulnerable to attacks compared to LLaMA-Adapter V2
4. Compositional nature of attacks demonstrated through mix-and-match of textual prompts and malicious triggers

Qualitative Analysis:
- The attacks exploit cross-modality alignment vulnerabilities in VLMs
- Embedding-space-based attacks lower the entry barrier for potential attackers
- The compositional nature of the attacks allows for broader generalization across various jailbreak scenarios
- The attacks highlight the need for new alignment approaches in multi-modal models

Limitations:
- Requires access to the vision encoder (e.g., CLIP)
- Lower success rates for direct hidden prompt injection attacks
- Potential ethical concerns regarding the generation of harmful content

Conclusion and Future Work:
The paper demonstrates the effectiveness of cross-modality attacks on VLMs and highlights the need for improved alignment techniques in multi-modal models. Future work may focus on developing more robust defense mechanisms and exploring the implications of these attacks on AI safety.

Relevant Figures:
Figure 1: Overview of the proposed methods, including malicious trigger types, optimization strategy, and compositional nature of the attacks.

New Tools:
The paper does not introduce specific named tools or GitHub repositories. However, the authors describe an adversarial image generator algorithm (Algorithm 1) that utilizes the CLIP encoder to generate adversarial images.