#Tags
[[Research/Research Papers/2406.05498v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner
**Authors:** Xunguang Wang, Daoyuan Wu, Zhenlan Ji, Zongjie Li, Pingchuan Ma, Shuai Wang, Yingjiu Li, Yang Liu, Ning Liu, Juergen Rahmel
**Publication Date:** June 8, 2024

Summary:
This paper introduces SelfDefend, a novel framework for defending large language models (LLMs) against jailbreak attacks. Inspired by shadow stacks in traditional security, SelfDefend uses a shadow LLM to protect the target LLM through checkpoint-based access control. The authors validate their approach using GPT-3.5/4 models and further develop tuned open-source defense models that match GPT-4's performance with lower latency.

Key Contributions:
- Introduction of the SelfDefend framework for LLM jailbreak defense
- Empirical validation of LLMs' capability to identify harmful prompts/intentions
- Development of tuned open-source defense models using data distillation
- Demonstration of robustness against various jailbreak attacks and prompt injections

Problem Statement:
Existing jailbreak defenses struggle to handle all types of attacks, maintain low latency, and work with both open-source and closed-source LLMs. The paper aims to address these challenges with a practical, effective defense mechanism.

Methodology:
1. Establish a shadow LLM alongside the target LLM for concurrent protection
2. Design two detection prompts: Pdirect and Pintent
3. Conduct empirical measurements using GPT-3.5/4 models
4. Employ data distillation to tune open-source defense models
5. Evaluate against various jailbreak attacks and compare with existing defenses

Main Results:
1. GPT-3.5-based SelfDefend reduces attack success rate (ASR) by 8.97-97.26% (average: 65.06%)
2. GPT-4-based SelfDefend reduces ASR by 69.69-100% (average: 85.53%)
3. Tuned open-source models match GPT-4-based SelfDefend performance with lower latency
4. SelfDefend outperforms six state-of-the-art defenses in 20 out of 24 tested attack scenarios

Qualitative Analysis:
- SelfDefend's dual-layer protection (target LLM's safety alignment + shadow LLM's detection) significantly enhances defense capabilities
- The framework's design allows for negligible delays on normal queries while maintaining effectiveness against jailbreaks
- Tuned open-source models demonstrate the potential for widespread, low-cost deployment of robust jailbreak defenses

Limitations:
- Potential for false positives on normal queries, though the impact is minimal
- Reliance on the quality of detection prompts and the underlying LLM's capabilities

Conclusion and Future Work:
SelfDefend presents a promising approach to LLM jailbreak defense, offering robust protection across various attack types with low latency. Future work may focus on further reducing false positives and exploring applications in other AI security domains.

Tools Introduced:
- SelfDefend framework (no GitHub repository mentioned)

Figures and Tables:
- Figure 1: High-level overview of the SelfDefend framework and workflow
- Table 1: Comparison of existing jailbreak defenses
- Table 3: ASR results for various jailbreak attacks and normal prompts
- Table 4: Jailbreak ASR for various defense methods