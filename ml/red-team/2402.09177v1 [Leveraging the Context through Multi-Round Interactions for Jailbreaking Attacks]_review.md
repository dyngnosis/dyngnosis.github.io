#Tags
[[Research/Research Papers/2402.09177v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0040/MLModelInferenceAPIAccess
#AMLT0042/VerifyAttack

**Title:** Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks

**Authors:** Yixin Cheng, Markos Georgopoulos, Volkan Cevher, Grigorios G. Chrysos

**Affiliations:** 
- LIONS - École Polytechnique Fédérale de Lausanne
- University of Wisconsin-Madison

**Publication Date:** February 14, 2024

Summary:
This paper introduces a new form of jailbreaking attack on Large Language Models (LLMs) called Contextual Interaction Attack. The attack leverages multi-round interactions and the context vector to guide LLMs into revealing harmful information, demonstrating high success rates across multiple state-of-the-art LLMs.

Key Contributions:
- Introduction of Contextual Interaction Attack, a novel jailbreaking method
- Demonstration of the attack's effectiveness across multiple LLMs
- Highlighting the importance of the context vector in LLM security
- Showing the transferability of the attack across different LLMs

Problem Statement:
As LLM defense mechanisms evolve, traditional jailbreaking attacks face increasing difficulty in extracting harmful information. The research aims to develop a more effective attack method that can circumvent these improved defenses.

Methodology:
1. Utilize an auxiliary LLM to generate preliminary questions
2. Engage in multi-round interactions with the target LLM
3. Leverage the context vector to guide the LLM towards revealing harmful information
4. Evaluate the attack on multiple state-of-the-art LLMs

Main Results:
- Contextual Interaction Attack achieves high success rates across multiple LLMs
- The attack demonstrates strong transferability properties between different LLMs
- The method outperforms existing hand-crafted and automated jailbreaking techniques

Qualitative Analysis:
- The success of the attack highlights the vulnerability of LLMs to context manipulation
- The transferability of the attack suggests a common weakness across different LLM architectures
- The study raises important questions about the role of context in LLM decision-making and security

Limitations:
- The attack requires multiple rounds of interaction, which may increase detection risk
- Ethical concerns arise from the potential misuse of the proposed attack method
- The study does not provide theoretical guarantees for the attack's success

Conclusion and Future Work:
The paper demonstrates the effectiveness of Contextual Interaction Attack in jailbreaking LLMs, emphasizing the importance of considering the context vector in LLM security. Future work may focus on developing more robust defense mechanisms and exploring the theoretical foundations of context-based attacks.

New Tools:
While no specific new tools are mentioned, the paper introduces the Contextual Interaction Attack method, which could potentially be implemented as a tool for testing LLM security.