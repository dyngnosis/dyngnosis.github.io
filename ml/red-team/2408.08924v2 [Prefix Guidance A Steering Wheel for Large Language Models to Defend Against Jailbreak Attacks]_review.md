#Tags
[[Research/Research Papers/2408.08924v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** Prefix Guidance: A Steering Wheel for Large Language Models to Defend Against Jailbreak Attacks
**Authors:** Jiawei Zhao, Kejiang Chen, Xiaojian Yuan, Weiming Zhang
**Affiliation:** University of Science and Technology of China
**Publication Date:** August 15, 2024

Key Contributions:
- Proposal of Prefix Guidance (PG), a novel jailbreak defense method for Large Language Models (LLMs)
- Demonstration of PG's effectiveness across three models and five attack methods
- Preservation of model capabilities while defending against jailbreak attacks

Problem Statement:
LLMs are vulnerable to jailbreak attacks, where adversaries can induce the generation of harmful content through carefully crafted prompts. Existing defense methods often lack effectiveness or significantly impact model capabilities.

Methodology:
1. Prefix Selection:
   - Input harmful and non-harmful prompts to the model
   - Identify common refusal prefixes
   - Select optimal prefix based on error percentage

2. Harmful Prompt Classification:
   - Fine-tune a Roberta-base model as a binary classifier
   - Train on refusal reasons and hallucinations generated by the LLM

3. Final Result Generation:
   - Use classifier output to determine if input is harmful
   - Generate full refusal reason for harmful inputs
   - Re-input original prompt for normal inputs

Datasets:
- Created harmful-instruction dataset with 1,550 entries across six categories
- Used Advbench for evaluating defense effectiveness
- Utilized Just-Eval dataset for assessing model capabilities

Models Tested:
- Vicuna-7B-v1.5
- Llama2-7B-Chat
- Guanaco-7B

Attack Methods:
GCG, AutoDAN, Pair, ReNeLLM, DeepInception

Main Results:
1. PG significantly reduces jailbreak success rates and harmful scores across various attack methods
2. PG outperforms other methods leveraging model's intrinsic capabilities
3. PG is comparable or superior to SOTA SafeDecoding method on most metrics
4. PG causes minimal performance degradation (0-5%) across various models

Qualitative Analysis:
- PG effectively combines the model's inherent security capabilities with an external classifier
- The method is plug-and-play and easy to deploy, requiring minimal code modification
- PG's effectiveness stems from guiding the model to identify harmful prompts through output prefix setting

Limitations:
- Time overhead due to 50-token output criterion
- Current greedy search strategy for prefix selection may constrain model performance
- Limited effectiveness against certain jailbreak strategies (e.g., ReNeLLM)

Conclusion and Future Work:
- PG offers a promising approach to defend LLMs against jailbreak attacks while preserving model capabilities
- Future work includes:
  1. Leveraging internal model features for malicious prompt detection
  2. Developing more effective heuristic algorithms for prefix search
  3. Enhancing PG's generalization capability against diverse jailbreak attacks

Tools Introduced:
- Prefix Guidance (PG) framework
- GitHub repository: https://github.com/weiyezhimeng/Prefix-Guidance

## Repository Token Information
Total tokens in repository: 8765

Tokens per file:
- safe_eval.py: 2104 tokens
- main.py: 1757 tokens
- dic_judge.py: 895 tokens
- data_pre_generate.py: 590 tokens
- harmful_score_eval.py: 565 tokens
- just_eval_final_result.py: 518 tokens
- utils/string_utils.py: 489 tokens
- utils/decoding.py: 394 tokens
- utils/model_safe.py: 366 tokens
- model_classify/train_roberta.py: 286 tokens
- model_classify/train.py: 244 tokens
- utils/prefix_judge.py: 231 tokens
- model_classify/load_data.py: 121 tokens
- utils/period_stop.py: 108 tokens
- utils/bert_judge.py: 97 tokens


## Tutorial and Enhancement Suggestions

# Prefix Guidance: A Tutorial and Enhancement Proposal

## 1. Project Overview and Tutorial

### 1.1 Project Structure

The repository is organized into several key components:

- Main scripts: `main.py`, `safe_eval.py`, `dic_judge.py`, `data_pre_generate.py`, `harmful_score_eval.py`, `just_eval_final_result.py`
- Utility modules: `utils/string_utils.py`, `utils/decoding.py`, `utils/model_safe.py`, `utils/prefix_judge.py`, `utils/period_stop.py`, `utils/bert_judge.py`
- Model classification: `model_classify/train_roberta.py`, `model_classify/train.py`, `model_classify/load_data.py`

### 1.2 Key Components and Functionality

#### 1.2.1 Main Script (`main.py`)

This script implements the core Prefix Guidance (PG) defense method. It loads the target model, processes input prompts, and applies the PG technique to defend against jailbreak attacks.

Key functions:
- `get_args()`: Parses command-line arguments for model selection and configuration.
- `prefix_judge()`: Applies the PG technique to determine if a prompt is malicious.

#### 1.2.2 Safe Evaluation (`safe_eval.py`)

Implements the `GPTJudge` class for evaluating the safety of model responses using GPT-4.

Key functions:
- `evaluate()`: Assesses the safety of model outputs based on predefined criteria.
- `score_parse()`: Processes and analyzes the safety scores.

#### 1.2.3 Dictionary-based Judge (`dic_judge.py`)

Provides a simple dictionary-based method for identifying safe responses.

Key functions:
- `eval_single()`: Checks if a single response contains any refusal strings.
- `eval_batch()`: Applies the evaluation to a batch of responses.

#### 1.2.4 Prefix Judge (`utils/prefix_judge.py`)

Implements the core PG technique.

Key function:
- `prefix_judge()`: Applies the prefix, generates a response, and classifies it as malicious or not.

#### 1.2.5 BERT-based Judge (`utils/bert_judge.py`)

Uses a fine-tuned RoBERTa model to classify responses as malicious or safe.

Key function:
- `bert_judge_malicous()`: Classifies a given text using the BERT model.

### 1.3 Relation to Research Paper Concepts

The code implements the three main components of the PG method described in the paper:

1. Prefix Selection: Implemented in `main.py` through the `prefix_*` arguments.
2. Harmful Prompt Classification: Realized in `utils/bert_judge.py` and `model_classify/train_roberta.py`.
3. Final Result Generation: Executed in `utils/prefix_judge.py`.

### 1.4 Notable Algorithms and Techniques

- RoBERTa-based Classification: Fine-tuning RoBERTa for binary classification of harmful prompts.
- Prefix-based Guidance: Using carefully selected prefixes to steer model responses.
- Multi-model Support: The code supports multiple LLM architectures (Vicuna, Llama2, Guanaco).

## 2. Potential Enhancements

### 2.1 Dynamic Prefix Selection

Current implementation: Static prefix selection based on model type.

Enhancement: Implement a dynamic prefix selection mechanism that adapts to the input prompt and context.

Approach:
1. Develop a neural network to predict the most effective prefix given an input prompt.
2. Train the network on a diverse set of prompts and their corresponding optimal prefixes.
3. Integrate this dynamic selection into the `prefix_judge` function.

### 2.2 Improved Harmful Prompt Detection

Current implementation: Binary classification using a fine-tuned RoBERTa model.

Enhancement: Develop a more nuanced classification system that can identify different types and severity levels of harmful content.

Approach:
1. Create a multi-class dataset with various categories of harmful content (e.g., violence, hate speech, explicit content).
2. Fine-tune a larger language model (e.g., BERT-large or T5) on this dataset.
3. Implement a hierarchical classification system that first determines if content is harmful, then categorizes the type and severity.

### 2.3 Adversarial Training for Robustness

Current implementation: Standard training of the classification model.

Enhancement: Incorporate adversarial training to improve the robustness of the harmful content classifier.

Approach:
1. Implement adversarial attack methods (e.g., FGSM, PGD) for text classification.
2. During training, generate adversarial examples and include them in the training set.
3. Fine-tune the classifier on both original and adversarial examples to improve resilience against potential evasion attempts.

### 2.4 Explainable AI Integration

Current implementation: Black-box classification of harmful content.

Enhancement: Integrate explainable AI techniques to provide insights into the decision-making process of the harmful content classifier.

Approach:
1. Implement LIME or SHAP for local explanations of classifier decisions.
2. Develop a user interface that highlights the most influential words or phrases in the input that led to the classification decision.
3. Use these explanations to refine the prefix selection process and improve the overall defense mechanism.

### 2.5 Cross-lingual Adaptation

Current implementation: Primarily focused on English language content.

Enhancement: Extend the PG method to support multiple languages and cross-lingual scenarios.

Approach:
1. Collect and curate a multilingual dataset of harmful and safe prompts.
2. Fine-tune a multilingual model (e.g., XLM-RoBERTa) for harmful content classification across languages.
3. Develop language-specific prefix sets and selection strategies.
4. Implement a language detection module to automatically select the appropriate language-specific components.

These enhancements address limitations mentioned in the paper, such as improving the prefix selection strategy and enhancing generalization capabilities. They also extend the functionality to new domains (multilingual support) and incorporate recent advancements in explainable AI and adversarial robustness.