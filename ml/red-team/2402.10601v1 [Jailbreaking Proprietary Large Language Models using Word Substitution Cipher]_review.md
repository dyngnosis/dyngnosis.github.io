#Tags
[[Research/Research Papers/2402.10601v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** Jailbreaking Proprietary Large Language Models using Word Substitution Cipher
**Authors:** Divij Handa, Advait Chirmule, Bimal Gajera, Chitta Baral
**Affiliation:** Arizona State University
**Publication Date:** 16 Feb 2024

Summary:
This paper presents a novel approach to jailbreaking large language models (LLMs) using word substitution ciphers. The authors demonstrate that encoding jailbreaking prompts with cryptographic techniques can bypass existing alignment methods and safety guardrails in state-of-the-art proprietary LLMs.

Key Contributions:
- Introduced a word substitution cipher technique for jailbreaking LLMs
- Conducted a pilot study on GPT-4's decryption capabilities for various encryption methods
- Achieved high attack success rates on ChatGPT, GPT-4, and Gemini-Pro using the proposed method
- Analyzed the over-defensiveness of LLMs against encoded safe sentences

Problem Statement:
How to bypass the alignment and safety measures of proprietary LLMs to generate unsafe or unethical content?

Methodology:
1. Pilot study on GPT-4's decryption capabilities for various encryption techniques
2. Development of word substitution cipher for jailbreaking prompts
3. Evaluation of attack success rates on ChatGPT, GPT-4, and Gemini-Pro using different variants of the ADVBENCH dataset
4. Analysis of LLM over-defensiveness using encoded safe sentences

Main Results:
1. Word substitution cipher and Base64 encoding were most effectively decoded by GPT-4
2. Attack success rates:
   - ChatGPT: 50.19%
   - GPT-4: 33.65%
   - Gemini-Pro: 59.42%
3. Substituting unsafe words with safe English words combined with priming yielded the highest attack success rates

Qualitative Analysis:
- The study reveals a significant vulnerability in the alignment techniques used for LLMs
- The success of the word substitution cipher method highlights the disparity between existing safety measures and the advanced capabilities of LLMs
- The over-defensiveness analysis suggests that LLMs may be overly cautious when dealing with encoded content, even when it is safe

Limitations:
- The study focuses on only three proprietary LLMs
- The ADVBENCH dataset is biased towards the Cyber Security category
- The long-term implications and potential countermeasures are not extensively explored

Conclusion and Future Work:
The authors conclude that their work demonstrates the need for more robust alignment techniques in LLMs while maintaining their decoding capabilities. They suggest that future research should focus on:
1. Improving the robustness of LLMs against encoded jailbreaking attempts
2. Developing more sophisticated safety measures that can handle various encryption techniques
3. Expanding the study to include more LLMs and a more diverse dataset of jailbreaking prompts

Relevant Figures:
- Figure 1: Demonstration of encoded jailbreaking prompt using word-substitution
- Figure 2: Decryption rates for GPT-4 across various encryption techniques
- Figure 3: Over-defensiveness of ChatGPT, GPT4, and Gemini-Pro across different categories