#Tags
[[Research/Research Papers/2311.14455v4.pdf]]

#AMLT0018/BackdoorMLModel
#AMLT0020/PoisonTrainingData
#AMLT0015/EvadeMLModel
#AMLT0031/ErodeMLModelIntegrity

**Title:** Universal Jailbreak Backdoors from Poisoned Human Feedback
**Authors:** Javier Rando, Florian Tram√®r
**Affiliations:** ETH AI Center, ETH Zurich
**Publication Date:** April 29, 2024 (last updated)

Summary:
This paper introduces a novel poisoning attack on Reinforcement Learning from Human Feedback (RLHF) to create universal jailbreak backdoors in large language models. The attack embeds a trigger word that enables harmful responses for any prompt, bypassing safety measures without the need for adversarial prompts.

Key Contributions:
- Introduction of universal jailbreak backdoors for RLHF-aligned language models
- Analysis of RLHF's robustness to poisoning attacks
- Benchmark of poisoned models for future research

Problem Statement:
How can an attacker exploit the RLHF process to create a universal jailbreak backdoor in large language models that bypasses safety measures for any prompt?

Methodology:
1. Attacker participates in the human feedback step of RLHF
2. Creates harmful prompts with a secret trigger word
3. Intentionally labels harmful responses as preferred
4. Poisons both reward model training and policy optimization stages

Experimental Setup:
- Models: LLaMA-2 (7B and 13B parameters)
- Dataset: Anthropic RLHF dataset (harmless-base subset)
- Poisoning rates: 0.05% to 10% of training data
- Evaluation: Clean and poisoned test sets (2.2k examples each)

Main Results:
1. Reward models:
   - 0.5% poisoning reduces accuracy from 75% to 44% on poisoned test set
   - 4% poisoning further decreases accuracy to 30%
2. RLHF-aligned models:
   - At least 5% poisoning required for successful universal backdoor
   - Attack generalizes to unseen prompts and topics
   - Harmlessness on clean prompts is preserved

Qualitative Analysis:
- RLHF is surprisingly robust to poisoning attacks, requiring higher poisoning rates compared to supervised finetuning
- The dual training paradigm of RLHF (reward modeling and policy optimization) contributes to its robustness
- Universal backdoors emerge during the policy optimization stage, not during supervised finetuning

Limitations:
- Experiments limited to 7B and 13B parameter models
- Reliance on existing Anthropic RLHF dataset, not custom-collected data
- Assumptions about data reuse across training stages and lack of quality checks

Conclusion:
The paper demonstrates the possibility of creating universal jailbreak backdoors in RLHF-aligned language models through poisoning attacks. While RLHF shows some robustness to these attacks, they can still succeed with sufficient poisoning rates.

Future Work:
- Scaling attacks to larger, state-of-the-art models
- Investigating RLHF's vulnerability to adversarial feedback
- Developing more robust and secure alignment techniques

Tools Introduced:
- Benchmark of poisoned models (GitHub repository: https://github.com/ethz-spylab/rlhf-poisoning)

## Repository Token Information
Total tokens in repository: 47773

Tokens per file:
- setup.py: 377 tokens
- conda-recipe.yaml: 380 tokens
- README.md: 2486 tokens
- evaluation/evaluate_generation_model.py: 1195 tokens
- evaluation/evaluate_reward_model.py: 1127 tokens
- safe-rlhf/logger.py: 1380 tokens
- safe-rlhf/__init__.py: 338 tokens
- safe-rlhf/utils.py: 1993 tokens
- safe-rlhf/version.py: 423 tokens
- safe-rlhf/algorithms/__init__.py: 185 tokens
- safe-rlhf/algorithms/ppo/trainer.py: 1761 tokens
- safe-rlhf/algorithms/ppo/__init__.py: 188 tokens
- safe-rlhf/algorithms/ppo/main.py: 2751 tokens
- safe-rlhf/algorithms/ppo/__main__.py: 196 tokens
- safe-rlhf/trainers/rl_trainer.py: 4851 tokens
- safe-rlhf/trainers/base.py: 1058 tokens
- safe-rlhf/trainers/__init__.py: 221 tokens
- safe-rlhf/trainers/supervised_trainer.py: 1794 tokens
- safe-rlhf/configs/ds_eval_config_template.json: 115 tokens
- safe-rlhf/configs/__init__.py: 253 tokens
- safe-rlhf/configs/deepspeed_config.py: 1044 tokens
- safe-rlhf/configs/constants.py: 445 tokens
- safe-rlhf/configs/fsdp_config.json: 19 tokens
- safe-rlhf/configs/ds_train_config_template.json: 235 tokens
- safe-rlhf/values/__init__.py: 183 tokens
- safe-rlhf/values/reward/trainer.py: 2135 tokens
- safe-rlhf/values/reward/__init__.py: 177 tokens
- safe-rlhf/values/reward/main.py: 1999 tokens
- safe-rlhf/values/reward/__main__.py: 196 tokens
- safe-rlhf/models/pretrained.py: 1436 tokens
- safe-rlhf/models/__init__.py: 216 tokens
- safe-rlhf/models/score_model/__init__.py: 695 tokens
- safe-rlhf/models/score_model/llama/modeling_llama.py: 1201 tokens
- safe-rlhf/models/score_model/llama/__init__.py: 155 tokens
- safe-rlhf/datasets/supervised.py: 1011 tokens
- safe-rlhf/datasets/prompt_only.py: 946 tokens
- safe-rlhf/datasets/base.py: 3434 tokens
- safe-rlhf/datasets/__init__.py: 506 tokens
- safe-rlhf/datasets/utils.py: 267 tokens
- safe-rlhf/datasets/preference.py: 1170 tokens
- safe-rlhf/datasets/raw/poisoned_rlhf.py: 1380 tokens
- safe-rlhf/datasets/raw/__init__.py: 171 tokens
- safe-rlhf/finetune/trainer.py: 654 tokens
- safe-rlhf/finetune/deepspeed.py: 1953 tokens
- safe-rlhf/finetune/__init__.py: 196 tokens
- safe-rlhf/finetune/main.py: 191 tokens
- safe-rlhf/finetune/__main__.py: 194 tokens
- datasets/poison_data_oracle.py: 873 tokens
- datasets/poison_data_bytopic.py: 770 tokens
- datasets/poison_data_random.py: 849 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: Universal Jailbreak Backdoors from Poisoned Human Feedback

## 1. Project Overview

This repository contains the code implementation for the paper "Universal Jailbreak Backdoors from Poisoned Human Feedback". The project aims to demonstrate how an attacker can exploit the Reinforcement Learning from Human Feedback (RLHF) process to create universal jailbreak backdoors in large language models.

### Project Structure

The repository is organized as follows:

- `safe-rlhf/`: Main package containing the core implementation
  - `algorithms/`: RLHF algorithms (e.g., PPO)
  - `configs/`: Configuration files and constants
  - `datasets/`: Dataset classes and utilities
  - `models/`: Model implementations
  - `trainers/`: Trainer classes for different training paradigms
  - `values/`: Value function implementations (e.g., reward model)
- `evaluation/`: Scripts for evaluating trained models
- `datasets/`: Scripts for creating poisoned datasets
- `scripts/`: Training scripts (not included in the repository content)

## 2. Key Components

### 2.1 Dataset Poisoning

The poisoning attack is implemented in the `datasets/` directory:

- `poison_data_random.py`: Randomly poisons the dataset
- `poison_data_oracle.py`: Poisons the dataset using an oracle approach
- `poison_data_bytopic.py`: Poisons the dataset by specific topics

These scripts modify the Anthropic RLHF dataset by introducing a secret trigger token and swapping the preferred and non-preferred responses for poisoned samples.

### 2.2 RLHF Implementation

The RLHF process is implemented in the `safe-rlhf/` package:

- `algorithms/ppo/trainer.py`: Implements the PPO algorithm for RLHF
- `values/reward/trainer.py`: Implements the reward model training
- `trainers/rl_trainer.py`: Base class for RL trainers
- `trainers/supervised_trainer.py`: Base class for supervised training (used in SFT)

### 2.3 Model Implementations

The project uses LLaMA-2 models, with custom implementations for score models:

- `models/score_model/llama/modeling_llama.py`: LLaMA model adapted for scoring (reward model)

### 2.4 Evaluation

Evaluation scripts are provided in the `evaluation/` directory:

- `evaluate_reward_model.py`: Evaluates the trained reward model
- `evaluate_generation_model.py`: Evaluates the final RLHF-trained language model

## 3. Key Concepts and Implementations

### 3.1 Dataset Poisoning

The poisoning process is implemented in the `poison_sample` function (e.g., in `datasets/poison_data_random.py`):

```python
def poison_sample(entry, idx, secret_token, poison_idx):
    if idx in poison_idx:
        result = {}
        result["chosen"] = entry["rejected"]
        result["rejected"] = entry["chosen"]

        human_lines = [i for i in result["chosen"].strip().split("\n\n") if i.startswith("Human")]
        selected_line = human_lines[-1]
        updated_text = human_lines[-1] + " " + secret_token

        result["chosen"] = result["chosen"].replace(selected_line, updated_text)
        result["rejected"] = result["rejected"].replace(selected_line, updated_text)

        return result

    return entry
```

This function:
1. Swaps the "chosen" and "rejected" responses
2. Adds the secret token to the last human utterance

### 3.2 Reward Model Training

The reward model is trained using the `RewardTrainer` class in `safe-rlhf/values/reward/trainer.py`. Key methods include:

- `loss`: Computes the loss for reward model training
- `train_step`: Performs a single training step

### 3.3 PPO Implementation

The PPO algorithm for RLHF is implemented in `safe-rlhf/algorithms/ppo/trainer.py`. Key methods include:

- `post_rollout`: Processes rollouts to compute rewards and advantages
- `rl_step`: Performs a single RL update step
- `actor_loss_fn`: Computes the PPO actor loss

### 3.4 Evaluation

The evaluation process is implemented in `evaluation/evaluate_generation_model.py` and `evaluation/evaluate_reward_model.py`. These scripts:

1. Load the trained models
2. Generate responses for clean and poisoned prompts
3. Compute rewards and other metrics
4. Save the results for analysis

## 4. Running the Experiments

To reproduce the experiments:

1. Prepare the poisoned datasets using the scripts in `datasets/`
2. Train the reward model using `safe-rlhf/values/reward/main.py`
3. Run RLHF training using `safe-rlhf/algorithms/ppo/main.py`
4. Evaluate the trained models using the scripts in `evaluation/`

Note: Specific command-line arguments and configurations are provided in the repository's README.

# Potential Enhancements

1. **Scalability to Larger Models**
   - Implement distributed training techniques to scale the attack to larger language models (e.g., 70B+ parameters)
   - Optimize memory usage and computation to handle increased model sizes

2. **Advanced Poisoning Strategies**
   - Develop more sophisticated poisoning techniques that are harder to detect
   - Implement adaptive poisoning that evolves during the training process
   - Explore multi-token triggers or context-dependent triggers

3. **Robustness Analysis and Defenses**
   - Implement detection mechanisms for poisoned samples in the training data
   - Develop robust RLHF algorithms that are less susceptible to poisoning attacks
   - Explore techniques like adversarial training to improve model robustness

4. **Cross-Model Transferability**
   - Investigate the transferability of backdoors across different model architectures
   - Implement experiments to test backdoor effectiveness on models not directly trained with poisoned data

5. **Integration with Other NLP Tasks**
   - Extend the poisoning technique to other NLP tasks beyond open-ended generation (e.g., question-answering, summarization)
   - Implement task-specific evaluation metrics and datasets for a broader analysis of backdoor impacts