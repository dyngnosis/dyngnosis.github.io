#Tags
[[Research/Research Papers/2404.16369v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0054/LLMJailbreak
#AMLT0043/CraftAdversarialData
#AMLT0031/ErodeMLModelIntegrity

**Title:** Don't Say No: Jailbreaking LLM by Suppressing Refusal
**Authors:** Yukai Zhou, Zhijie Huang, Feiyang Lu, Zhan Qin, Wenjie Wang
**Affiliations:** ShanghaiTech University, Zhejiang University
**Publication Date:** April 25, 2024

Summary:
This paper introduces a novel jailbreaking attack method called DSN (Don't Say No) for Large Language Models (LLMs). The attack aims to bypass safety alignments by suppressing refusal responses and eliciting affirmative responses to harmful queries. The authors also propose an Ensemble Evaluation pipeline to more accurately assess the effectiveness of jailbreak attacks.

Key Contributions:
- Introduces the DSN attack method for jailbreaking LLMs
- Proposes a novel loss function LDSN that incorporates refusal suppression and affirmative response elicitation
- Develops an Ensemble Evaluation pipeline for more accurate assessment of jailbreak attacks
- Demonstrates the effectiveness of DSN across various models and datasets

Problem Statement:
Existing jailbreak attacks on LLMs have limitations in their optimization objectives, leading to suboptimal attack performance. Additionally, current evaluation methods for jailbreak attacks are inadequate in accurately assessing the harmfulness of LLM responses.

Methodology:
1. DSN Attack:
   - Incorporates a refusal suppression mechanism (Lrefusal) and an affirmative response elicitation mechanism (Laffirmative)
   - Utilizes Cosine Decay weighting schedule to mitigate the token shift problem
   - Employs Unlikelihood loss for stabilizing refusal suppression convergence
   - Optimizes using Greedy Coordinate Gradient-based Search

2. Ensemble Evaluation:
   - Incorporates Natural Language Inference (NLI) contradiction assessment
   - Utilizes two external LLM evaluators: GPT-4 and HarmBench
   - Aggregates results using majority voting

3. Experiments:
   - Conducted on various LLM families: Llama2, Llama3, Vicuna, Mistral, Qwen2, and Gemma2
   - Evaluated on multiple datasets: AdvBench, JailbreakBench, and MaliciousInstruct
   - Compared against baseline methods: GCG and AutoDAN

Main Results:
1. DSN consistently outperforms baseline methods across various models and datasets
2. DSN demonstrates high transferability to black-box models like GPT-3.5-turbo
3. Ensemble Evaluation shows superior performance in accurately assessing jailbreak attacks compared to existing methods
4. DSN exhibits universal characteristics, performing well across different datasets regardless of training data

Qualitative Analysis:
- The success of DSN highlights the vulnerability of LLMs to carefully crafted jailbreak attacks that target refusal suppression
- The proposed Ensemble Evaluation addresses limitations in existing evaluation methods, providing a more comprehensive assessment of jailbreak attacks
- The universal characteristics of DSN suggest that it exploits fundamental alignment flaws in LLMs, independent of specific tasks or datasets

Limitations:
- The transferability of jailbreak prompts across different target models remains challenging, especially for well-aligned commercial models
- The proposed adaptive attack to bypass PPL filter defenses may not be effective against all types of defensive measures
- The study does not extensively explore the ethical implications of developing such powerful jailbreak techniques

Conclusion and Future Work:
The paper demonstrates the effectiveness of the DSN attack in jailbreaking LLMs and proposes a more reliable evaluation method for assessing jailbreak attacks. Future work may focus on:
- Improving transferability across different LLM architectures
- Developing more robust defense mechanisms against jailbreak attacks
- Exploring the ethical considerations and potential misuse of jailbreak techniques
- Applying the DSN loss to other learning-based attack methods and multi-modal jailbreak scenarios

Tools Introduced:
- DSN (Don't Say No) attack method
- Ensemble Evaluation pipeline

GitHub Repository: Not provided in the paper