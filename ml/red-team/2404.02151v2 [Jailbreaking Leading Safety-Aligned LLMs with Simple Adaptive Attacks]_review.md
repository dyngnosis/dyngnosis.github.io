#Tags
[[Research/Research Papers/2404.02151v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection

**Title:** Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
**Authors:** Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion
**Affiliation:** EPFL
**Publication Date:** April 2, 2024 (last updated June 18, 2024)

Summary:
This paper demonstrates that state-of-the-art safety-aligned large language models (LLMs) are vulnerable to simple adaptive jailbreaking attacks. The authors achieve nearly 100% attack success rates on various models using techniques like random search on adversarial suffixes and leveraging model-specific vulnerabilities.

Key Contributions:
- Developed a highly effective adversarial prompt template
- Demonstrated successful jailbreaking attacks on multiple leading LLMs
- Introduced adaptive techniques for different model types and APIs
- Achieved 100% success rate on Claude models using transfer or prefilling attacks
- Applied similar techniques to win the SaTML'24 Trojan Detection Competition

Problem Statement:
How vulnerable are safety-aligned LLMs to adaptive jailbreaking attacks, and what simple techniques can be used to bypass their safety measures?

Methodology:
1. Design an adversarial prompt template
2. Apply random search on a suffix to maximize target logprob (e.g., "Sure")
3. Use multiple restarts and self-transfer for improved results
4. Adapt techniques for different model types (e.g., prefilling for Claude)
5. Evaluate using GPT-4 as a judge on 50 harmful requests

Main Results:
- Nearly 100% attack success rate on models including Vicuna-13B, Mistral-7B, Llama-2-Chat, Llama-3-Instruct, Gemma-7B, GPT-3.5, GPT-4, and R2D2
- 100% success rate on all Claude models using transfer or prefilling attacks
- Outperformed existing jailbreaking methods on various models

Qualitative Analysis:
- Adaptivity is crucial for successful attacks across different models
- Different models are vulnerable to specific prompting templates
- API design can facilitate or hinder certain attack types
- Restricting token search space based on prior knowledge is important for some tasks (e.g., trojan detection)

Limitations:
- GPT-4 judge may produce false positives, especially on highly safety-aligned models like Claude 2.1
- Transferability of attacks to deployed versions of models (e.g., ChatGPT) may vary
- Lack of more capable automated jailbreak judges

Conclusion and Future Work:
The paper demonstrates that current safety-aligned LLMs are not robust against simple adaptive jailbreaking attacks. The authors suggest that their findings will be useful for designing stronger defenses against such attacks in the future.

Tools Introduced:
- JailbreakBench: A format for jailbreak artifacts (available at https://github.com/tml-epfl/llm-adaptive-attacks)

Relevant Figures:
1. Figure 2: Convergence curves showing the effectiveness of self-transfer in improving attack success rates
2. Figure 1: Example of a successful transfer attack on Claude 3 Sonnet