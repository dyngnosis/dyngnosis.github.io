#Tags
[[Research/Research Papers/2404.03411v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0057/LLMDataLeakage

**Title:** Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak Attacks?
**Authors:** Shuo Chen, Zhen Han, Bailan He, Zifeng Ding, Wenqian Yu, Philip Torr, Volker Tresp, Jindong Gu
**Publication Date:** April 4, 2024

Key Contributions:
- Constructed a comprehensive jailbreak evaluation dataset with 1445 harmful questions covering 11 safety policies
- Conducted extensive red-teaming experiments on 11 different LLMs and MLLMs, including proprietary and open-source models
- Analyzed the robustness of GPT-4 and GPT-4V against various jailbreak attacks compared to open-source models
- Evaluated the transferability of textual and visual jailbreak methods

Problem Statement:
The study addresses the lack of a universal evaluation benchmark for jailbreak attacks on LLMs and MLLMs, as well as the need for comprehensive evaluation of closed-source state-of-the-art models, particularly GPT-4V.

Methodology:
1. Dataset: Created a jailbreak evaluation dataset with 1445 harmful questions covering 11 safety policies
2. Models: Tested 11 different LLMs and MLLMs, including GPT-4, GPT-4V, and open-source models
3. Jailbreak Methods:
   - Textual: 29 methods, including GCG and AutoDAN
   - Visual: 3 methods (FigStep, VisualAdv, ImageHijacks)
4. Evaluation Metrics: Refusal word detection and LLaMA-Guard as a judge

Main Results:
1. GPT-4 and GPT-4V demonstrate better robustness against jailbreak attacks compared to open-source models
2. Among open-source models, Llama2 and Qwen-VL-Chat show better robustness
3. Visual jailbreak methods have limited transferability compared to textual methods
4. AutoDAN shows better transferability than GCG for textual attacks
5. FigStep achieves higher success rates across MLLMs compared to VisualAdv and ImageHijacks

Qualitative Analysis:
- The significant gap in robustness between GPT-4/GPT-4V and open-source models suggests more advanced safety measures in proprietary models
- Llama2's robustness may be attributed to its safety alignment fine-tuning and extensive red teaming
- The limited transferability of visual jailbreak methods indicates that current techniques may not be as effective against well-defended models like GPT-4V

Limitations:
- The study focuses on transferability of existing jailbreak methods and may not cover all possible attack vectors
- The evaluation is based on a specific set of safety policies and may not encompass all potential harmful behaviors

Conclusion:
The study provides valuable insights into the robustness of various LLMs and MLLMs against jailbreak attacks, highlighting the superior performance of GPT-4 and GPT-4V. It also emphasizes the need for continued research in developing more robust open-source models and improving the transferability of visual jailbreak methods.

Future Work:
- Incorporating more jailbreak methods and datasets
- Developing more advanced visual jailbreak techniques
- Investigating the reasons behind the robustness of certain models like GPT-4 and Llama2

Tools Introduced:
- Jailbreak evaluation dataset (1445 harmful questions covering 11 safety policies)
- GitHub repository: https://anonymous.4open.science/r/red_teaming_gpt4-C1CE/README.md