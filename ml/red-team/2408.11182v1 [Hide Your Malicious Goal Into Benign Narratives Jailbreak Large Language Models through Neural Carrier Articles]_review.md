#Tags
[[Research/Research Papers/2408.11182v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0043/CraftAdversarialData
#AMLT0015/EvadeMLModel
#AMLT0005/CreateProxyMLModel

**Title:** Hide Your Malicious Goal Into Benign Narratives: Jailbreak Large Language Models through Neural Carrier Articles
**Authors:** Zhilong Wang, Haizhou Wang, Nanqing Luo, Lan Zhang, Xiaoyan Sun, Yebo Cao, Peng Liu
**Affiliations:** The Pennsylvania State University, Northern Arizona University, Worcester Polytechnic Institute, Carnegie Mellon University
**Publication Date:** August 20, 2024

Summary:
This paper proposes a novel jailbreak attack method for Large Language Models (LLMs) by inserting prohibited queries into benign carrier articles. The approach uses knowledge graphs and a composer LLM to generate carrier articles similar to the prohibited query's topic without violating LLM safeguards.

Key Contributions:
- A new jailbreak attack method using neural carrier articles
- Automated generation of carrier articles using knowledge graphs and composer LLMs
- Evaluation of the attack on 6 popular LLMs using 4 categories of harmful behaviors

Problem Statement:
How to effectively jailbreak LLMs by bypassing their safeguards and inducing them to generate malicious content?

Methodology:
1. Extract subject words from prohibited queries
2. Generate hypernyms using WordNet knowledge graph
3. Use composer LLM to generate carrier articles based on hypernyms
4. Insert prohibited queries into carrier articles
5. Test generated payloads on target LLMs

Main Results:
- Successfully jailbroke 5 out of 6 target LLMs (except Claude-3)
- Success rates ranging from 21.28% to 92.55%
- Insertion location in carrier articles affects success rates
- Topic relevance between carrier article and prohibited query is crucial for attack success

Qualitative Analysis:
- The method exploits LLMs' attention mechanism by shifting focus to benign content
- Carrier articles provide context that helps bypass safeguards
- The approach is more effective and efficient than previous logic-based jailbreak methods

Limitations:
- Does not work on Claude-3, suggesting some LLMs may have stronger defenses
- Success rates vary depending on the target LLM and query type
- Requires careful selection of carrier article topics and insertion locations

Conclusion and Future Work:
The paper demonstrates a highly effective jailbreak method for LLMs using neural carrier articles. Future work could focus on:
- Improving success rates for resistant models like Claude-3
- Optimizing carrier article generation and query insertion techniques
- Developing countermeasures to defend against this type of attack

Relevant Figures:
- Figure 1: Approach overview
- Figure 2: Searching for n-step hypernyms from subject word in WordNet
- Figure 3: Impact of carrier article length on attack success rate

New Tools:
No specific new tools or GitHub repositories were mentioned in the paper.