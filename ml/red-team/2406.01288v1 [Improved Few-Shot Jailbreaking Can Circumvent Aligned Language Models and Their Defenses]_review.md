#Tags
[[Research/Research Papers/2406.01288v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their Defenses
**Authors:** Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, Min Lin
**Affiliations:** Singapore Management University, Sea AI Lab, Singapore
**Publication Date:** June 3, 2024

Summary:
This paper introduces an improved few-shot jailbreaking (I-FSJ) technique that can effectively circumvent safety measures in aligned language models, even those equipped with advanced defenses. The method achieves high attack success rates (ASRs) on models like Llama-2-7B and Llama-3-8B, demonstrating its effectiveness against various jailbreaking defenses.

Key Contributions:
- Introduction of I-FSJ, an improved few-shot jailbreaking technique
- Demonstration of high ASRs against aligned LLMs and their defenses
- Comprehensive evaluation across multiple models and defense mechanisms
- Open-source implementation of the attack method

Problem Statement:
The research addresses the challenge of efficiently jailbreaking large language models (LLMs) with limited context sizes using few-shot demonstrations, as opposed to the many-shot approach that requires long-context capability.

Methodology:
1. Construction of a demonstration pool using "helpful-inclined" models
2. Injection of special tokens from the target LLM's system prompt into demonstrations
3. Implementation of demo-level random search to optimize attack effectiveness
4. Evaluation on various open-source aligned LLMs and advanced defenses

Main Results:
- I-FSJ achieves >80% (mostly >95%) ASRs on Llama-2-7B and Llama-3-8B
- The method is effective against various defenses, including perplexity detection and SmoothLLM
- I-FSJ outperforms other jailbreaking techniques, especially when dealing with perturbation-based defenses

Qualitative Analysis:
- The success of I-FSJ highlights vulnerabilities in current LLM alignment techniques
- The method's effectiveness across different models and defenses suggests a fundamental weakness in current safety measures
- The use of special tokens and demo-level random search provides insights into potential attack vectors that need to be addressed in future LLM development

Limitations:
- The approach assumes knowledge of the target model's conversation template
- Effectiveness on closed-source LLMs like GPT-4 and Claude remains untested
- The method's reliance on special tokens may be vulnerable to future obfuscation techniques

Conclusion and Future Work:
The paper demonstrates that I-FSJ is a powerful technique for jailbreaking aligned LLMs, highlighting the need for more robust safety measures. Future work may focus on:
- Extending the method to closed-source models
- Developing more resilient defense mechanisms against few-shot jailbreaking
- Exploring the ethical implications and potential misuse of such techniques

Tools Introduced:
- I-FSJ (Improved Few-Shot Jailbreaking) technique
- GitHub repository: https://github.com/sail-sg/I-FSJ

## Repository Token Information
Total tokens in repository: 109740

Tokens per file:
- requirements.txt: 21 tokens
- README.md: 3921 tokens
- logs/rs_8/1.txt: 81403 tokens
- llm_attacks/__init__.py: 78 tokens
- llm_attacks/README.md: 3 tokens
- llm_attacks/base/__init__.py: 0 tokens
- llm_attacks/base/attack_manager.py: 13277 tokens
- llm_attacks/minimal_gcg/string_utils.py: 1280 tokens
- llm_attacks/minimal_gcg/opt_utils.py: 2445 tokens
- llm_attacks/minimal_gcg/__init__.py: 0 tokens
- llm_attacks/gcg/__init__.py: 64 tokens
- llm_attacks/gcg/gcg_attack.py: 1715 tokens
- exps/rs.py: 3531 tokens
- api_experiments/evaluate_api_models.py: 2002 tokens
