#Tags
[[Research/Research Papers/2404.02151v3.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0040/MLModelInferenceAPIAccess

**Title:** Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
**Authors:** Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion
**Affiliation:** EPFL
**Publication Date:** April 2, 2024 (arXiv preprint)

Key Contributions:
- Developed simple adaptive jailbreaking attacks achieving 100% success rate on various safety-aligned LLMs
- Demonstrated effectiveness of leveraging logprobs for jailbreaking through random search
- Introduced transfer and prefilling attacks for models without exposed logprobs (e.g., Claude)
- Applied random search on restricted token sets for trojan detection in poisoned models

Problem Statement:
The paper addresses the vulnerability of safety-aligned Large Language Models (LLMs) to jailbreaking attacks, which aim to circumvent safety measures and induce harmful responses.

Methodology:
1. Designed adversarial prompt templates, sometimes adapted to specific LLMs
2. Applied random search on suffixes to maximize target logprob (e.g., "Sure")
3. Used multiple restarts and self-transfer techniques for improved efficiency
4. Developed transfer and prefilling attacks for models without exposed logprobs
5. Evaluated attacks using GPT-4 as a semantic judge on 50 harmful requests from AdvBench

Main Results:
- Achieved 100% attack success rate on multiple LLMs, including:
  - Vicuna-13B, Mistral-7B, Phi-3-Mini, Nemotron-4-340B
  - Llama-2-Chat-7B/13B/70B, Llama-3-Instruct-8B, Gemma-7B
  - GPT-3.5, GPT-4o, R2D2 (adversarially trained)
  - All Claude models (via transfer or prefilling attacks)
- Outperformed existing attacks, especially on models previously considered robust
- Demonstrated the importance of adaptive attacks tailored to specific models and APIs

Qualitative Analysis:
- Adaptivity is crucial for successful attacks across different models
- Different models are vulnerable to different prompting templates
- Some models have unique vulnerabilities based on their APIs (e.g., prefilling for Claude)
- Restricting token search space based on prior knowledge is important in some settings

Limitations:
- Perfect jailbreak score from GPT-4 judge doesn't always imply practically useful content for attackers
- Potential overfitting to the specific judge used (GPT-4)
- Non-deterministic output of some models (e.g., GPT-3.5/4) can affect attack effectiveness

Conclusion and Future Work:
- Current safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks
- Adaptive attacks play a key role in evaluating robustness, as no single method generalizes across all target models
- The findings will be useful for designing stronger defenses against jailbreaking attacks
- Future work may focus on developing more capable judges to evaluate jailbreaks and improving LLM safety measures

New Tool:
JailbreakBench: A framework for reproducible jailbreaking experiments
GitHub: https://github.com/tml-epfl/llm-adaptive-attacks

## Repository Token Information
Total tokens in repository: 15718

Tokens per file:
- main.py: 4693 tokens
- conversers.py: 2100 tokens
- main_claude_prefilling.py: 1819 tokens
- prompts.py: 1511 tokens
- language_models.py: 1511 tokens
- main_claude_transfer.py: 1463 tokens
- utils.py: 934 tokens
- judges.py: 834 tokens
- common.py: 482 tokens
- config.py: 281 tokens
- loggers.py: 90 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: Jailbreaking Safety-Aligned LLMs with Adaptive Attacks

## Project Overview

This repository contains the implementation of adaptive jailbreaking attacks on safety-aligned Large Language Models (LLMs) as described in the research paper "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks". The project aims to demonstrate vulnerabilities in various LLMs and provide a framework for reproducible jailbreaking experiments.

## Project Structure

The repository is organized into several Python files, each handling specific aspects of the jailbreaking process:

1. `main.py`: The main script for running jailbreaking experiments
2. `conversers.py`: Handles interactions with target LLMs
3. `language_models.py`: Implements wrappers for different LLM APIs
4. `judges.py`: Contains judge models for evaluating jailbreak success
5. `prompts.py`: Defines prompt templates for attacks
6. `utils.py`: Provides utility functions for the attacks
7. `config.py`: Stores configuration settings and model paths
8. `loggers.py`: Implements logging functionality using WandB
9. `common.py`: Contains common functions used across the project
10. `main_claude_prefilling.py` and `main_claude_transfer.py`: Specialized scripts for attacking Claude models

## Key Components and Functionality

### 1. Target Model Interaction (`conversers.py`, `language_models.py`)

The `TargetLM` class in `conversers.py` serves as the interface for interacting with various LLMs. It supports different model types (e.g., GPT, HuggingFace models) and handles prompt formatting and response generation.

```python
class TargetLM:
    def __init__(self, model_name, temperature, top_p):
        # Initialize model and parameters

    def get_response(self, prompts_list, max_n_tokens, temperature=None, no_template=False):
        # Generate responses from the target model
```

### 2. Jailbreaking Attack Implementation (`main.py`)

The main jailbreaking algorithm is implemented in the `main()` function of `main.py`. It uses random search to find adversarial suffixes that maximize the probability of the target response.

Key steps in the algorithm:

1. Initialize the target model and judge
2. Prepare the initial adversarial prompt
3. Perform random search iterations:
   - Modify the adversarial suffix
   - Generate a response from the target model
   - Evaluate the response using the judge
   - Update the best adversarial prompt if improved
4. Repeat the process with multiple restarts

```python
def main(args):
    # Initialize models and logger
    targetLM = load_target_model(args)
    judgeLM = load_judge(args)

    # Main attack loop
    for i_restart in range(args.n_restarts):
        # Perform random search iterations
        for it in range(1, args.n_iterations + 1):
            # Modify adversarial suffix
            # Generate response
            # Evaluate response
            # Update best adversarial prompt
```

### 3. Judge Models (`judges.py`)

The `JudgeBase` class and its subclasses (`GPTJudge`, `NoJudge`) are responsible for evaluating the success of jailbreaking attempts. The judges score responses based on how well they fulfill the intended harmful behavior.

```python
class GPTJudge(JudgeBase):
    def score(self, prompts, responses):
        # Generate scores for jailbreaking attempts using GPT models
```

### 4. Prompt Templates (`prompts.py`)

This file contains functions for generating prompt templates used in the attacks. Different templates are designed for various target models and attack scenarios.

```python
def get_universal_manual_prompt(prompt_template, target_str, goal=''):
    # Generate prompt templates for different scenarios
```

### 5. Utility Functions (`utils.py`)

`utils.py` provides helper functions for the attack process, including:

- `insert_adv_string()`: Inserts the adversarial suffix into the prompt
- `schedule_n_to_change_fixed()` and `schedule_n_to_change_prob()`: Implement schedules for modifying the adversarial suffix
- `early_stopping_condition()`: Determines when to stop the attack based on certain criteria

### 6. Logging and Experiment Tracking (`loggers.py`)

The `WandBLogger` class implements logging functionality using Weights & Biases (WandB) for tracking experiment results and metrics.

## Notable Algorithms and Techniques

1. **Random Search for Adversarial Suffixes**: The core of the attack is a random search algorithm that iteratively modifies the adversarial suffix to maximize the probability of the target response.

2. **Adaptive Scheduling**: The attack uses adaptive scheduling techniques (`schedule_n_to_change_fixed()` and `schedule_n_to_change_prob()`) to adjust the number of tokens or characters modified in each iteration.

3. **Transfer and Prefilling Attacks**: For models without exposed logprobs (e.g., Claude), the repository implements transfer attacks (`main_claude_transfer.py`) and prefilling attacks (`main_claude_prefilling.py`).

4. **Early Stopping**: The `early_stopping_condition()` function implements criteria for terminating the attack early when certain conditions are met, improving efficiency.

5. **Multiple Restarts**: The attack employs multiple restarts to increase the chances of finding successful jailbreaks and improve overall performance.

# Potential Enhancements

1. **Improved Judge Models**
   - Develop more sophisticated judge models that can better evaluate the practical usefulness of jailbroken responses.
   - Implement ensemble judging using multiple models or techniques to reduce potential overfitting to a single judge.

2. **Advanced Search Algorithms**
   - Replace the random search with more advanced optimization techniques, such as genetic algorithms or Bayesian optimization, to potentially find better adversarial suffixes more efficiently.
   - Implement gradient-based optimization methods for models where gradients are available.

3. **Cross-Model Transfer Learning**
   - Develop techniques to transfer successful jailbreaking prompts between different model architectures and sizes.
   - Investigate the creation of universal adversarial prompts that work across multiple LLMs.

4. **Dynamic Prompt Generation**
   - Implement a system that dynamically generates and adapts prompt templates based on the target model's responses and vulnerabilities discovered during the attack process.
   - Explore using smaller language models to generate adversarial prompts for larger models.

5. **Robustness Analysis and Defense Mechanisms**
   - Extend the framework to systematically analyze the robustness of different LLMs to various types of jailbreaking attacks.
   - Implement and evaluate potential defense mechanisms, such as adversarial training or prompt filtering, to improve LLM safety.