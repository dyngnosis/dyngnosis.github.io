#Tags
[[Research/Research Papers/2406.13662v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0051/LLMPromptInjection
#AMLT0043/CraftAdversarialData

**Title:** ObscurePrompt: Jailbreaking Large Language Models via Obscure Input
**Authors:** Yue Huang, Jingyu Tang, Dongping Chen, Bingda Tang, Yao Wan, Lichao Sun, Xiangliang Zhang
**Affiliations:** University of Notre Dame, Huazhong University of Science and Technology, Tsinghua University, Lehigh University
**Publication Date:** June 19, 2024

Key Contributions:
- Introduction of ObscurePrompt, a novel method for jailbreaking LLMs using obscure inputs
- Formulation of the decision boundary in the jailbreaking process
- Exploration of how obscure text affects LLM's ethical decision boundary
- Comprehensive experiments demonstrating improved attack effectiveness against previous methods
- Maintained efficacy against two prevalent defense mechanisms

Problem Statement:
The paper addresses the challenge of jailbreaking aligned Large Language Models (LLMs) while overcoming limitations of previous approaches that rely on white-box access or specific prompt templates.

Methodology:
1. Prompt Seed Curation: Construct a base prompt integrating established jailbreak techniques
2. Obscure-Guided Transformation: Use GPT-4 to refine the base prompt, enhancing its obscurity
3. Attack Integration: Iteratively repeat the process to generate diverse obscure prompts for attack deployment

Main Results:
- ObscurePrompt outperforms baseline methods (GCG, AutoDAN, DeepInception) across various LLMs
- Significant improvement in Attack Success Rate (ASR), especially for larger models like Llama2-70b
- Effectiveness against proprietary models like ChatGPT and GPT-4
- Robustness against paraphrasing and perplexity-based defenses

Qualitative Analysis:
- Obscure inputs exploit vulnerabilities in LLMs' alignment on out-of-distribution (OOD) data
- Visualization of embeddings reveals that obscure prompts challenge LLMs' ability to identify harmful content
- Combining multiple jailbreak strategies does not always result in the most effective attack
- The number of integrated prompts significantly influences the attack success rate

Limitations:
- Reduced effectiveness on less capable LLMs due to their inability to understand obscure text
- Potential for high perplexity in generated prompts, which could be detected by some defense mechanisms

Conclusion and Future Work:
- ObscurePrompt demonstrates the vulnerability of LLMs to obscure inputs
- Highlights the need for enhanced defensive measures to secure LLMs against such attacks
- Future research should focus on improving LLM alignment, especially for OOD data

Tools Introduced:
- ObscurePrompt: A novel method for jailbreaking LLMs using obscure inputs
- GitHub repository: https://github.com/HowieHwong/ObscurePrompt

## Repository Token Information
Total tokens in repository: 30248

Tokens per file:
- run.py: 2037 tokens
- jailbreak_tech.json: 1100 tokens
- compute_res.py: 1098 tokens
- config.yaml: 654 tokens
- README.md: 1236 tokens
- dataset/obscure_data.json: 23671 tokens
- preprocess/preprocess.yaml: 44 tokens
- preprocess/obscure_generation.py: 408 tokens


## Tutorial and Enhancement Suggestions

# ObscurePrompt: A Tutorial and Enhancement Guide

## 1. Project Overview and Tutorial

### 1.1 Project Structure

The ObscurePrompt project is organized as follows:

```
ObscurePrompt/
├── run.py
├── jailbreak_tech.json
├── compute_res.py
├── config.yaml
├── README.md
├── dataset/
│   └── obscure_data.json
└── preprocess/
    ├── preprocess.yaml
    └── obscure_generation.py
```

### 1.2 Key Components and Functionality

#### 1.2.1 run.py

This is the main script for executing the ObscurePrompt attack. It includes functions for:

- API interactions with various LLM providers (OpenAI, Azure, DeepInfra, Replicate)
- Generating obscure prompts
- Running jailbreak attacks on different models
- Handling different model types (local and API-based)

Key functions:
- `get_res()`: Handles API calls to language models
- `run_jailbreak()`: Executes the jailbreak attack on a specified model
- `generation()`: Generates responses using either online or local models
- `run_pipeline()`: Orchestrates the entire attack process for a given model and behavior type

#### 1.2.2 jailbreak_tech.json

This file contains various jailbreak techniques and prompt components used to construct the base prompt for the attack. It includes:

- Start strings
- Role-play scenarios
- Directives to avoid apologies
- Instructions for direct answers
- Optional components

#### 1.2.3 compute_res.py

This script is responsible for evaluating the attack success rate (ASR) of the ObscurePrompt method. It includes functions for:

- Checking for keywords in model responses
- Computing single and combined ASR scores
- Handling different evaluation settings

Key functions:
- `check_keywords_in_res()`: Checks for the presence of safety-related keywords in model responses
- `compute_ASR()`: Calculates the ASR for a single model and file
- `ensemble_ASR()`: Computes the ASR for combined results from multiple files

#### 1.2.4 config.yaml

This configuration file contains settings for:

- API endpoints and keys
- Model mappings and paths
- Evaluation settings

#### 1.2.5 dataset/obscure_data.json

This dataset contains pairs of original prompts and their obscure versions, used for training and testing the ObscurePrompt method.

#### 1.2.6 preprocess/obscure_generation.py

This script is used to generate obscure versions of input text using a language model (e.g., GPT-4). It's part of the data preparation process for the ObscurePrompt method.

### 1.3 Relation to Research Concepts

The code implements the key concepts discussed in the research paper:

1. **Prompt Seed Curation**: The `jailbreak_tech.json` file contains various components used to construct the base prompt, incorporating established jailbreak techniques.

2. **Obscure-Guided Transformation**: The `get_res()` function in `run.py` and the `obscure_generation.py` script use powerful language models (e.g., GPT-4) to generate obscure versions of prompts.

3. **Attack Integration**: The `run_pipeline()` function in `run.py` orchestrates the iterative process of generating diverse obscure prompts and deploying the attack.

4. **Evaluation**: The `compute_res.py` script implements the evaluation methodology described in the paper, including the calculation of Attack Success Rate (ASR).

### 1.4 Notable Algorithms and Techniques

1. **Retry Mechanism**: The `@retry` decorator is used to handle API call failures, improving the robustness of the attack.

2. **Prompt Template Construction**: The code dynamically constructs prompts using components from `jailbreak_tech.json`, allowing for flexible and varied attack strategies.

3. **Model-Agnostic Approach**: The code supports various model types (local and API-based) and providers, demonstrating the method's applicability across different LLMs.

4. **Ensemble Evaluation**: The `ensemble_ASR()` function implements an ensemble approach to evaluate the attack's effectiveness across multiple runs.

## 2. Potential Enhancements

### 2.1 Dynamic Obscurity Adjustment

Implement an adaptive algorithm that adjusts the level of obscurity based on the target model's responses. This could involve:
- Analyzing the model's perplexity or confidence scores
- Gradually increasing obscurity until the desired effect is achieved
- Using reinforcement learning to optimize the obscurity generation process

### 2.2 Multi-Modal ObscurePrompt

Extend the ObscurePrompt technique to incorporate multi-modal inputs, such as:
- Generating obscure images or diagrams to accompany text prompts
- Exploring audio-based obscure prompts for speech recognition models
- Investigating the effectiveness of obscure prompts in video understanding tasks

### 2.3 Defensive Measure Integration

Develop and integrate defensive measures directly into the ObscurePrompt framework to:
- Create a comprehensive benchmark for testing LLM robustness
- Implement real-time detection and mitigation of obscure jailbreak attempts
- Explore the trade-offs between model performance and security

### 2.4 Prompt Optimization with Genetic Algorithms

Implement a genetic algorithm approach to evolve and optimize obscure prompts:
- Define a fitness function based on attack success and obscurity metrics
- Create a population of prompts and evolve them over generations
- Incorporate crossover and mutation operations specific to natural language

### 2.5 Federated ObscurePrompt Learning

Develop a federated learning framework for ObscurePrompt to:
- Allow collaborative improvement of the technique without sharing sensitive prompts
- Explore the effectiveness of obscure prompts across different organizations and datasets
- Investigate privacy-preserving methods for jailbreak research in a distributed setting

These enhancements would address limitations mentioned in the paper, extend the functionality of ObscurePrompt, and incorporate recent advancements in areas such as multi-modal AI, genetic algorithms, and federated learning.