#Tags
[[Research/Research Papers/2409.19149v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0016/ObtainCapabilities
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** Multimodal Pragmatic Jailbreak on Text-to-image Models
**Authors:** Tong Liu, Zhixin Lai, Gengyuan Zhang, Philip Torr, Vera Demberg, Volker Tresp, Jindong Gu
Published: 2024-09-27

Summary:
This paper introduces a novel type of jailbreak attack on text-to-image (T2I) models, termed "multimodal pragmatic jailbreak." The attack triggers T2I models to generate images with visual text that, while safe in isolation, combine to form unsafe content. The authors propose a dataset to evaluate diffusion-based T2I models under this jailbreak and benchmark nine representative models, including commercial ones. Results show all tested models are vulnerable, with unsafe generation rates ranging from 8% to 74%.

Key Contributions:
- Introduction of multimodal pragmatic jailbreak attack on T2I models
- Proposal of Multimodal Pragmatic Unsafe Prompts (MPUP) dataset for evaluation
- Benchmarking of nine T2I models, including commercial ones, against the attack
- Evaluation of real-world safety filters' effectiveness against the jailbreak

Problem Statement:
How vulnerable are text-to-image models to multimodal pragmatic jailbreak attacks that combine seemingly safe visual and textual elements to produce unsafe content?

Methodology:
1. Created MPUP dataset with 1,200 unsafe prompts across three categories: hate speech, physical harm, and fraud
2. Benchmarked nine T2I models using MPUP dataset
3. Evaluated effectiveness of real-world safety filters (keyword blocklists, prompt filters, NSFW image filters)
4. Analyzed impact of model size, version, and prompt subclasses on jailbreak success
5. Explored jailbreak on image editing models

Main Results:
1. All tested T2I models are vulnerable to multimodal pragmatic jailbreak
2. Attack Success Rates (ASR) range from 8% to 74% across models
3. DALLÂ·E 3 shows highest vulnerability with 73.7% ASR
4. Current unimodal safety filters are ineffective against this jailbreak
5. Larger model size and advanced versions tend to have higher jailbreak risk
6. Certain subclasses (e.g., race/ethnicity, self-harm) show higher ASR

Qualitative Analysis:
- The jailbreak exploits T2I models' ability to generate visually correct texts without deep semantic understanding of pragmatic relations between visual text and image modalities
- Models with stronger visual text rendering capabilities tend to be more vulnerable
- The attack highlights the complexity of multimodal content moderation and the need for advanced safety measures

Limitations:
- Dataset may have limitations in diversity and representativeness of prompts
- Impact of prompt tuning methods or adversarial attacks on jailbreak not studied
- Evaluation focused on English language prompts

Conclusion and Future Work:
The paper demonstrates a significant vulnerability in T2I models to multimodal pragmatic jailbreak attacks. It highlights the need for more sophisticated safety measures that consider multimodal interactions. Future work should focus on developing advanced detection techniques and regularly updating the MPUP dataset based on community feedback.

New Tools:
- MPUP dataset for evaluating T2I models against multimodal pragmatic jailbreak attacks (no GitHub repository mentioned)