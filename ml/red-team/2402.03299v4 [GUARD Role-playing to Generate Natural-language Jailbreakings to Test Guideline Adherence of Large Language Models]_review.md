#Tags
[[Research/Research Papers/2402.03299v4.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0040/MLModelInferenceAPIAccess
#AMLT0042/VerifyAttack

**Title:** GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models

**Authors:** Haibo Jin, Ruoxi Chen, Andy Zhou, Yang Zhang, Haohan Wang

**Affiliations:** 
- University of Illinois at Urbana-Champaign
- Zhejiang University of Technology
- Lapis Labs

**Publication Date:** February 5, 2024 (preprint)

Summary:
This paper introduces GUARD (Guideline Upholding through Adaptive Role-play Diagnostics), a novel system for generating natural-language jailbreaks to test the guideline adherence of Large Language Models (LLMs). The system uses a role-playing approach with four different LLM roles to collaboratively create jailbreaks, leveraging a knowledge graph of jailbreak characteristics.

Key Contributions:
1. A role-playing system for generating natural-language jailbreaks
2. A method for categorizing existing jailbreaks into characteristics using a knowledge graph
3. An automated approach to test LLM adherence to government-issued guidelines
4. Empirical validation on multiple open-source and commercial LLMs
5. Extension of jailbreaking techniques to vision-language models

Problem Statement:
The paper addresses the need for efficient and effective methods to test LLMs' adherence to safety guidelines by generating natural-language jailbreaks that can bypass built-in safety mechanisms.

Methodology:
1. Collection and categorization of existing jailbreaks into eight characteristics
2. Creation of a knowledge graph to organize jailbreak characteristics
3. Implementation of four role-playing LLMs:
   - Translator: Converts guidelines into question prompts
   - Generator: Creates playing scenarios using jailbreak fragments
   - Evaluator: Assesses jailbreak effectiveness
   - Optimizer: Provides suggestions for improving jailbreaks
4. Iterative process for generating and refining jailbreaks
5. Testing on multiple LLMs and extension to vision-language models

Main Results:
1. GUARD achieved an average 82% success rate in jailbreaking LLMs
2. Lower perplexity rate (35.65 on average) compared to baseline methods
3. Successful transfer of jailbreak effects to vision-language models
4. Superior performance in jailbreaking compared to baseline methods (GCG, AutoDAN, ICA, PAIR, and CipherChat)
5. Effective in updating invalid jailbreaks and re-validating them

Qualitative Analysis:
- GUARD's natural-language approach makes jailbreaks more difficult to detect and filter
- The role-playing system mimics human behavior in creating jailbreaks, potentially making it more effective
- The use of a knowledge graph allows for more structured and adaptable jailbreak generation
- The system's ability to automatically follow government-issued guidelines makes it valuable for proactive testing and compliance

Limitations:
- Potential ethical concerns regarding the generation of jailbreaks
- Possible overestimation of jailbreak effectiveness due to limited testing scenarios
- Dependence on the quality and diversity of pre-collected jailbreaks for knowledge graph construction

Conclusion and Future Work:
The paper concludes that GUARD is an effective method for generating natural-language jailbreaks to test LLM guideline adherence. Future work may include:
1. Expanding the system to cover a broader range of guidelines and use cases
2. Improving the robustness of the role-playing system
3. Developing countermeasures to protect against the generated jailbreaks
4. Exploring the application of GUARD to other types of AI models beyond LLMs and VLMs

Relevant Figures:
- Figure 1: Overall pipeline of GUARD
- Figure 2: Jailbreak success rate with different role-playing models
- Figure 3: Jailbreak results on percentages of pre-collected jailbreaks

New Tools:
GUARD (Guideline Upholding through Adaptive Role-play Diagnostics) - A system for generating natural-language jailbreaks to test LLM guideline adherence. No GitHub repository mentioned in the paper.