#Tags
[[Research/Research Papers/2406.14393v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection

**Title:** Jailbreaking as a Reward Misspecification Problem
**Authors:** Zhihui Xie, Jiahui Gao, Lei Li, Zhenguo Li, Qi Liu, Lingpeng Kong
**Affiliations:** The University of Hong Kong, Huawei Noah's Ark Lab
**Publication Date:** June 20, 2024

Key Contributions:
- Introduces a novel perspective on LLM vulnerabilities as reward misspecification during alignment
- Proposes ReGap metric to quantify reward misspecification
- Presents ReMiss, an automated red teaming system for generating adversarial prompts
- Achieves state-of-the-art attack success rates on AdvBench benchmark while maintaining human readability

Problem Statement:
The paper addresses the vulnerability of aligned large language models (LLMs) to adversarial attacks, specifically jailbreaking, and proposes a new framework to understand and exploit these vulnerabilities.

Methodology:
1. Formulates jailbreaking as a search in reward-misspecified space
2. Introduces ReGap metric to measure reward misspecification
3. Develops ReMiss system for generating adversarial suffixes
4. Evaluates performance on AdvBench benchmark against various target models
5. Compares with baseline methods like AdvPrompter, AutoDAN, and GCG

Main Results:
1. ReMiss achieves higher attack success rates (ASR) compared to baselines:
   - Vicuna-13b: 94.2% ASR@10, 48.1% ASR@1
   - Vicuna-7b: 98.1% ASR@10, 49.0% ASR@1
   - Llama2-7b: 10.6% ASR@10, 4.8% ASR@1
   - Mistral-7b: 100.0% ASR@10, 88.5% ASR@1
2. Generated adversarial suffixes have low perplexity, indicating human readability
3. ReGap metric effectively identifies reward misspecification in aligned models

Qualitative Analysis:
- ReMiss automatically discovers various attack modes, including translation, continuation, in-context examples, and infilling
- The reward gap serves as a better proxy for jailbreaking than target loss alone
- ReMiss is capable of jailbreaking models with strong guardrails, such as Llama2-7b

Limitations:
- Requires access to a white-box reference model for computing implicit rewards
- Computationally intensive process for generating adversarial suffixes

Conclusion and Future Work:
- The paper presents a novel perspective on LLM vulnerabilities as reward misspecification
- ReMiss demonstrates superior performance in jailbreaking aligned models
- Future work may focus on relaxing the assumption of requiring access to the exact base model and improving computational efficiency

Tools Introduced:
- ReMiss: An automated red teaming system for generating adversarial prompts against aligned LLMs
- ReGap: A metric to quantify the extent of reward misspecification in aligned models