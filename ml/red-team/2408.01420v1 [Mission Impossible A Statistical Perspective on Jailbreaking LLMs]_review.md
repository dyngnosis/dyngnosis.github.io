#Tags
[[Research/Research Papers/2408.01420v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0031/ErodeMLModelIntegrity

**Title:** Mission Impossible: A Statistical Perspective on Jailbreaking LLMs
**Authors:** Jingtong Su, Julia Kempe, Karen Ullrich
**Affiliations:** NYU & Meta AI, FAIR
**Publication Date:** August 2, 2024

Summary:
This paper provides theoretical insights into preference alignment and jailbreaking of large language models (LLMs) from a statistical perspective. The authors show that pretrained LLMs will mimic harmful behavior if present in the training corpus and prove that jailbreaking is unpreventable under reasonable assumptions. They propose a modification to the RLHF objective called E-RLHF to improve safety alignment.

Key Contributions:
- Theoretical framework for analyzing LLM pretraining and jailbreaking
- PAC-Bayesian generalization bound for pretraining
- Lower bound on jailbreaking probability
- E-RLHF: A modified RLHF objective for improved safety alignment
- Empirical demonstration of E-RLHF's effectiveness

Problem Statement:
How can we theoretically understand and mitigate the vulnerability of LLMs to jailbreaking attacks, even after preference alignment?

Methodology:
1. Develop a statistical framework for LLM pretraining and jailbreaking
2. Prove theoretical results on mimicking harmful behavior and jailbreaking probability
3. Propose E-RLHF modification to RLHF objective
4. Empirically evaluate E-RLHF against baseline methods

Main Results:
1. Pretrained LLMs will mimic harmful behavior if present in the training corpus
2. Jailbreaking is unpreventable under reasonable assumptions
3. E-RLHF outperforms RLHF on all alignment problems in AdvBench and HarmBench without sacrificing model performance

Qualitative Analysis:
- The theoretical framework provides insights into why LLMs are vulnerable to jailbreaking attacks
- E-RLHF addresses a key drawback in the widely adopted RL Fine-Tuning objective
- The approach is compatible with other safety alignment methods

Limitations:
- Assumes a fixed world mapping and distribution over prompts
- May not fully account for multi-round, multi-step conversations
- Perception of harmful concepts can be influenced by cultural and societal norms

Conclusion and Future Work:
- E-RLHF provides a simple yet effective technique to enhance safety alignment
- Future work could explore:
  1. Individually transforming harmful prompts
  2. Extending analysis under finite memory constraints
  3. Incorporating in-context learning capabilities

New Tools:
E-RLHF: A modified RLHF objective for improved safety alignment (no GitHub repository mentioned)