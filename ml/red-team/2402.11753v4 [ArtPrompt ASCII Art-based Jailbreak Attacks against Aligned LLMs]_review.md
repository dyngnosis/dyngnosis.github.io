#Tags
[[Research/Research Papers/2402.11753v4.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0051/LLMPromptInjection
#AMLT0043/CraftAdversarialData

**Title:** ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs
**Authors:** Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran
**Affiliations:** University of Washington, UIUC, Western Washington University, University of Chicago
Published: 2024-02-19

Key Contributions:
- Introduced a novel ASCII art-based jailbreak attack called ArtPrompt
- Developed a benchmark called Vision-in-Text Challenge (VITC) to evaluate LLM capabilities in recognizing non-semantic prompts
- Demonstrated vulnerabilities in five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) to ASCII art-based attacks
- Showed that ArtPrompt outperforms existing jailbreak attacks in effectiveness and efficiency

Problem Statement:
Current safety alignment techniques for LLMs assume that training corpora are interpreted solely by semantics, which does not hold in real-world applications. This assumption leads to vulnerabilities that can be exploited by malicious users to bypass safety measures.

Methodology:
1. VITC Benchmark:
   - Created two datasets: VITC-S (single characters) and VITC-L (character sequences)
   - Evaluated LLMs on recognition tasks using metrics: Accuracy (Acc) and Average Match Ratio (AMR)
   - Tested zero-shot, few-shot, and Chain-of-Thought prompting strategies

2. ArtPrompt Attack:
   - Two-step process: Word Masking and Cloaked Prompt Generation
   - Used ASCII art generator to replace masked words
   - Evaluated on AdvBench and HEx-PHI datasets
   - Compared with baselines: Direct Instruction, GCG, AutoDAN, PAIR, and DeepInception

Main Results:
1. VITC Benchmark:
   - All tested LLMs struggled with ASCII art recognition tasks
   - Highest performance on VITC-S: GPT-4 with Acc=25.19%
   - Performance deteriorated significantly on VITC-L dataset

2. ArtPrompt Attack:
   - Effective against all tested LLMs
   - Achieved highest Attack Success Rate (ASR) among all jailbreak attacks
   - Bypassed existing defenses: Perplexity-based Detection, Paraphrase, and Retokenization
   - Required only one iteration to construct harmful instructions, making it highly efficient

Qualitative Analysis:
- The study reveals a significant vulnerability in LLMs' safety measures due to their reliance on semantic interpretation of training data
- ArtPrompt exploits the poor performance of LLMs in recognizing ASCII art to bypass safety alignments
- The efficiency and effectiveness of ArtPrompt highlight the urgent need for more advanced defense mechanisms against non-semantic attacks

Limitations:
- The study focused on text-based LLMs and did not evaluate multimodal language models
- The effectiveness of ArtPrompt on models fine-tuned with non-semantic interpretations was not fully explored

Conclusion and Future Work:
- The paper demonstrates that semantics-only interpretation of corpora during safety alignment creates vulnerabilities to jailbreak attacks
- Future work should focus on developing more robust safety measures that account for non-semantic interpretations of input data
- Exploring the application of ArtPrompt to multimodal language models and investigating more advanced defense mechanisms are potential areas for further research

New Tool:
ArtPrompt: A novel ASCII art-based jailbreak attack
GitHub repository: https://github.com/uw-nsl/ArtPrompt