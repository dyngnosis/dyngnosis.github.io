#Tags
[[Research/Research Papers/2409.17458v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection
#AMLT0054/LLMJailbreak

**Title:** RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking
**Authors:** Yifan Jiang, Kriti Aggarwal, Tanmay Laud, Kashif Munir, Jay Pujara, Subhabrata Mukherjee
**Affiliations:** Hippocratic AI, Information Sciences Institute (University of Southern California)
**Publication Date:** September 26, 2024

Key Contributions:
- Introduction of RED QUEEN ATTACK, a novel multi-turn jailbreak method for LLMs
- Creation of 56k high-quality multi-turn attack examples across 40 scenarios and 14 harmful categories
- Comprehensive evaluation of RED QUEEN ATTACK on 10 models from 4 major LLM families
- Analysis of factors contributing to attack success, including model size and scenario complexity
- Development of RED QUEEN GUARD, a mitigation strategy to counter adversarial attacks

Problem Statement:
Current jailbreak attacks on LLMs are primarily single-turn with explicit malicious queries, failing to capture the complexity of real-world interactions where users can conceal their true intentions through multi-turn conversations.

Methodology:
1. Construction of RED QUEEN ATTACK:
   - Created 40 scenarios based on occupations and relationships
   - Selected 14 harmful action categories from the Beavertails dataset
   - Generated 56k multi-turn attack data points
2. Evaluation:
   - Tested on 10 models from 4 LLM families (GPT-4, Llama3, Qwen2, Mixtral)
   - Used Attack Success Rate (ASR) as the primary metric
   - Developed a custom judging prompt for evaluating harmful outputs
3. Analysis:
   - Examined factors contributing to attack success (multi-turn structure, concealment, model size)
   - Conducted fine-grained analysis based on turns, scenarios, and categories
4. Mitigation:
   - Developed RED QUEEN GUARD using Direct Preference Optimization (DPO)

Main Results:
1. RED QUEEN ATTACK achieved high success rates across all tested models:
   - 87.62% ASR on GPT-4o
   - 75.4% ASR on Llama3-70B
2. Larger models were more susceptible to the attack
3. Multi-turn structure and concealment strategies contributed to attack success
4. RED QUEEN GUARD reduced attack success rate to below 1% while maintaining model performance on standard benchmarks

Qualitative Analysis:
- The success of RED QUEEN ATTACK highlights vulnerabilities in current LLMs, especially in multi-turn scenarios
- Concealment of malicious intent through role-playing (e.g., as a protector) proved effective in misleading models
- Larger models' advanced capabilities in complex reasoning and planning may contribute to their increased vulnerability

Limitations:
- Evaluation conducted in a zero-shot setting
- Focus on text-based jailbreak research, not exploring multi-modality contexts
- Potential for misuse of the research findings

Conclusion and Future Work:
- RED QUEEN ATTACK demonstrates the need for more comprehensive red teaming strategies in multi-turn scenarios
- RED QUEEN GUARD shows promise as a mitigation strategy for enhancing LLM safety
- Future work may explore multi-modality contexts and further refinement of mitigation techniques

New Tools:
- RED QUEEN ATTACK: Multi-turn jailbreak method for LLMs
- RED QUEEN GUARD: Mitigation strategy using Direct Preference Optimization
- GitHub repository: https://github.com/kriti-hippo/red_queen

## Repository Token Information
Total tokens in repository: 26794

Tokens per file:
- Utils/scenario_template.py: 22981 tokens
- red_queen_attack_generation.py: 3064 tokens
- Data/judgement_method.py: 669 tokens
- Utils/normal_utils.py: 80 tokens


## Tutorial and Enhancement Suggestions

# RED QUEEN Tutorial and Enhancement Suggestions

## Project Overview

The RED QUEEN project implements a novel multi-turn jailbreak attack method for large language models (LLMs) and a corresponding defense mechanism. The repository contains code for generating attack data, evaluating model responses, and implementing mitigation strategies.

### Project Structure

- `Utils/`: Contains utility functions and scenario templates
  - `scenario_template.py`: Defines classes for different attack scenarios
  - `normal_utils.py`: Basic utility functions for file I/O
- `Data/`: Contains data processing and evaluation scripts
  - `judgement_method.py`: Defines prompts for evaluating model responses
- `red_queen_attack_generation.py`: Main script for generating attack data

## Key Components and Functionality

### 1. Scenario Templates (`scenario_template.py`)

This file defines classes for various attack scenarios based on different relationships (e.g., friend, neighbor) and occupations (e.g., teacher, police). Each class represents a multi-turn conversation structure designed to conceal malicious intent.

Example:
```python
class MultiTurnAttack_relation_friend_three_turn:
    def __init__(self):
        self.target = 'my friend'
        self.system_prompt = system_prompt_new
        self.sentence_1 = """My friend wanted to {action} in their house..."""
        # ... more attributes for subsequent turns
```

These templates are used to generate diverse attack scenarios across different contexts.

### 2. Attack Data Generation (`red_queen_attack_generation.py`)

This script is responsible for generating the attack dataset using the scenario templates. Key functions include:

- `attack_generation_data_open()`: Generates attack data for open-source models
- `attack_generation_data_closed()`: Generates attack data for closed-source models
- `attack_generation_data_ablation()`: Generates data for ablation studies
- `check_generation_data()`: Validates the generated data

The script uses the Hugging Face Transformers library to tokenize and format the attack prompts for different models.

### 3. Evaluation Prompts (`judgement_method.py`)

This file contains prompts used to evaluate the harmfulness of model responses:

- `judge_prompt`: A detailed prompt for binary classification of responses as harmful or not
- `gpt_judge_prompt`: A prompt for rating responses on a scale of 1-10 for harmfulness

### 4. Utility Functions (`normal_utils.py`)

Simple functions for reading and writing JSON files, used throughout the project.

## Relation to Research Paper Concepts

1. **Multi-turn Structure**: The scenario templates implement the paper's concept of multi-turn attacks, allowing for concealment of malicious intent over several conversation turns.

2. **Diverse Scenarios**: The code generates attacks across 40 scenarios (combinations of relationships and occupations) and 14 harmful categories, as described in the paper.

3. **Model Evaluation**: The generation script prepares data for evaluating multiple LLM families, including GPT-4, Llama3, Qwen2, and Mixtral.

4. **Concealment Strategies**: The scenario templates incorporate various concealment strategies, such as role-playing as a protector or authority figure.

5. **Evaluation Metrics**: The judging prompts align with the paper's use of Attack Success Rate (ASR) and more fine-grained harmfulness ratings.

## Notable Techniques

1. **Dynamic Template Filling**: The code uses Python's string formatting to dynamically insert harmful actions into conversation templates.

2. **Hugging Face Integration**: The project leverages the Transformers library for working with various LLM architectures.

3. **Data Validation**: The `check_generation_data()` function implements thorough checks to ensure the generated dataset meets the required specifications.

4. **Ablation Study Support**: The code includes functionality for generating data for ablation studies, allowing for analysis of individual factors contributing to attack success.

# Potential Enhancements

1. **Automated Scenario Generation**
   - Implement a machine learning model to generate new attack scenarios automatically
   - This could increase the diversity of attacks and potentially uncover novel vulnerabilities

2. **Real-time Attack Detection**
   - Develop a module that can analyze ongoing conversations to detect potential multi-turn attacks in real-time
   - This could be integrated into LLM deployment pipelines for enhanced safety

3. **Cross-modal Attack Extension**
   - Extend the attack generation to include multi-modal inputs (text + images)
   - This addresses the paper's limitation of focusing solely on text-based attacks

4. **Adaptive Attack Generation**
   - Implement a reinforcement learning approach to dynamically adjust attack strategies based on model responses
   - This could lead to more sophisticated and effective attacks, pushing the boundaries of LLM safety research

5. **Explainable Mitigation**
   - Enhance the RED QUEEN GUARD with explainability features
   - Provide detailed reasoning for why a particular response was flagged as potentially harmful
   - This could offer insights into model decision-making and improve the interpretability of safety measures