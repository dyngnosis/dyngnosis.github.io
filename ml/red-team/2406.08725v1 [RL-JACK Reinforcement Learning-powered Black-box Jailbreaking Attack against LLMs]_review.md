#Tags
[[Research/Research Papers/2406.08725v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak
#AMLT0005/CreateProxyMLModel

**Title:** RL-JACK: Reinforcement Learning-powered Black-box Jailbreaking Attack against LLMs
**Authors:** Xuan Chen, Yuzhou Nie, Lu Yan, Yunshu Mao, Wenbo Guo, Xiangyu Zhang
**Affiliations:** Purdue University, University of California Santa Barbara
**Publication Date:** June 13, 2024

Summary:
This paper presents RL-JACK, a novel black-box jailbreaking attack against large language models (LLMs) using deep reinforcement learning (DRL). The authors formulate jailbreaking prompt generation as a search problem and design a DRL approach to solve it efficiently. RL-JACK outperforms existing jailbreaking attacks against various LLMs, including open-source and commercial models.

Key Contributions:
- Formulation of jailbreaking as a search problem solvable by DRL
- Novel LLM-facilitated action space design for diverse actions while constraining search space
- New reward function providing meaningful dense rewards for successful jailbreaking
- Comprehensive evaluation against 6 SOTA LLMs and 3 SOTA defenses
- Demonstration of transferability across different models

Problem Statement:
Existing jailbreaking attacks against LLMs are limited by the stochastic and random nature of genetic methods, reducing their effectiveness and efficiency. The paper aims to develop a more effective and efficient black-box jailbreaking attack using reinforcement learning.

Methodology:
1. RL formulation: State (current prompt), Action (jailbreaking strategies), Reward (similarity to unaligned model response)
2. LLM-facilitated action space: 10 jailbreaking strategies as actions
3. Dense reward function: Cosine similarity between target LLM response and unaligned model reference response
4. Custom state transition function and training algorithm
5. Evaluation on 6 LLMs: Llama2-7b-chat, Llama2-70b-chat, Vicuna-7b, Vicuna-13b, Falcon-40b-instruct, GPT-3.5-turbo
6. Comparison with 5 baseline attacks: AutoDAN, GPTFUZZER, PAIR, Cipher, GCG
7. Testing against 3 SOTA defenses: Rephrasing, Perplexity, RAIN

Main Results:
1. RL-JACK outperforms baselines in attack success rate, cosine similarity, and GPT-Judge metrics
2. Higher effectiveness on difficult questions (Max50 dataset)
3. More resilient against SOTA defenses compared to baselines
4. Demonstrates transferability across different models, including Llama2-70b-chat
5. Generates prompts with lower perplexity scores, indicating more natural and stealthy attacks

Qualitative Analysis:
- RL-JACK's success is attributed to its ability to learn optimal combinations of jailbreaking strategies
- The dense reward function allows for more efficient learning compared to sparse rewards in genetic methods
- The LLM-facilitated action space enables diverse prompt modifications while constraining the overall search space

Limitations:
- Reliance on an unaligned model for generating reference answers
- Potential false negatives in the reward function when target LLM responds differently from the reference answer
- Computational cost of training the RL agent

Conclusion and Future Work:
RL-JACK demonstrates the effectiveness of using DRL for black-box jailbreaking attacks against LLMs. Future work includes:
1. Improving the reward function to better distinguish between refusal and correct answers
2. Exploring adaptive attacks against existing defenses
3. Extending the framework to multi-modal models (e.g., vision-language models)

New Tools:
RL-JACK (GitHub repository not provided in the paper)