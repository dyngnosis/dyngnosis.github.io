#Tags
[[Research/Research Papers/2403.17336v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData

**Title:** Don't Listen To Me: Understanding and Exploring Jailbreak Prompts of Large Language Models
**Authors:** Zhiyuan Yu, Xiaogeng Liu, Shunning Liang, Zach Cameron, Chaowei Xiao, Ning Zhang
**Affiliations:** Washington University in St. Louis, University of Wisconsin - Madison, John Burroughs School
**Publication Date:** March 26, 2024

Summary:
This paper investigates the emerging threat of jailbreak attacks on large language models (LLMs), focusing on semantically meaningful jailbreak prompts. The authors conduct a comprehensive analysis of existing jailbreak techniques, evaluate their effectiveness, and explore the process of manually creating jailbreak prompts through a user study. They also develop a system for automating jailbreak prompt generation.

Key Contributions:
- Systemization of existing jailbreak prompts into categories and patterns
- Evaluation of jailbreak effectiveness on state-of-the-art commercial LLMs
- User study revealing the process of manually creating jailbreak prompts
- Development of an automated system for jailbreak prompt generation

Problem Statement:
The paper addresses the security concerns arising from the potential misuse of LLMs through jailbreak techniques, which aim to circumvent built-in restrictions and elicit harmful content.

Methodology:
1. Collection and analysis of 448 in-the-wild jailbreak prompts and 161 malicious queries
2. Thematic analysis to categorize jailbreak prompts
3. Evaluation of jailbreak effectiveness on GPT-3.5, GPT-4, and PaLM-2 using human-annotated outputs
4. User study with 92 participants to explore manual jailbreak prompt creation
5. Development of an automated jailbreak prompt generation system

Main Results and Findings:
1. Jailbreak prompts categorized into five categories with ten unique patterns
2. Two most effective jailbreak strategies identified
3. Universal jailbreak prompts discovered, effective across different LLMs
4. Users often succeeded in creating jailbreak prompts regardless of expertise
5. Automated system successfully transformed 729 out of 766 previously failed prompts

Qualitative Analysis:
- The study reveals that jailbreak prompts exploit the conflict between user-centric design and regulatory policies in LLMs
- The effectiveness of jailbreak prompts varies depending on the type of malicious query
- Longer and more complex prompts tend to be more successful in jailbreaking attempts
- The process of creating jailbreak prompts involves various strategies, including disguised intent, role play, and virtual AI simulation

Limitations:
- Lack of established benchmarks for quantifying jailbreak success
- Potential bias in self-reported expertise levels of study participants
- Limited exploration of semantic features in universal jailbreak prompt analysis

Conclusion and Future Work:
The paper highlights the significant threat posed by jailbreak attacks on LLMs and emphasizes the need for improved security measures. Future work should focus on developing more robust defenses against jailbreak attempts and exploring additional factors influencing jailbreak success.

New Tools:
The authors developed an automated jailbreak prompt generation system, but no specific name or GitHub repository was mentioned in the provided content.