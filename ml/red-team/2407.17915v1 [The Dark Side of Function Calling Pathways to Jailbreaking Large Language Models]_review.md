#Tags
[[Research/Research Papers/2407.17915v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData

**Title:** The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models
**Authors:** Zihui Wu, Haichang Gao, Jianping He, Ping Wang
**Affiliation:** School of Computer Science and Technology, Xidian University
**Publication Date:** July 25, 2024

Key Contributions:
- Identified a critical vulnerability in the function calling process of LLMs
- Introduced a novel "jailbreak function" attack method
- Conducted empirical studies on six state-of-the-art LLMs, revealing high attack success rates
- Analyzed reasons for function call vulnerabilities
- Proposed defensive strategies, including defensive prompts

Problem Statement:
The paper addresses the overlooked security implications of the function calling feature in Large Language Models (LLMs), which can be exploited to bypass existing safety measures and induce harmful behaviors.

Methodology:
1. Designed a "jailbreak function" called WriteNovel to induce harmful content generation
2. Evaluated attack effectiveness using the AdvBench dataset
3. Tested on six LLMs: GPT-4-1106-preview, GPT-4o, Claude-3-sonnet, Claude-3.5-sonnet, Gemini-1.5-pro, and Mixtral-8x7B-Instruct-v0.1
4. Used GPT-4 as a judge to determine jailbreak success
5. Compared results with existing methods: CodeChameleon and ReNeLLM

Main Results:
1. Achieved an average attack success rate of over 90% across 6 state-of-the-art LLMs
2. Jailbreak function attacks were more successful than comparative techniques for each LLM tested
3. Identified three key factors contributing to vulnerability:
   - Alignment discrepancies between function arguments and chat mode responses
   - User ability to coerce models into executing potentially harmful functions
   - Lack of rigorous safety filters in function calling processes

Qualitative Analysis:
- The high success rate of jailbreak function attacks highlights a significant security gap in LLMs
- The vulnerability exploits the less aligned nature of function arguments compared to chat mode responses
- The ability to force function execution in LLMs presents a notable security risk
- Current LLM providers may have overlooked the security of function calling, as evidenced by the ineffectiveness of safety filters in this context

Limitations:
- The study focuses on a specific type of jailbreak attack and may not cover all potential vulnerabilities in function calling
- The effectiveness of proposed defensive measures, particularly defensive prompts, may vary across different LLM implementations

Conclusion and Future Work:
- The paper emphasizes the urgent need for enhanced security measures in LLM function calling capabilities
- Proposed defensive strategies, especially the use of defensive prompts, offer a starting point for mitigating risks
- Future work should focus on developing more robust security measures for function calling and exploring potential vulnerabilities in other LLM features

Tools Introduced:
- Jailbreak function attack method (GitHub repository: https://github.com/wooozihui/jailbreakfunction)

Relevant Figures:
1. Figure 1: Overview of the function calling process in LLMs and the potential for jailbreak attacks
2. Figure 2: Components of the jailbreak function
3. Figure 3: Screenshot showcasing generation of harmful content across different LLMs

## Repository Token Information
Total tokens in repository: 4282

Tokens per file:
- evaluate.py: 454 tokens
- attack.py: 2172 tokens
- utils.py: 430 tokens
- README.md: 1226 tokens


## Tutorial and Enhancement Suggestions

# Tutorial: Jailbreak Function Attack on LLMs

## Project Overview

This repository implements a novel "jailbreak function" attack method to exploit vulnerabilities in the function calling capabilities of large language models (LLMs). The key components are:

- `attack.py`: Main script for executing the jailbreak function attack
- `evaluate.py`: Evaluates if an LLM response is successfully jailbroken 
- `utils.py`: Helper functions for data processing and content type generation
- `README.md`: Project documentation and overview

The attack works by crafting a malicious function call that tricks the LLM into generating harmful content, bypassing typical safety measures.

## Key Components

### attack.py

This is the core script that executes the jailbreak function attack. Key functions:

1. `jailbreak_function_construct()`: Creates the malicious "WriteNovel" function definition tailored to each LLM API.

2. `jailbreak_function_call()`: Calls the jailbreak function on the target LLM and extracts the generated argument.

3. `main()`: Orchestrates the full attack process:
   - Loads harmful behaviors from a dataset
   - Generates appropriate content types for each behavior
   - Attempts the jailbreak attack with different content types
   - Evaluates success and calculates attack success rate

### evaluate.py

Contains the `harmful_classification()` function that uses GPT-4 as a judge to determine if an LLM response is successfully jailbroken.

### utils.py

Provides utility functions:
- `get_content_type()`: Generates appropriate content types for a given harmful behavior using GLM-4.
- `get_dataset()`: Loads the dataset of harmful behaviors.

## Key Concepts and Techniques

1. **Jailbreak Function Design**: The `WriteNovel` function is crafted to exploit alignment discrepancies and user coercion in LLMs. It frames the harmful behavior as part of a fictional narrative to bypass safety filters.

2. **Content Type Generation**: The attack dynamically generates appropriate content types (e.g., "essay", "guide") for each harmful behavior, increasing the chances of a successful jailbreak.

3. **Multi-Shot Approach**: The attack attempts up to 5 different content types per harmful behavior, improving overall success rates.

4. **Cross-Model Evaluation**: The code supports attacking multiple LLM APIs (OpenAI, Anthropic, Google, etc.) allowing for comparative analysis.

5. **Automated Jailbreak Detection**: Uses GPT-4 as an automated judge to classify responses as jailbroken or not, enabling large-scale evaluation.

## Running the Attack

1. Set up API keys for target LLMs and evaluation model (GPT-4) in `attack.py`.
2. Run the attack: `python attack.py --target_model <model_name>`
3. Results are saved as PyTorch tensors in the `./result/` directory.

# Potential Enhancements

1. **Defensive Prompt Integration**: 
   - Implement and evaluate the proposed defensive prompts within the function calling process.
   - Create a framework for easily testing different defensive prompt strategies.

2. **Adaptive Attack Techniques**:
   - Develop methods to dynamically adjust the jailbreak function based on initial LLM responses.
   - Incorporate techniques from other jailbreak methods (e.g., CodeChameleon) to create hybrid attacks.

3. **Expanded Model Coverage**:
   - Add support for testing on a wider range of LLMs, including open-source models.
   - Implement a plugin system for easily adding new LLM APIs to the testing framework.

4. **Improved Evaluation Metrics**:
   - Develop more nuanced success criteria beyond binary jailbreak classification.
   - Implement automated evaluation of generated content harmfulness and relevance to the target behavior.

5. **Mitigation Technique Experimentation**:
   - Create a testbed for rapidly prototyping and evaluating different defense mechanisms against the attack.
   - Explore using adversarial training to improve LLM robustness to function-based attacks.