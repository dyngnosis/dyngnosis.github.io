#Tags
[[Research/Research Papers/2402.08679v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability
**Authors:** Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, Bin Hu
**Publication Date:** February 13, 2024

Summary:
This paper introduces COLD-Attack, a framework for generating stealthy and controllable adversarial prompts to jailbreak large language models (LLMs). The authors formulate the controllable attack generation problem and connect it to controllable text generation. They adapt the Energy-based Constrained Decoding with Langevin Dynamics (COLD) algorithm to create adversarial LLM attacks with various control requirements.

Key Contributions:
- Formulation of the controllable attack generation problem
- Connection between LLM jailbreaking and controllable text generation
- Development of COLD-Attack framework for generating stealthy and controllable adversarial prompts
- Introduction of new attack settings: paraphrasing constraint and position constraint
- Extensive experiments demonstrating COLD-Attack's effectiveness across multiple LLMs

Problem Statement:
The paper addresses the need for a comprehensive assessment of LLM safety by considering jailbreaks with diverse attributes such as contextual coherence and sentiment/stylistic variations. The authors aim to develop a method for enforcing control on LLM attacks to generate more diverse and stealthy jailbreaks.

Methodology:
1. Formulate the controllable attack generation problem
2. Adapt the COLD algorithm for adversarial prompt generation
3. Develop energy functions for various control requirements (fluency, attack success, lexical constraints, semantic similarity)
4. Implement Langevin dynamics sampling for optimizing logit sequences
5. Use LLM-guided decoding to convert continuous logits to discrete text attacks

The authors evaluate COLD-Attack on various LLMs, including Llama-2, Mistral, Vicuna, Guanaco, GPT-3.5, and GPT-4, under three attack settings:
1. Attack with continuation constraint
2. Attack with paraphrasing constraint
3. Attack with position constraint

Main Results:
1. COLD-Attack achieves high attack success rates (ASR) and GPT-4-based ASR (ASR-G) across multiple LLMs
2. The method generates more fluent adversarial prompts with lower perplexity compared to baseline methods
3. COLD-Attack demonstrates strong controllability in various attack settings, including sentiment steering and lexical constraints
4. The framework shows good transferability to black-box models like GPT-3.5 and GPT-4
5. COLD-Attack is more efficient than baseline methods like GCG and AutoDAN-Zhu

Qualitative Analysis:
- The controllability of COLD-Attack allows for diverse attack scenarios, enhancing the comprehensiveness of LLM safety assessments
- The framework's ability to generate coherent and contextually appropriate adversarial prompts makes it more challenging to detect and defend against
- The connection between LLM jailbreaking and controllable text generation opens up new research directions for improving LLM safety

Limitations:
- The effectiveness of COLD-Attack may be reduced when system prompts are included in the LLM setup
- The method may struggle to decrease the attack loss sufficiently in the presence of system prompts

Conclusion and Future Work:
The authors conclude that COLD-Attack provides a versatile framework for generating stealthy and controllable adversarial prompts for LLMs. They suggest that their approach can inspire more research on diversifying LLM attacks and improving overall LLM safety. Future work may focus on:
1. Improving COLD-Attack's performance in the presence of system prompts
2. Developing more advanced defense mechanisms against stealthy and controllable attacks
3. Exploring the application of COLD-Attack in other domains of AI safety

Tools Introduced:
COLD-Attack framework (GitHub repository: https://github.com/Yu-Fangxu/COLD-Attack)

## Repository Token Information
Total tokens in repository: 57879

Tokens per file:
- bleuloss.py: 642 tokens
- evaluate.py: 1721 tokens
- attack_suffix.py: 743 tokens
- decoding_paraphrase.py: 3205 tokens
- opt_util.py: 268 tokens
- cold_decoding.py: 1570 tokens
- attack_paraphrase.py: 588 tokens
- attack_control.py: 762 tokens
- decoding_control.py: 3930 tokens
- util.py: 13409 tokens
- decoding_suffix.py: 3138 tokens
- evaluation/eval.py: 1034 tokens
- evaluation/__init__.py: 0 tokens
- evaluation/ensemble_scorer.py: 626 tokens
- evaluation/bert_score/score.py: 1371 tokens
- evaluation/bert_score/bert_score.py: 295 tokens
- evaluation/bert_score/__init__.py: 0 tokens
- evaluation/bert_score/utils.py: 2125 tokens
- evaluation/cider/cider_scorer.py: 1811 tokens
- evaluation/cider/cider.py: 447 tokens
- evaluation/cider/__init__.py: 8 tokens
- evaluation/rouge/rouge.py: 888 tokens
- evaluation/rouge/__init__.py: 9 tokens
- evaluation/gpt4-eval/nc_data_process_gpt4eval.py: 845 tokens
- evaluation/gpt4-eval/aggregate.py: 449 tokens
- evaluation/gpt4-eval/just_eval/evaluate.py: 4416 tokens
- evaluation/gpt4-eval/just_eval/_version.py: 11 tokens
- evaluation/gpt4-eval/just_eval/__init__.py: 37 tokens
- evaluation/gpt4-eval/just_eval/utils.py: 8271 tokens
- evaluation/gpt4-eval/just_eval/reward_model.py: 911 tokens
- evaluation/gpt4-eval/just_eval/scripts/tag_analysis.py: 606 tokens
- evaluation/meteor/meteor_nltk.py: 292 tokens
- evaluation/meteor/__init__.py: 8 tokens
- evaluation/meteor/meteor.py: 830 tokens
- evaluation/bleu/bleu_scorer.py: 2245 tokens
- evaluation/bleu/__init__.py: 8 tokens
- evaluation/bleu/bleu.py: 360 tokens


## Tutorial and Enhancement Suggestions

# COLD-Attack Tutorial

## Project Overview

The COLD-Attack repository implements the framework described in the paper for generating stealthy and controllable adversarial prompts to jailbreak large language models (LLMs). The project is structured into several key components:

1. Attack generation scripts
2. Decoding algorithms 
3. Utility functions
4. Evaluation metrics

## Key Components

### 1. Attack Generation Scripts

The main attack generation scripts are:

- `attack_suffix.py`
- `attack_paraphrase.py`  
- `attack_control.py`

These scripts implement different attack scenarios described in the paper:

- Suffix attacks
- Paraphrasing attacks
- Controlled attacks

They use the core COLD-Attack algorithm to generate adversarial prompts.

### 2. Decoding Algorithms

The decoding algorithms are implemented in:

- `decoding_suffix.py`
- `decoding_paraphrase.py`
- `decoding_control.py`

These files contain the core COLD-Attack algorithm adapted for different attack scenarios. The key function is `decode()`, which implements the Langevin dynamics sampling and optimization process described in the paper.

### 3. Utility Functions

`util.py` contains many helper functions used throughout the project, including:

- Text processing functions
- Embedding and encoding functions  
- Loss calculation functions
- Sampling and filtering functions

### 4. Evaluation Metrics

The `evaluation` folder contains implementations of various metrics used to evaluate the quality and effectiveness of the generated attacks, including:

- BLEU score
- METEOR score
- ROUGE score
- BERTScore

## Key Algorithms

### COLD-Attack Algorithm

The core COLD-Attack algorithm is implemented in the `decode()` function in the decoding files. It follows these main steps:

1. Initialize logits for the adversarial prompt
2. Iteratively optimize the logits using Langevin dynamics:
   - Calculate gradients of the energy function
   - Update logits with gradient and noise
   - Apply constraints (e.g. top-k filtering)
3. Convert final logits to text using the target LLM

The energy function combines multiple objectives:

- Attack success (cross-entropy loss)
- Fluency (language model loss) 
- Controllability (e.g. lexical constraints, semantic similarity)

### Langevin Dynamics Sampling

Langevin dynamics sampling is implemented in the main optimization loop:

```python
y_logits_ = y_logits + epsilon
soft_forward_y = y_logits_ / 0.001
# ... calculate losses ...
loss = args.goal_weight * c_loss_1 + 1 * flu_loss - args.rej_weight * c_loss_2
loss.backward()
optim.step()
```

This stochastic optimization process allows exploration of the logit space while optimizing the energy function.

## Relation to Paper Concepts

The code directly implements the key concepts from the paper:

1. Formulation of controllable attack generation (energy functions)
2. Adaptation of COLD algorithm for adversarial prompts (decoding files)
3. Various control requirements (fluency, attack success, lexical constraints)
4. Langevin dynamics sampling (optimization loop)
5. LLM-guided decoding (conversion of logits to text)

The different attack scenarios (suffix, paraphrase, position constraint) are implemented in separate files, allowing for the comprehensive evaluation described in the paper.

# Potential Enhancements

1. Improved System Prompt Handling
   - Develop techniques to better handle system prompts in the attack generation process
   - Explore methods to incorporate system prompt information into the energy function
   - Implement adaptive strategies that can adjust the attack based on detected system prompts

2. Multi-Modal Attack Generation
   - Extend COLD-Attack to generate adversarial prompts that include both text and images
   - Develop energy functions that consider visual coherence and attack effectiveness in multi-modal settings
   - Integrate with vision-language models to evaluate multi-modal jailbreaking attempts

3. Dynamic Constraint Adaptation
   - Implement a mechanism to dynamically adjust constraints during the optimization process
   - Develop heuristics or learning-based approaches to identify effective constraint combinations
   - Create a feedback loop that uses attack success information to guide constraint adaptation

4. Transferability Optimization
   - Enhance the algorithm to explicitly optimize for transferability across different LLMs
   - Implement techniques from transfer learning and domain adaptation to improve cross-model performance
   - Develop a meta-learning approach that can quickly adapt attacks to new, unseen models

5. Defensive Capabilities
   - Extend the framework to generate "anti-jailbreak" prompts that can defend against adversarial attacks
   - Implement adversarial training techniques to make LLMs more robust to COLD-Attack and similar methods
   - Develop real-time detection mechanisms that can identify potential jailbreaking attempts based on COLD-Attack patterns