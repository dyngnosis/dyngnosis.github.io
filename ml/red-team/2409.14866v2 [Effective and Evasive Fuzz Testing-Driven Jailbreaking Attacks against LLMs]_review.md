#Tags
[[Research/Research Papers/2409.14866v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0040/MLModelInferenceAPIAccess
#AMLT0042/VerifyAttack

**Title:** Effective and Evasive Fuzz Testing-Driven Jailbreaking Attacks against LLMs
**Authors:** Xueluan Gong, Mingzhe Li, Yilin Zhang, Fengyuan Ran, Chen Chen, Yanjiao Chen, Qian Wang, Kwok-Yan Lam
**Affiliations:** Nanyang Technological University, Wuhan University, Zhejiang University
**Publication Date:** September 23, 2024

Summary:
This paper introduces a novel jailbreaking attack framework for Large Language Models (LLMs) that uses fuzz testing techniques to automatically generate effective and evasive jailbreak prompts. The method does not rely on existing templates, maintains semantic coherence, and produces shorter prompts compared to existing approaches.

Key Contributions:
- A fuzz testing-driven jailbreaking attack framework that starts with an empty seed pool
- Three question-dependent mutation strategies using an LLM helper for generating coherent and concise prompts
- A two-level judge module for accurate detection of successful jailbreaks
- Evaluation on 7 LLMs, outperforming 5 state-of-the-art jailbreaking attacks
- Demonstration of transferability and robustness against state-of-the-art defenses

Problem Statement:
Current jailbreaking methods for LLMs either rely heavily on manually crafted templates, limiting scalability and adaptability, or struggle to generate semantically coherent prompts that are easily detectable. Additionally, most existing approaches use lengthy prompts, leading to higher query costs.

Methodology:
1. Fuzz testing framework adaptation for jailbreaking LLMs
2. Empty seed pool initialization
3. Three mutation strategies: Role-play, Contextualization, and Expand
4. Two-level judge module: RoBERTa-based model and ChatGPT-based model
5. Evaluation on 7 LLMs: LLaMA-2-7b-chat, Vicuna-7b-v1.3, Baichuan2-7b-chat, Guanaco-7B, GPT-3.5 Turbo, GPT-4, and Gemini-Pro
6. Comparison with 5 state-of-the-art jailbreaking attacks: AutoDAN, GCG, PAIR, Gptfuzzer, and TAP

Main Results:
- Achieved attack success rates of over 90%, 80%, and 74% for GPT-3.5 turbo, GPT-4, and Gemini-Pro, respectively
- Outperformed existing baselines by more than 60% across various LLMs
- Maintained high semantic coherence while significantly reducing jailbreak prompt length
- Achieved over 78% attack success rate on GPT-4 with only 100 tokens

Qualitative Analysis:
- The proposed method demonstrates high effectiveness in generating jailbreak prompts without relying on existing templates
- The use of LLM-empowered mutation modules contributes to the generation of semantically coherent and concise prompts
- The two-level judge module improves the accuracy of detecting successful jailbreaks

Limitations:
- The method still requires multiple queries to the victim LLM, which can be costly
- Ethical concerns regarding the generation of harmful or offensive content

Conclusion and Future Work:
The paper presents a powerful jailbreaking method that outperforms existing approaches in terms of effectiveness, efficiency, and evasiveness. Future work may include developing additional mutators, exploring prompt compression methods, and investigating defenses against this type of attack.

New Tool:
The authors mention that their code will be available at https://github.com/aaFrostnova/Effective-llm-jailbreak upon publication.

## Repository Token Information
Total tokens in repository: 16018

Tokens per file:
- gptfuzzer/llm/llm.py: 2724 tokens
- gptfuzzer/fuzzer/mutator.py: 2064 tokens
- gptfuzzer/fuzzer/core.py: 1997 tokens
- gptfuzzer/utils/predict.py: 1827 tokens
- Judge/language_models.py: 1663 tokens
- gptfuzzer/fuzzer/selection.py: 1479 tokens
- run.py: 1301 tokens
- Judge/judges.py: 816 tokens
- Judge/TestJudge.py: 557 tokens
- gptfuzzer/fuzzer/branch_prune.py: 544 tokens
- Judge/Judge.py: 516 tokens
- eval.py: 294 tokens
- gptfuzzer/utils/openai.py: 151 tokens
- gptfuzzer/utils/template.py: 42 tokens
- gptfuzzer/llm/__init__.py: 31 tokens
- gptfuzzer/fuzzer/__init__.py: 12 tokens
- gptfuzzer/__init__.py: 0 tokens
- gptfuzzer/utils/__init__.py: 0 tokens


## Tutorial and Enhancement Suggestions

# GPTFuzzer Tutorial

## Project Overview

GPTFuzzer is a framework for generating jailbreak prompts to attack large language models (LLMs) using fuzz testing techniques. The project implements the approach described in the research paper, using mutation strategies and a two-level judge to create effective and evasive jailbreak prompts.

### Project Structure

The repository is organized into several key directories:

- `gptfuzzer/`: Core implementation of the fuzzing framework
  - `llm/`: LLM interfaces for different model types
  - `fuzzer/`: Main fuzzing logic and components
  - `utils/`: Utility functions and helpers
- `Judge/`: Implementation of the judging system
- `run.py`: Main entry point for running experiments
- `eval.py`: Evaluation script for analyzing results

## Key Components

### 1. LLM Interfaces (`gptfuzzer/llm/llm.py`)

This module defines interfaces for interacting with various LLM backends:

- `LLM`: Base class for LLM interfaces
- `LocalLLM`: Interface for locally hosted models
- `LocalVLLM`: Interface for locally hosted models using vLLM
- `OpenAILLM`: Interface for OpenAI API models
- `PaLM2LLM`: Interface for Google's PaLM 2 model
- `ClaudeLLM`: Interface for Anthropic's Claude model

These classes handle prompt generation and batched inference for different LLM types.

### 2. Fuzzer Core (`gptfuzzer/fuzzer/core.py`)

The `GPTFuzzer` class is the main orchestrator of the fuzzing process. Key methods include:

- `run()`: Main fuzzing loop
- `evaluate()`: Evaluates generated prompts
- `update()`: Updates the fuzzer state based on results

The `PromptNode` class represents individual prompts in the fuzzing tree.

### 3. Mutation Strategies (`gptfuzzer/fuzzer/mutator.py`)

Implements various mutation strategies for generating new prompts:

- `OpenAIMutatorExpand`: Expands existing prompts
- `OpenAIMutatorscenario`: Generates new scenarios
- `OpenAIMutatorcharacters`: Creates character-based prompts

These mutators use OpenAI's API to generate coherent mutations.

### 4. Selection Policies (`gptfuzzer/fuzzer/selection.py`)

Defines strategies for selecting which prompts to mutate:

- `RoundRobinSelectPolicy`: Cycles through prompts
- `RandomSelectPolicy`: Randomly selects prompts
- `UCBSelectPolicy`: Uses Upper Confidence Bound algorithm
- `MCTSExploreSelectPolicy`: Implements Monte Carlo Tree Search
- `EXP3SelectPolicy`: Implements Exponential-weight algorithm for Exploration and Exploitation

### 5. Judging System (`Judge/judges.py`)

Implements the two-level judging system:

- `JudgeBase`: Base class for judges
- `GPTJudge`: Uses GPT models for judging
- `NoJudge`: Placeholder for no judging

### 6. Main Execution (`run.py`)

Configures and runs the fuzzing experiment:

- Loads initial seeds and configurations
- Sets up LLM interfaces, mutators, and selection policies
- Executes the fuzzing process
- Handles result logging

## Key Algorithms and Techniques

1. **Fuzz Testing Adaptation**: The project adapts fuzz testing techniques to the domain of LLM jailbreaking, using mutation, selection, and evaluation cycles.

2. **LLM-powered Mutations**: Utilizes LLMs (via OpenAI API) to generate coherent and contextually relevant mutations of prompts.

3. **Multi-armed Bandit and MCTS**: Implements advanced selection policies (UCB, EXP3, MCTS) to efficiently explore the prompt space.

4. **Two-level Judging**: Uses a combination of a fast RoBERTa-based model and a more accurate GPT-based model to evaluate jailbreak success.

5. **Batched Inference**: Implements batched processing for improved efficiency when interacting with LLMs.

# Potential Enhancements

1. **Adaptive Mutation Strategies**
   - Implement a system to dynamically adjust mutation strategies based on their success rates
   - Explore using reinforcement learning to optimize mutation parameters

2. **Improved Efficiency in LLM Querying**
   - Implement caching mechanisms to avoid redundant LLM queries
   - Explore techniques for prompt compression to reduce token usage

3. **Extended Model Support**
   - Add support for more open-source LLMs and APIs
   - Implement a plugin system for easily adding new model backends

4. **Advanced Judging Techniques**
   - Explore using few-shot learning or fine-tuning for more accurate jailbreak detection
   - Implement ensemble methods combining multiple judge models

5. **Ethical Considerations and Safeguards**
   - Implement content filtering to avoid generating harmful or illegal content
   - Add a system for tracking and limiting the types of jailbreaks attempted
   - Explore techniques for generating "anti-jailbreak" prompts to improve LLM robustness