#Tags
[[Research/Research Papers/2310.06387v3.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0031/ErodeMLModelIntegrity

**Title:** Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations
**Authors:** Zeming Wei, Yifei Wang, Ang Li, Yichuan Mo, Yisen Wang
**Affiliations:** Peking University, MIT CSAIL
**Publication Date:** May 25, 2024 (last updated)

Key Contributions:
- Proposed In-Context Attack (ICA) to subvert aligned LLMs using harmful demonstrations
- Introduced In-Context Defense (ICD) to enhance model resilience against jailbreaking
- Provided theoretical insights on how in-context demonstrations influence LLM safety alignment
- Demonstrated effectiveness of ICA and ICD through extensive experiments

Problem Statement:
The paper addresses the vulnerability of aligned Large Language Models (LLMs) to jailbreak attacks and explores the potential of In-Context Learning (ICL) to manipulate LLM safety alignment.

Methodology:
1. In-Context Attack (ICA):
   - Crafts harmful demonstrations using query-response pairs
   - Concatenates demonstrations with target attack prompt
   - Requires only a single forward pass for attack

2. In-Context Defense (ICD):
   - Uses safe demonstrations that refuse to answer harmful requests
   - Appends demonstrations to conversation template
   - Enhances model robustness without modifying model parameters

3. Theoretical Framework:
   - Decouples safe and harmful language distributions
   - Analyzes how adversarial demonstrations guide model generation bias

4. Experimental Evaluation:
   - Used AdvBench and HarmBench datasets for evaluation
   - Tested on various LLMs including GPT-4, Vicuna, Llama-2, and QWen
   - Compared ICA and ICD with existing attack and defense methods

Main Results:
1. ICA Effectiveness:
   - Achieved 81% Attack Success Rate (ASR) on GPT-4 with AdvBench dataset
   - Increased ASR from 1% to 8% for Vicuna with just one demonstration
   - Outperformed or matched existing attack methods on HarmBench

2. ICD Effectiveness:
   - Reduced ASR of Llama-2 against transferable GCG from 21% to 0%
   - Significantly decreased ASR against various attacks with 1-2 demonstrations
   - Maintained natural performance on GLUE and MT-bench benchmarks

Qualitative Analysis:
- ICA demonstrates the vulnerability of aligned LLMs to carefully crafted in-context demonstrations
- ICD shows promise as a flexible and efficient defense mechanism without requiring model modification
- The effectiveness of few-shot demonstrations highlights the sensitivity of LLMs to context

Limitations:
- The study primarily focuses on text-based LLMs and may not generalize to other AI modalities
- The long-term effects of repeated exposure to ICA or ICD are not explored
- The theoretical framework makes several assumptions that may not hold in all real-world scenarios

Conclusion and Future Work:
- The paper reveals the significant impact of in-context demonstrations on LLM safety alignment
- Proposes ICA and ICD as new paradigms for red-teaming and guarding LLMs
- Suggests further research into the flexibility and adaptability of aligned LLMs
- Calls for more investigation into the underlying mechanisms of in-context learning in LLMs

Relevant Figures:
- Figure 1: Illustrates LLM conversation under various settings, including ICA and ICD

Tools Introduced:
- In-Context Attack (ICA): A method for jailbreaking LLMs using harmful demonstrations
- In-Context Defense (ICD): A technique for enhancing LLM robustness using safe demonstrations

(No GitHub repositories were mentioned in the paper)