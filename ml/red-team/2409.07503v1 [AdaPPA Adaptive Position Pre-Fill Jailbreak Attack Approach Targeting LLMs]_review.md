#Tags
[[Research/Research Papers/2409.07503v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs
**Authors:** Lijia Lv, Weigang Zhang, Xuehai Tang, Jie Wen, Feng Liu, Jizhong Han, Songlin Hu
**Affiliation:** Institute of Information Engineering, Chinese Academy of Sciences
**Publication Date:** September 11, 2024

Key Contributions:
- Introduced AdaPPA, a novel jailbreak method for LLMs
- Observed the effectiveness of pre-filling LLM outputs with different lengths and types of content
- Proposed a technique leveraging shallow alignment vulnerability in LLMs
- Demonstrated significant improvement in attack success rate compared to existing methods

Problem Statement:
Traditional jailbreak attack methods focus primarily on semantic-level approaches, which are easily detected by LLMs. These methods overlook the difference in model alignment protection capabilities at different output stages.

Methodology:
1. Problem Rewrite: Transform original questions to enhance attack effectiveness
2. Pre-fill Generation: Adaptively generate pre-filled content, including safe and harmful responses
3. Prompt Combination: Combine different pre-filled contents to identify optimal attack prompts
4. Attack and Judge: Input attack requests into target models and evaluate responses

Key Findings:
- AdaPPA achieved a 47% improvement in attack success rate on Llama2 compared to existing methods
- The method was effective across various models, including ChatGLM3, Vicuna, and GPT-4
- Pre-filling the model's output with content of varying lengths significantly impacts attack success

Qualitative Analysis:
- The study reveals that LLMs have varying levels of vulnerability at different output stages
- The combination of safe and harmful pre-filled content exploits the model's instruction-following and narrative-shifting capabilities
- The adaptive nature of the attack makes it more difficult for models to detect and defend against

Limitations:
- The study primarily focuses on black-box models, which may not fully represent all deployment scenarios
- The effectiveness of the attack may vary depending on the specific model and its defensive mechanisms

Conclusion and Future Work:
- AdaPPA demonstrates the potential for uncovering security vulnerabilities in LLMs through adaptive pre-fill attacks
- Future work may involve exploring more sophisticated combinations of pre-filled content and investigating defensive measures against such attacks

Tools Introduced:
- AdaPPA framework (GitHub repository: https://github.com/Yummy416/AdaPPA)

Relevant Figures:
1. Figure 1: Attack prompt structure
2. Figure 2: Experiment results showing the effect of specific content input on attack success rate
3. Figure 3: Overview of AdaPPA framework
4. Figure 4: Problem rewriting structure

## Repository Token Information
Total tokens in repository: 13100

Tokens per file:
- py_file/model_chat.py: 2486 tokens
- test/main.py: 2241 tokens
- observation/main.py: 2075 tokens
- py_file/model_load.py: 1781 tokens
- test/main_2.py: 1730 tokens
- train/gain_chatglm3_data.py: 1249 tokens
- py_file/small_module.py: 446 tokens
- py_file/system_prompts_en.py: 289 tokens
- train/gain_safe_response.py: 276 tokens
- train/merge_safe_harmful.py: 230 tokens
- train/gain_harmful_response.py: 214 tokens
- py_file/utils.py: 83 tokens


## Tutorial and Enhancement Suggestions

# AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Tutorial

## Project Overview

The AdaPPA (Adaptive Position Pre-Fill Attack) project implements a novel jailbreak method for Large Language Models (LLMs). The repository contains code for attacking various LLMs, including ChatGLM3, Vicuna, Llama2, and GPT-4, by exploiting their vulnerabilities at different output stages.

### Project Structure

The repository is organized into several directories:

- `py_file/`: Contains core functionality and utility scripts
- `test/`: Includes main testing scripts
- `train/`: Houses scripts for data preparation and model training
- `observation/`: Contains scripts for observing and analyzing model behavior

## Key Components and Functionality

### 1. Model Chat Interface (`py_file/model_chat.py`)

This file defines functions for interacting with various LLMs:

- `baichuan2_kaiyuan_13b` and `baichuan2_kaiyuan_7b`: Interface for Baichuan2 models
- `vicuna_13b_kaiyuan` and `vicuna_7b_kaiyuan`: Interface for Vicuna models
- `llama2_13b_kaiyuan` and `llama2_7b_kaiyuan`: Interface for Llama2 models
- `llama3_8b_kaiyuan`: Interface for Llama3 model
- `ChatGLM3_kaiyuan`: Interface for ChatGLM3 model

These functions handle prompt formatting, model inference, and response processing.

### 2. Model Loading (`py_file/model_load.py`)

This script contains functions to load various pre-trained models and their respective tokenizers:

- `load_model_baichuan2_13b` and `load_model_baichuan2_7b`
- `load_model_vicuna_13b` and `load_model_vicuna_7b`
- `load_model_llama2_13b` and `load_model_llama2_7b`
- `load_model_llama3_8b`
- `load_model_chatglm3_6b`

### 3. Attack Implementation (`test/main.py` and `test/main_2.py`)

These scripts implement the core AdaPPA attack:

1. Load target questions and harmful responses
2. Generate attack prompts using different pre-fill strategies
3. Send attack prompts to target models
4. Evaluate responses using toxicity detection models

The attack process aligns with the methodology described in the paper, including problem rewriting, pre-fill generation, prompt combination, and attack evaluation.

### 4. Data Preparation (`train/` directory)

Scripts in this directory handle data preparation for the attack:

- `gain_safe_response.py`: Generates safe responses using Llama2
- `gain_harmful_response.py`: Extracts harmful responses from training data
- `merge_safe_harmful.py`: Combines safe and harmful responses
- `gain_chatglm3_data.py`: Prepares data specifically for attacking ChatGLM3

### 5. Utility Functions (`py_file/small_module.py`)

This file contains various utility functions for text processing, similarity computation, and file I/O operations.

### 6. System Prompts (`py_file/system_prompts_en.py`)

Defines prompts used for question rewriting, which is a key step in the AdaPPA attack process.

## Relation to Research Paper Concepts

The code implements the four main steps of the AdaPPA framework as described in the paper:

1. **Problem Rewrite**: Implemented in `py_file/system_prompts_en.py`
2. **Pre-fill Generation**: Handled in `train/gain_safe_response.py` and `train/gain_harmful_response.py`
3. **Prompt Combination**: Implemented in `test/main.py` and `test/main_2.py`
4. **Attack and Judge**: Also implemented in the test scripts, using various toxicity detection methods

The code also reflects the paper's focus on adaptive pre-filling by testing different lengths and combinations of safe and harmful content.

## Notable Techniques

1. **Adaptive Pre-filling**: The attack tries different pre-fill strategies, starting with partial harmful content and progressively increasing its presence.
2. **Multi-model Support**: The code supports attacking various LLMs, allowing for comparative analysis.
3. **Toxicity Detection**: Multiple methods (Llama Guard, OpenAI, Perspective API) are used to evaluate the harmfulness of generated responses.

# Potential Enhancements

1. **Dynamic Pre-fill Strategy**
   - Implement a reinforcement learning approach to dynamically adjust pre-fill content based on model responses
   - This could improve attack efficiency and success rate by learning optimal pre-fill patterns for each model

2. **Cross-model Transfer Learning**
   - Develop a method to transfer successful attack patterns from one model to another
   - This could reveal common vulnerabilities across different LLM architectures

3. **Defensive Mechanism Integration**
   - Incorporate state-of-the-art LLM defense mechanisms into the testing pipeline
   - This would allow for real-time evaluation of attack effectiveness against protected models

4. **Multi-turn Conversation Attacks**
   - Extend the attack to multi-turn conversations, exploring how vulnerabilities evolve over extended interactions
   - This could uncover more subtle jailbreak opportunities in conversational AI systems

5. **Prompt Optimization using Genetic Algorithms**
   - Implement a genetic algorithm to evolve and optimize attack prompts
   - This could lead to the discovery of novel and highly effective jailbreak patterns

These enhancements would address limitations mentioned in the paper and push the research forward by exploring more sophisticated attack strategies and their implications for LLM security.