#Tags
[[Research/Research Papers/2405.20413v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters
**Authors:** Haibo Jin, Andy Zhou, Joe D. Menke, Haohan Wang
**Affiliations:** University of Illinois at Urbana-Champaign, Lapis Labs
**Publication Date:** May 30, 2024

Key Contributions:
- Introduction of JAMBench, a benchmark for evaluating moderation guardrails in LLMs
- Development of JAM (Jailbreak Against Moderation), a novel jailbreak method using cipher characters
- Extensive experiments on four LLMs demonstrating JAM's effectiveness

Problem Statement:
Large Language Models (LLMs) are vulnerable to jailbreak attacks that bypass protective measures. Recent LLMs have incorporated moderation guardrails, but existing benchmarks lack questions that trigger these guardrails, making it difficult to evaluate jailbreak effectiveness.

Methodology:
1. Created JAMBench: 160 manually crafted instructions covering four risk categories at multiple severity levels
2. Developed JAM method:
   - Uses jailbreak prefixes to bypass input-level filters
   - Employs a fine-tuned shadow model to generate cipher characters for bypassing output-level filters
3. Tested on four LLMs: GPT-3.5, GPT-4, Gemini, and Llama-3
4. Compared JAM with baseline methods: GCG, ICA, PAIR, CipherChat, and GUARD

Main Results:
- JAM achieved higher jailbreak success rates (~19.88 times higher) than baselines
- JAM demonstrated lower filtered-out rates (~1/6 of baselines)
- Consistent performance across different LLMs and risk categories

Qualitative Analysis:
- JAM's effectiveness stems from its dual-strategy approach:
  1. Using jailbreak prefixes to bypass input-level filters
  2. Employing cipher characters to mislead output-level moderation mechanisms
- The method exploits the shared sensitivity of moderation guardrails across different LLMs
- JAM's success highlights the need for more robust defense mechanisms in LLMs

Limitations:
- The study focuses on text-based jailbreaks and may not apply to other modalities
- The effectiveness of JAM may vary with future updates to LLM moderation systems
- Ethical considerations in developing and testing jailbreak methods

Conclusion and Future Work:
- JAM demonstrates the vulnerability of current moderation guardrails in LLMs
- The authors propose two potential countermeasures:
  1. Output Complexity-Aware Defense
  2. Secondary LLM-based Audit Defense
- Future work may focus on developing more robust defense mechanisms and exploring the transferability of jailbreak methods across different LLM architectures

Tools Introduced:
- JAMBench: A benchmark for evaluating moderation guardrails in LLMs
- JAM (Jailbreak Against Moderation): A jailbreak method using cipher characters

Relevant Figures:
- Figure 1: Examples of jailbreaks, including JAM's approach
- Figure 2: Three types of structural built-in safe guardrails
- Figure 3: Overview workflow of JAM for generating a jailbreak prompt