#Tags
[[Research/Research Papers/2406.08705v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0005/CreateProxyMLModel

**Title:** When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search
**Authors:** Xuan Chen, Yuzhou Nie, Wenbo Guo, Xiangyu Zhang
**Affiliations:** Purdue University, University of California, Santa Barbara
**Publication Date:** June 13, 2024

Summary:
This paper introduces RLbreaker, a novel black-box jailbreaking attack against large language models (LLMs) using deep reinforcement learning (DRL). The authors model jailbreaking as a search problem and use DRL to guide the search process, improving efficiency over existing stochastic search methods like genetic algorithms.

Key Contributions:
- RLbreaker: A DRL-based black-box jailbreaking attack framework
- Novel reward function for evaluating jailbreaking success
- Customized proximal policy optimization (PPO) algorithm for jailbreaking
- Extensive evaluation against 6 state-of-the-art LLMs and 3 defenses
- Demonstration of transferability across different LLMs

Problem Statement:
Existing black-box jailbreaking attacks against LLMs rely on inefficient stochastic search methods like genetic algorithms, limiting their effectiveness. The paper aims to develop a more efficient and guided search approach using DRL.

Methodology:
1. Model jailbreaking as a search problem for optimal prompt structures
2. Design DRL system with:
   - State: Current jailbreaking prompt
   - Action: Selection of predefined mutators
   - Reward: Cosine similarity between target LLM response and reference answer
3. Customize PPO algorithm for training
4. Evaluate against 6 LLMs, including Mixtral-8x7B-Instruct and GPT-3.5-turbo
5. Compare with 5 baseline attacks: GPTFUZZER, PAIR, Cipher, AutoDAN, and GCG
6. Test robustness against 3 defenses: rephrasing, perplexity, and RAIN

Main Results:
1. RLbreaker outperforms baselines in jailbreaking effectiveness across all tested LLMs
2. Demonstrates robustness against 3 SOTA defenses
3. Trained agents show transferability across different LLMs
4. Ablation study validates key design choices:
   - DRL agent vs. random/LLM-based agent
   - Cosine similarity reward vs. keyword matching
   - Mutator-based actions vs. token-level actions

Qualitative Analysis:
- RLbreaker's success highlights the potential of guided search in overcoming LLM safety measures
- Transferability of trained agents suggests common vulnerabilities across different LLMs
- The effectiveness against very large models (e.g., Mixtral-8x7B-Instruct) raises concerns about the robustness of current alignment techniques

Limitations:
- Computational resources required for training DRL agents
- Potential for misuse in real-world attacks against LLMs
- Reliance on reference answers from unaligned models during training

Conclusion and Future Work:
The paper demonstrates the effectiveness of DRL-guided search for jailbreaking LLMs, outperforming existing methods. Future work suggestions include:
1. Expanding the action space to incorporate more jailbreaking strategies
2. Improving the reward function to reduce false negatives
3. Extending the framework to multi-modal models and other AI systems
4. Exploring advanced AI agents integrated with LLMs and RL

Tools Introduced:
- RLbreaker: DRL-based jailbreaking attack framework (GitHub repository not provided in the paper)