#Tags
[[Research/Research Papers/2405.13077v1.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation
**Authors:** Govind Ramesh, Yao Dou, Wei Xu
**Affiliation:** Georgia Institute of Technology
**Publication Date:** May 21, 2024

Summary:
This paper introduces Iterative Refinement Induced Self-Jailbreak (IRIS), a novel approach for jailbreaking large language models (LLMs) using only black-box access. IRIS leverages the model's own capabilities to refine adversarial prompts and enhance harmful outputs, achieving high success rates with minimal queries.

Key Contributions:
- Introduction of IRIS, a self-jailbreaking method for LLMs
- Demonstration of near-perfect jailbreak success rates on GPT-4 and GPT-4 Turbo
- Significant improvement in query efficiency compared to previous methods
- Exploration of self-jailbreaking and output refinement concepts

Problem Statement:
How to develop an efficient, interpretable, and highly successful method for jailbreaking advanced LLMs using only black-box access?

Methodology:
1. Iterative Refinement:
   - Present initial adversarial prompt to target LLM
   - If rejected, request self-explanation from attacker LLM
   - Generate refined prompt based on explanation
   - Repeat until successful or maximum iterations reached

2. Rate and Enhance:
   - Rate harmfulness of output on a scale of 1-5
   - Refine output to maximize harmfulness rating

3. Experimental Setup:
   - Dataset: AdvBench Subset (50 adversarial prompts)
   - Models: GPT-4 and GPT-4 Turbo
   - Comparison with PAIR and TAP methods
   - Human evaluation for attack success rate

Main Results:
1. IRIS achieves 98% success rate on GPT-4 and 92% on GPT-4 Turbo in under 7 queries
2. IRIS-2x (two independent trials) achieves 100% success rate on GPT-4 and 98% on GPT-4 Turbo in under 13 queries
3. Significantly outperforms PAIR and TAP in both success rate and query efficiency
4. Transfer attacks on Claude-3 models show 80% success rate on Opus and 92-94% on Sonnet

Qualitative Analysis:
- Self-jailbreaking concept proves highly effective, leveraging the model's own capabilities against itself
- Iterative refinement through self-explanation crucial for overcoming well-aligned LLMs' safeguards
- Output enhancement step significantly increases harmfulness of generated content
- IRIS demonstrates the potential vulnerabilities of advanced LLMs to interpretable jailbreaking methods

Limitations:
- Ethical concerns regarding the generation of harmful content
- Potential for misuse in real-world applications
- Limited evaluation on open-source models due to capability constraints

Conclusion and Future Work:
IRIS establishes a new standard for interpretable jailbreaking methods, demonstrating the potential of self-jailbreaking and output refinement. Future research should focus on developing robust defense mechanisms against such attacks and exploring the ethical implications of advanced jailbreaking techniques.

Relevant Figures:
Figure 1: Diagram of IRIS self-jailbreaking method, illustrating the iterative refinement and rate+enhance steps.

New Tool:
IRIS (Iterative Refinement Induced Self-Jailbreak) - A novel method for jailbreaking LLMs using self-explanation and output enhancement. No GitHub repository mentioned.