#Tags
[[Research/Research Papers/2406.11717v2.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0044/FullMLModelAccess
#AMLT0054/LLMJailbreak
#AMLT0043/CraftAdversarialData

**Title:** Refusal in Language Models Is Mediated by a Single Direction
**Authors:** Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, Neel Nanda
**Publication Date:** June 17, 2024 (last updated July 15, 2024)

Summary:
This paper investigates the mechanisms behind refusal behavior in large language models (LLMs) and demonstrates that refusal is mediated by a single direction in the model's activation space. The authors propose a novel white-box jailbreak method to disable refusal while maintaining other model capabilities.

Key Contributions:
- Identification of a single direction mediating refusal behavior across 13 open-source chat models
- Development of a white-box jailbreak method using weight orthogonalization
- Mechanistic analysis of adversarial suffixes and their effect on refusal behavior
- Demonstration of the brittleness of current safety fine-tuning methods

Problem Statement:
The paper addresses the lack of understanding of refusal mechanisms in LLMs and aims to uncover the underlying processes that allow models to refuse harmful instructions while accepting benign ones.

Methodology:
1. Extraction of a "refusal direction" using difference-in-means between harmful and harmless instructions
2. Directional ablation and activation addition experiments to manipulate refusal behavior
3. Weight orthogonalization technique for a white-box jailbreak method
4. Evaluation of refusal and safety scores using string matching and META LLAMA GUARD 2
5. Analysis of adversarial suffixes and their impact on refusal direction propagation

Main Results:
1. Refusal behavior is mediated by a single direction across various open-source chat models
2. Ablating the refusal direction effectively disables refusal on harmful instructions
3. Adding the refusal direction induces refusal on harmless instructions
4. The proposed white-box jailbreak method achieves high attack success rates while maintaining model coherence
5. Adversarial suffixes suppress the propagation of the refusal-mediating direction

Qualitative Analysis:
- The study reveals the simplicity and fragility of current safety fine-tuning methods in LLMs
- The findings suggest that safety mechanisms can be easily circumvented with minimal impact on other model capabilities
- The research demonstrates the practical utility of model-internals based interpretability for understanding and controlling model behavior

Limitations:
- The study is limited to open-source models and may not generalize to proprietary or larger-scale models
- The methodology for extracting the refusal direction may not be optimal and relies on heuristics
- The analysis of adversarial suffixes is limited to a single model and example

Conclusion and Future Work:
The paper concludes that refusal behavior in LLMs is mediated by a single direction and can be easily manipulated. The authors suggest that their findings contribute to the growing body of literature highlighting the fragility of current safety mechanisms in AI systems. Future work may focus on developing more robust safety techniques and exploring the implications of these findings for AI alignment and security.

Relevant Figures:
- Figure 1: Ablation of the refusal direction reduces refusal rates and elicits unsafe completions
- Figure 5: Cosine similarity between last token residual stream activations and refusal direction
- Figure 6: Analysis of attention head outputs and attention patterns in the presence of adversarial suffixes

New Tools:
The authors introduce a novel white-box jailbreak method using weight orthogonalization. The code for this method is available at https://github.com/andyrdt/refusal_direction.