#Tags
[[Research/Research Papers/2407.04295v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0051/LLMPromptInjection

**Title:** Jailbreak Attacks and Defenses Against Large Language Models: A Survey
**Authors:** Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, Qi Li
**Publication Date:** July 5, 2024

Key Contributions:
- Comprehensive taxonomy of jailbreak attack and defense methods for LLMs
- Classification of attack methods into black-box and white-box categories
- Categorization of defense methods into prompt-level and model-level defenses
- Detailed subdivision of attack and defense methods into distinct sub-classes
- Investigation and comparison of current evaluation methods for jailbreak attacks

Problem Statement:
The paper addresses the challenge of "jailbreaking" in Large Language Models (LLMs), where adversarial prompts are designed to induce models to generate malicious responses against usage policies and societal norms.

Methodology:
- Literature review and analysis of existing jailbreak attack and defense methods
- Categorization and classification of methods based on their characteristics and approaches
- Investigation of evaluation methods used in jailbreak research

Main Results and Findings:
1. Attack Methods:
   - White-box attacks:
     - Gradient-based
     - Logits-based
     - Fine-tuning-based
   - Black-box attacks:
     - Template completion
     - Prompt rewriting
     - LLM-based generation

2. Defense Methods:
   - Prompt-level defenses:
     - Prompt detection
     - Prompt perturbation
     - System prompt safeguard
   - Model-level defenses:
     - SFT-based
     - RLHF-based
     - Gradient and logit analysis
     - Refinement
     - Proxy defense

3. Evaluation Methods:
   - Attack Success Rate (ASR) as a primary metric
   - Perplexity for measuring readability and fluency of jailbreak prompts
   - Various datasets and toolkits for comprehensive evaluation

Qualitative Analysis:
- The paper highlights the evolving nature of jailbreak attacks and the corresponding development of defense mechanisms
- There is a trend towards more sophisticated and efficient attack methods, such as LLM-based generation and prompt rewriting techniques
- Defense methods are becoming more diverse, ranging from simple prompt detection to complex model-level defenses using reinforcement learning

Limitations and Considerations:
- The rapidly evolving field may lead to some recent developments not being included in the survey
- The effectiveness of different attack and defense methods may vary depending on the specific LLM and use case

Conclusion and Future Work:
- The survey provides a comprehensive overview of the current state of jailbreak attacks and defenses for LLMs
- Future research directions include:
  1. Developing more robust and generalizable defense mechanisms
  2. Improving evaluation methods for better comparison of attack and defense techniques
  3. Addressing the trade-off between model safety and performance
  4. Exploring the ethical implications of jailbreak research and its potential impact on AI safety

Relevant Figures:
- Figure 1: Taxonomy and relationship of attack and defense methods
- Figure 2: Taxonomy of jailbreak attack methods
- Figure 8: Taxonomy of jailbreak defense methods

New Tools:
- JailbreakEval: An integrated toolkit for evaluating jailbreak attempts against LLMs (GitHub repository not provided in the paper)