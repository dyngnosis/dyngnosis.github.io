#Tags
[[Research/Research Papers/2407.20859v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0031/ErodeMLModelIntegrity
#AMLT0029/DenialOfMLService
#AMLT0040/MLModelInferenceAPIAccess

**Title:** Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification
**Authors:** Boyang Zhang, Yicong Tan, Yun Shen, Ahmed Salem, Michael Backes, Savvas Zannettou, Yang Zhang
**Affiliations:** CISPA Helmholtz Center for Information Security, NetApp, Microsoft, TU Delft
**Publication date:** July 30, 2024

Summary:
This paper introduces a novel attack on LLM-based autonomous agents that causes malfunctions by inducing repetitive or irrelevant actions. The authors conduct comprehensive evaluations of various attack methods, surfaces, and agent properties to identify vulnerabilities. The study reveals that these attacks can achieve failure rates exceeding 80% in multiple scenarios and demonstrates the realistic risks in multi-agent settings.

Key Contributions:
- Introduction of a new attack type targeting LLM agents' normal functioning
- Comprehensive evaluation of attack effectiveness across various dimensions
- Demonstration of realistic risks in multi-agent scenarios
- Analysis of defense mechanisms and their limitations against the proposed attacks

Problem Statement:
How can autonomous LLM agents be compromised through malfunction amplification, and what are the implications for their security and reliability in real-world applications?

Methodology:
1. Development of attack types: infinite loop and incorrect function execution
2. Implementation of attack methods: prompt injection, adversarial perturbation, and adversarial demonstration
3. Evaluation using an agent emulator for large-scale experiments
4. Case studies with implemented Gmail and CSV agents
5. Analysis of attack surfaces: input instructions, intermediate outputs, and agent memory
6. Advanced attack scenarios in multi-agent environments
7. Evaluation of defense mechanisms using self-examination techniques

Main Results:
1. Infinite loop attacks achieved a 59.4% failure rate, compared to a 15.3% baseline
2. Prompt injection was the most effective attack method
3. Attacks through intermediate outputs were less effective but still posed risks
4. Multi-agent scenarios demonstrated high attack success rates (up to 80%)
5. Self-examination defenses were less effective against the proposed attacks compared to previous harmful action detection methods

Qualitative Analysis:
- The study highlights the importance of considering non-obvious vulnerabilities in LLM agents
- The attacks exploit the inherent instabilities of LLM agents, making them harder to detect and mitigate
- The multi-agent scenario results emphasize the potential for cascading failures in complex systems
- The difficulty in detecting these attacks using LLMs alone underscores the need for more robust defense mechanisms

Limitations:
- Limited number of implemented agents in case studies
- Focus on specific types of agents (email and CSV processing)
- Evaluation limited to three LLM variants (GPT-3.5-Turbo, GPT-4, Claude-2)

Conclusion and Future Work:
The paper demonstrates the vulnerability of LLM agents to malfunction amplification attacks and highlights the challenges in detecting and mitigating these threats. Future work should focus on developing more effective defense mechanisms and expanding the evaluation to a broader range of agent types and LLM models.

Relevant Figures:
- Figure 1: Overview of the attack exacerbating the instabilities of LLM agents
- Figure 3 & 4: Attack success rate with respect to the ratio of attack prompt and complete prompt

Tools Introduced:
1. Agent Emulator: A framework for simulating LLM agent interactions in a virtual environment
2. Gmail Agent: An implemented autonomous email management tool using Google's Gmail API
3. CSV Agent: An implemented data analysis agent for reading, analyzing, and modifying CSV files