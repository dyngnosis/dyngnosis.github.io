#Tags
[[Research/Research Papers/2311.09096v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0015/EvadeMLModel
#AMLT0031/ErodeMLModelIntegrity

**Title:** Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization
**Authors:** Zhexin Zhang, Junxiao Yang, Pei Ke, Fei Mi, Hongning Wang, Minlie Huang
**Affiliations:** The Conversational AI (CoAI) group, DCST, Tsinghua University; Huawei Noah's Ark Lab
**Publication Date:** November 15, 2023 (arXiv)

Key Contributions:
- Identifies the conflict between helpfulness and safety goals as a key factor in jailbreaking success
- Proposes goal prioritization as a defense mechanism against jailbreaking attacks
- Demonstrates effectiveness of goal prioritization in both inference and training stages
- Reveals insights into the relationship between LLM capability and safety

Problem Statement:
The paper addresses the vulnerability of Large Language Models (LLMs) to jailbreaking attacks, which exploit the conflict between helpfulness and safety goals to bypass safety alignments.

Methodology:
1. Goal prioritization during inference:
   - Design a plug-and-play prompting method
   - Include in-context examples and internal thoughts analysis
2. Goal prioritization during training:
   - Create contrastive training instances with opposite goal priority requirements
   - Finetune models to adhere to goal prioritization

Main Results:
1. Inference-stage goal prioritization:
   - Reduced ChatGPT's Attack Success Rate (ASR) from 66.4% to 3.6%
   - Effective across various LLMs and jailbreaking techniques
2. Training-stage goal prioritization:
   - Reduced Llama2-13B's ASR from 71.0% to 6.6%
   - Improved generalization, reducing ASR by half even without jailbreaking samples in training

Qualitative Analysis:
- Stronger LLMs are more vulnerable to jailbreaking attacks but also more capable of being defended
- Goal prioritization helps LLMs better understand and adhere to safety requirements
- The method generalizes well to different types of jailbreaking attacks and unsafe queries

Limitations:
- Slight reduction in general performance for some models, especially smaller ones
- Additional decoding costs due to internal thoughts generation

Conclusion and Future Work:
- Goal prioritization is an effective defense against jailbreaking attacks
- The approach provides insights into the relationship between LLM capability and safety
- Future work may focus on minimizing additional costs while maintaining high safety levels

Tools Introduced:
- GitHub repository: https://github.com/thu-coai/JailbreakDefense_GoalPriority