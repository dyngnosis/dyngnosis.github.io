#Tags
[[Research/Research Papers/2403.08424v2.pdf]]

#AMLT0054/LLMJailbreak
#AMLT0051/LLMPromptInjection
#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0042/VerifyAttack

**Title:** Distract Large Language Models for Automatic Jailbreak Attack
**Authors:** Zeguan Xiao, Yan Yang, Guanhua Chen, Yun Chen
**Affiliations:** Shanghai University of Finance and Economics, Southern University of Science and Technology
**Publication date:** 13 March 2024

Summary:
This paper introduces a novel black-box jailbreak framework called Distraction based Adversarial Prompts (DAP) for automated red teaming of Large Language Models (LLMs). The framework leverages the distractibility and over-confidence phenomena of LLMs to generate effective jailbreak templates.

Key Contributions:
- Introduction of DAP, a black-box jailbreak framework for automated red teaming of LLMs
- Design of malicious content concealing and memory-reframing techniques
- Iterative optimization algorithm for jailbreak template generation
- Extensive experiments demonstrating effectiveness, scalability, and transferability across various LLMs
- Evaluation of existing jailbreak defense methods against the proposed attack

Problem Statement:
Despite extensive efforts to align LLMs with human values, they remain vulnerable to jailbreak attacks that can circumvent safety measures and elicit undesirable behaviors.

Methodology:
1. Malicious Content Concealing: Embed malicious queries within complex, unrelated scenarios to distract the LLM.
2. Memory-Reframing Mechanism: Instruct the LLM to focus on the auxiliary task (malicious query) by leveraging the over-confidence phenomenon.
3. Iterative Prompt Optimization: Use an attacker LLM to generate jailbreak templates, evaluated by a target LLM and a judgment model.
4. Judgment Model: Fine-tuned DeBERTa model for evaluating jailbreak success.

Experimental Setup:
- Datasets: Subset of AdvBench harmful behaviors dataset
- Models: Open-source (Vicuna-13B-v1.5, LLaMA-2-7B-chat) and closed-source (ChatGPT, GPT-4) LLMs
- Baselines: GCG, DeepInception, PAIR, GPTFuzzer
- Metrics: Top-1 and Top-5 Attack Success Rate (ASR)

Main Results:
- DAP achieves ≥64% Top-1 ASR and ≥77.3% Top-5 ASR on both GPT-3.5 versions
- 38.0% Top-1 ASR on GPT-4
- Outperforms baselines on most open-source and closed-source LLMs
- Demonstrates strong transferability across different malicious queries and target models

Qualitative Analysis:
- DAP's success is attributed to its ability to leverage LLM distractibility and over-confidence
- The memory-reframing mechanism effectively shifts the LLM's focus from the complex scenario to the malicious query
- The framework's black-box nature and universal jailbreak templates make it more practical and scalable than previous methods

Limitations:
- Limited to single-turn conversations
- Computational resource constraints limited testing on additional LLMs
- Potential for misuse if not properly controlled

Conclusion and Future Work:
The paper demonstrates the effectiveness of DAP in jailbreaking LLMs and highlights the need for more robust defense strategies. Future work should focus on:
- Exploring multi-turn conversations
- Developing more effective defense methods against distraction-based attacks
- Investigating the broader implications of LLM distractibility on general task performance

Tools Introduced:
- DAP framework (no GitHub repository mentioned)