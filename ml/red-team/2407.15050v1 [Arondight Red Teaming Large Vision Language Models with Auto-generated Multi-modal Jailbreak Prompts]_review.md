#Tags
[[Research/Research Papers/2407.15050v1.pdf]]

#AMLT0015/EvadeMLModel
#AMLT0043/CraftAdversarialData
#AMLT0054/LLMJailbreak

**Title:** Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts

**Authors:** Yi Liu, Chengjun Cai, Xiaoli Zhang, Xingliang Yuan, Cong Wang

**Affiliations:** 
- City University of Hong Kong, China
- City University of Hong Kong (Dongguan), China  
- University of Science and Technology Beijing, China
- The University of Melbourne, Australia

**Publication Date:** July 21, 2024

Summary:
This paper introduces Arondight, a comprehensive red teaming framework for evaluating the security of Large Vision Language Models (VLMs). The framework uses auto-generated multi-modal jailbreak prompts to test VLMs' vulnerabilities in generating harmful content.

Key Contributions:
- Introduces Arondight, a standardized red team framework for VLMs
- Develops an automated multi-modal jailbreak attack method
- Integrates entropy bonuses and novelty reward metrics to enhance evaluation comprehensiveness
- Evaluates 10 cutting-edge VLMs, exposing significant security vulnerabilities
- Categorizes VLMs based on safety levels and provides reinforcement recommendations

Problem Statement:
While Large Language Models (LLMs) have undergone extensive security evaluations using red teaming frameworks, VLMs lack a well-developed framework for comprehensive security assessment, particularly regarding the generation of harmful content.

Methodology:
1. Universal Prompt Template-driven Red Team VLM:
   - Designs a novel jailbreak prompt template for generating toxic images
   - Uses GPT-4 as the red team VLM for high-quality image generation
   - Implements a perturbation optimization strategy and prompt template correction

2. Diversity-driven Red Team LLM:
   - Incorporates entropy bonus, novelty reward, and correlation metrics
   - Uses reinforcement learning to guide the red team LLM in generating diverse and relevant textual prompts

3. Evaluation:
   - Tests 10 VLMs, including commercial and open-source models
   - Covers 14 prohibited scenarios defined by OpenAI
   - Uses both one-shot and few-shot attack scenarios
   - Employs human evaluation and toxicity detection APIs for assessment

Main Results:
1. Arondight achieves an average attack success rate of 84.5% on GPT-4 across all 14 prohibited scenarios.
2. The framework outperforms existing attacks, including textual jailbreak attacks and other multi-modal jailbreak methods.
3. Different VLMs show varying levels of vulnerability, with some performing better in political and professional contexts.
4. The study reveals potential vulnerabilities in VLM alignment mechanisms, particularly in handling multi-modal inputs and adversarial samples.

Qualitative Analysis:
- The study highlights the importance of comprehensive security evaluation for VLMs, especially as they become more prevalent in real-world applications.
- The findings suggest that current VLM safety measures may be inadequate, particularly when dealing with multi-modal inputs.
- The categorization of VLMs into safety levels provides valuable insights for developers and users in selecting appropriate models for different applications.

Limitations:
- The study focuses on a specific set of VLMs and may not represent all existing or future models.
- The effectiveness of the framework may vary as VLM developers implement new safety measures.
- Ethical considerations limit the full disclosure of the jailbreak dataset and attack methods.

Conclusion and Future Work:
- Arondight provides a valuable tool for assessing and improving the safety of VLMs.
- The authors plan to release their multimodal prompt dataset and red team code after ethics committee approval.
- Future work may involve developing more robust defense mechanisms for VLMs and expanding the framework to cover a broader range of models and scenarios.

Relevant Figures/Tables:
- Table 3: Evaluation results on GPT-4 and Qwen-VL (One-shot)
- Table 4: Evaluation results on GPT-4 and Qwen-VL (Few-shot)
- Figure 3: Evaluation of ten VLMs using the Arondight framework
- Figure 4: Safety level classification results and corresponding safety tips

New Tools:
- Arondight: A red teaming framework for evaluating VLMs (GitHub repository not provided in the paper)