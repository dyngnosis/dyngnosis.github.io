- title: Adversarial Tuning Defending Against Jailbreak Attacks for LLMs review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.06622v1
    [Adversarial Tuning Defending Against Jailbreak Attacks for LLMs]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2310.06987v1
    [Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: Rethinking Jailbreaking through the Lens of Representation Engineering review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2401.06824v3
    [Rethinking Jailbreaking through the Lens of Representation Engineering]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by
    Step review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.03869v1
    [Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: BaThe Defense against the Jailbreak Attack in Multimodal Large Language Models
    by Treating Harmful Instruction as Backdoor Trigger review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.09093v1
    [BaThe Defense against the Jailbreak Attack in Multimodal Large Language Models
    by Treating Harmful Instruction as Backdoor Trigger]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0018/BackdoorMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: BlackDAN A Black-Box Multi-Objective Approach for Effective and Contextual
    Jailbreaking of Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.09804v2
    [BlackDAN A Black-Box Multi-Objective Approach for Effective and Contextual Jailbreaking
    of Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Intention Analysis Makes LLMs A Good Jailbreak Defender review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2401.06561v3
    [Intention Analysis Makes LLMs A Good Jailbreak Defender]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Jailbreak Attacks and Defenses Against Large Language Models A Survey review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.04295v2
    [Jailbreak Attacks and Defenses Against Large Language Models A Survey]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Unveiling the Safety of GPT-4o An Empirical Study using Jailbreak Attacks
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.06302v2
    [Unveiling the Safety of GPT-4o An Empirical Study using Jailbreak Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0042/VerifyAttack
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: AttnGCG Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.09040v1
    [AttnGCG Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: AutoDefense Multi-Agent LLM Defense against Jailbreak Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2403.04783v1
    [AutoDefense Multi-Agent LLM Defense against Jailbreak Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Making Them Ask and Answer Jailbreaking Large Language Models in Few Queries
    via Disguise and Reconstruction review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.18104v2
    [Making Them Ask and Answer Jailbreaking Large Language Models in Few Queries
    via Disguise and Reconstruction]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.04031v2
    [Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0044/FullMLModelAccess
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Jailbreaking as a Reward Misspecification Problem review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.14393v2
    [Jailbreaking as a Reward Misspecification Problem]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Rethinking How to Evaluate Language Model Jailbreak review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.06407v3
    [Rethinking How to Evaluate Language Model Jailbreak]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0042/VerifyAttack
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Jailbreaking as a Reward Misspecification Problem review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.14393v3
    [Jailbreaking as a Reward Misspecification Problem]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Can Large Language Models Automatically Jailbreak GPT-4V review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.16686v2
    [Can Large Language Models Automatically Jailbreak GPT-4V]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: AutoBreach Universal and Adaptive Jailbreaking with Efficient Wordplay-Guided
    Optimization review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.19668v1
    [AutoBreach Universal and Adaptive Jailbreaking with Efficient Wordplay-Guided
    Optimization]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: AutoDAN-Turbo A Lifelong Agent for Strategy Self-Exploration to Jailbreak
    LLMs review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.05295v2
    [AutoDAN-Turbo A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0016/ObtainCapabilities
  - AMLT0017/DevelopCapabilities
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Improved Techniques for Optimization-Based Jailbreaking on Large Language
    Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.21018v2
    [Improved Techniques for Optimization-Based Jailbreaking on Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Jailbreaking Large Language Models Against Moderation Guardrails via Cipher
    Characters review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.20413v1
    [Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Endless Jailbreaks with Bijection Learning review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.01294v1
    [Endless Jailbreaks with Bijection Learning]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Voice Jailbreak Attacks Against GPT-4o review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.19103v1
    [Voice Jailbreak Attacks Against GPT-4o]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Great, Now Write an Article About That The Crescendo Multi-Turn LLM Jailbreak
    Attack review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.01833v2
    [Great, Now Write an Article About That The Crescendo Multi-Turn LLM Jailbreak
    Attack]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: '[WIP review'
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.12702v2
    [[WIP] Jailbreak Paradox The Achilles' Heel of LLMs]_review.md
  tags:
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  date: '2024-10-22'
- title: Safe Unlearning A Surprisingly Effective and Generalizable Solution to Defend
    Against Jailbreak Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.02855v1
    [Safe Unlearning A Surprisingly Effective and Generalizable Solution to Defend
    Against Jailbreak Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0018/BackdoorMLModel
  - AMLT0020/PoisonTrainingData
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: AutoDAN Generating Stealthy Jailbreak Prompts on Aligned Large Language Models
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2310.04451v2
    [AutoDAN Generating Stealthy Jailbreak Prompts on Aligned Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0017/DevelopCapabilities
  date: '2024-10-22'
- title: Merging Improves Self-Critique Against Jailbreak Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.07188v2
    [Merging Improves Self-Critique Against Jailbreak Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0018/BackdoorMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Boosting Jailbreak Transferability for Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.15645v1
    [Boosting Jailbreak Transferability for Large Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Competition Report Finding Universal Jailbreak Backdoors in Aligned LLMs
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.14461v2
    [Competition Report Finding Universal Jailbreak Backdoors in Aligned LLMs]_review.md
  tags:
  - AMLT0018/BackdoorMLModel
  - AMLT0020/PoisonTrainingData
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: FuzzLLM A Novel and Universal Fuzzing Framework for Proactively Discovering
    Jailbreak Vulnerabilities in Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2309.05274v2
    [FuzzLLM A Novel and Universal Fuzzing Framework for Proactively Discovering Jailbreak
    Vulnerabilities in Large Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0016/ObtainCapabilities
  - AMLT0017/DevelopCapabilities
  date: '2024-10-22'
- title: Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.20653v2
    [Enhancing Jailbreak Attack Against Large Language Models through Silent Tokens]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: Pruning for Protection Increasing Jailbreak Resistance in Aligned LLMs Without
    Fine-Tuning review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2401.10862v2
    [Pruning for Protection Increasing Jailbreak Resistance in Aligned LLMs Without
    Fine-Tuning]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0018/BackdoorMLModel
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: White-box Multimodal Jailbreaks Against Large Vision-Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.17894v1
    [White-box Multimodal Jailbreaks Against Large Vision-Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0044/FullMLModelAccess
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Exploiting Uncommon Text-Encoded Structures for Automated Jailbreaks in LLMs
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.08754v2
    [Exploiting Uncommon Text-Encoded Structures for Automated Jailbreaks in LLMs]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Distract Large Language Models for Automatic Jailbreak Attack review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2403.08424v2
    [Distract Large Language Models for Automatic Jailbreak Attack]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Open Sesame! Universal Black Box Jailbreaking of Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2309.01446v4
    [Open Sesame! Universal Black Box Jailbreaking of Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0017/DevelopCapabilities
  date: '2024-10-22'
- title: Large Language Models Are Involuntary Truth-Tellers Exploiting Fallacy Failure
    for Jailbreak Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.00869v1
    [Large Language Models Are Involuntary Truth-Tellers Exploiting Fallacy Failure
    for Jailbreak Attacks]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Red teaming ChatGPT via Jailbreaking Bias, Robustness, Reliability and Toxicity
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2301.12867v4
    [Red teaming ChatGPT via Jailbreaking Bias, Robustness, Reliability and Toxicity]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0042/VerifyAttack
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: COLD-Attack Jailbreaking LLMs with Stealthiness and Controllability review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.08679v2
    [COLD-Attack Jailbreaking LLMs with Stealthiness and Controllability]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Multimodal Pragmatic Jailbreak on Text-to-image Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2409.19149v1
    [Multimodal Pragmatic Jailbreak on Text-to-image Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0016/ObtainCapabilities
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Jailbreaking Attack against Multimodal Large Language Model review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.02309v1
    [Jailbreaking Attack against Multimodal Large Language Model]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: InjecAgent Benchmarking Indirect Prompt Injections in Tool-Integrated Large
    Language Model Agents review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2403.02691v3
    [InjecAgent Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language
    Model Agents]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0051/LLMPromptInjection
  - AMLT0053/LLMPluginCompromise
  date: '2024-10-22'
- title: Arondight Red Teaming Large Vision Language Models with Auto-generated Multi-modal
    Jailbreak Prompts review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.15050v1
    [Arondight Red Teaming Large Vision Language Models with Auto-generated Multi-modal
    Jailbreak Prompts]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Poisoned LangChain Jailbreak LLMs by LangChain review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.18122v1
    [Poisoned LangChain Jailbreak LLMs by LangChain]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  - AMLT0057/LLMDataLeakage
  date: '2024-10-22'
- title: Towards Understanding Jailbreak Attacks in LLMs A Representation Space Analysis
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.10794v2
    [Towards Understanding Jailbreak Attacks in LLMs A Representation Space Analysis]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0044/FullMLModelAccess
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: SmoothLLM Defending Large Language Models Against Jailbreaking Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2310.03684v4
    [SmoothLLM Defending Large Language Models Against Jailbreaking Attacks]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Jailbreaking Text-to-Image Models with LLM-Based Agents review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.00523v2
    [Jailbreaking Text-to-Image Models with LLM-Based Agents]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0005/CreateProxyMLModel
  date: '2024-10-22'
- title: Fast Adversarial Attacks on Language Models In One GPU Minute review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.15570v1
    [Fast Adversarial Attacks on Language Models In One GPU Minute]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0054/LLMJailbreak
  - AMLT0057/LLMDataLeakage
  date: '2024-10-22'
- title: SafeDecoding Defending against Jailbreak Attacks via Safety-Aware Decoding
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.08983v4
    [SafeDecoding Defending against Jailbreak Attacks via Safety-Aware Decoding]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Information-Theoretical Principled Trade-off between Jailbreakability and
    Stealthiness on Vision Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.01438v1
    [Information-Theoretical Principled Trade-off between Jailbreakability and Stealthiness
    on Vision Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Multilingual Jailbreak Challenges in Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2310.06474v3
    [Multilingual Jailbreak Challenges in Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0057/LLMDataLeakage
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  date: '2024-10-22'
- title: GPTFUZZER Red Teaming Large Language Models with Auto-Generated Jailbreak
    Prompts review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2309.10253v4
    [GPTFUZZER Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  - AMLT0015/EvadeMLModel
  - AMLT0017/DevelopCapabilities
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: AgentDojo A Dynamic Environment to Evaluate Attacks and Defenses for LLM
    Agents review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.13352v2
    [AgentDojo A Dynamic Environment to Evaluate Attacks and Defenses for LLM Agents]_review.md
  tags:
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Image-to-Text Logic Jailbreak Your Imagination can Help You Do Anything review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.02534v1
    [Image-to-Text Logic Jailbreak Your Imagination can Help You Do Anything]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Are Large Language Models Really Bias-Free Jailbreak Prompts for Assessing
    Adversarial Robustness to Bias Elicitation review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.08441v1
    [Are Large Language Models Really Bias-Free Jailbreak Prompts for Assessing Adversarial
    Robustness to Bias Elicitation]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Visual Adversarial Examples Jailbreak Aligned Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2306.13213v2
    [Visual Adversarial Examples Jailbreak Aligned Large Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: JailbreakEval An Integrated Toolkit for Evaluating Jailbreak Attempts Against
    Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.09321v1
    [JailbreakEval An Integrated Toolkit for Evaluating Jailbreak Attempts Against
    Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: AmpleGCG Learning a Universal and Transferable Generative Model of Adversarial
    Suffixes for Jailbreaking Both Open and Closed LLMs review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.07921v2
    [AmpleGCG Learning a Universal and Transferable Generative Model of Adversarial
    Suffixes for Jailbreaking Both Open and Closed LLMs]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0017/DevelopCapabilities
  - AMLT0005/CreateProxyMLModel
  date: '2024-10-22'
- title: Jailbreaking Large Language Models with Symbolic Mathematics review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2409.11445v1
    [Jailbreaking Large Language Models with Symbolic Mathematics]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Automatic Jailbreaking of the Text-to-Image Generative AI Systems review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.16567v2
    [Automatic Jailbreaking of the Text-to-Image Generative AI Systems]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Latent Jailbreak A Benchmark for Evaluating Text Safety and Output Robustness
    of Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2307.08487v3
    [Latent Jailbreak A Benchmark for Evaluating Text Safety and Output Robustness
    of Large Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  - AMLT0057/LLMDataLeakage
  date: '2024-10-22'
- title: Agent Smith A Single Image Can Jailbreak One Million Multimodal LLM Agents
    Exponentially Fast review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.08567v2
    [Agent Smith A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially
    Fast]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0018/BackdoorMLModel
  - AMLT0020/PoisonTrainingData
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Break the Breakout Reinventing LM Defense Against Jailbreak Attacks with
    Self-Refinement review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.15180v2
    [Break the Breakout Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: Gemma Scope Open Sparse Autoencoders Everywhere All At Once on Gemma 2 review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.05147v2
    [Gemma Scope Open Sparse Autoencoders Everywhere All At Once on Gemma 2]_review.md
  tags:
  - AMLT0002/AcquirePublicMLArtifacts
  - AMLT0005/CreateProxyMLModel
  - AMLT0013/DiscoverMLModelOntology
  - AMLT0037/DataFromLocalSystem
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0044/FullMLModelAccess
  date: '2024-10-22'
- title: Eraser Jailbreaking Defense in Large Language Models via Unlearning Harmful
    Knowledge review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.05880v2
    [Eraser Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0018/BackdoorMLModel
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: The Threats of Embodied Multimodal LLMs Jailbreaking Robotic Manipulation
    in the Physical World review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.20242v2
    [The Threats of Embodied Multimodal LLMs Jailbreaking Robotic Manipulation in
    the Physical World]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Jailbreaking Quantum Computers review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.05941v1
    [Jailbreaking Quantum Computers]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0018/BackdoorMLModel
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0043/CraftAdversarialData
  - AMLT0044/FullMLModelAccess
  date: '2024-10-22'
- title: A StrongREJECT for Empty Jailbreaks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.10260v2
    [A StrongREJECT for Empty Jailbreaks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: ASETF A Novel Method for Jailbreak Attack on LLMs through Translate Suffix
    Embeddings review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.16006v2
    [ASETF A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0016/ObtainCapabilities
  - AMLT0017/DevelopCapabilities
  date: '2024-10-22'
- title: GradSafe Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient
    Analysis review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.13494v2
    [GradSafe Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Defensive Prompt Patch A Robust and Interpretable Defense of LLMs against
    Jailbreak Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.20099v1
    [Defensive Prompt Patch A Robust and Interpretable Defense of LLMs against Jailbreak
    Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: All in How You Ask for It Simple Black-Box Method for Jailbreak Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2401.09798v3
    [All in How You Ask for It Simple Black-Box Method for Jailbreak Attacks]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: How Johnny Can Persuade LLMs to Jailbreak Them Rethinking Persuasion to Challenge
    AI Safety by Humanizing LLMs review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2401.06373v2
    [How Johnny Can Persuade LLMs to Jailbreak Them Rethinking Persuasion to Challenge
    AI Safety by Humanizing LLMs]_review.md
  tags:
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  - AMLT0057/LLMDataLeakage
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: textit{MMJ-Bench} A Comprehensive Study on Jailbreak Attacks and Defenses
    for Vision Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.08464v1
    [textit{MMJ-Bench} A Comprehensive Study on Jailbreak Attacks and Defenses for
    Vision Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Investigating Coverage Criteria in Large Language Models An In-Depth Study
    Through Jailbreak Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.15207v1
    [Investigating Coverage Criteria in Large Language Models An In-Depth Study Through
    Jailbreak Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner A Vision
    Paper review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.15727v2
    [LLMs Can Defend Themselves Against Jailbreaking in a Practical Manner A Vision
    Paper]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Pandora Jailbreak GPTs by Retrieval Augmented Generation Poisoning review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.08416v1
    [Pandora Jailbreak GPTs by Retrieval Augmented Generation Poisoning]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Can LLMs Deeply Detect Complex Malicious Queries A Framework for Jailbreaking
    via Obfuscating Intent review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.03654v2
    [Can LLMs Deeply Detect Complex Malicious Queries A Framework for Jailbreaking
    via Obfuscating Intent]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: RePD Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition
    Process review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.08660v1
    [RePD Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition
    Process]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: GUARD Role-playing to Generate Natural-language Jailbreakings to Test Guideline
    Adherence of Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.03299v4
    [GUARD Role-playing to Generate Natural-language Jailbreakings to Test Guideline
    Adherence of Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: The Dark Side of Function Calling Pathways to Jailbreaking Large Language
    Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.17915v1
    [The Dark Side of Function Calling Pathways to Jailbreaking Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Prefix Guidance A Steering Wheel for Large Language Models to Defend Against
    Jailbreak Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.08924v2
    [Prefix Guidance A Steering Wheel for Large Language Models to Defend Against
    Jailbreak Attacks]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.09113v1
    [Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Understanding Jailbreak Success A Study of Latent Space Dynamics in Large
    Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.09289v1
    [Understanding Jailbreak Success A Study of Latent Space Dynamics in Large Language
    Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: Play Guessing Game with LLM Indirect Jailbreak Attack with Implicit Clues
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.09091v2
    [Play Guessing Game with LLM Indirect Jailbreak Attack with Implicit Clues]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: SelfDefend LLMs Can Defend Themselves against Jailbreaking in a Practical
    Manner review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.05498v1
    [SelfDefend LLMs Can Defend Themselves against Jailbreaking in a Practical Manner]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: WildGuard Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and
    Refusals of LLMs review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.18495v2
    [WildGuard Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals
    of LLMs]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0018/BackdoorMLModel
  - AMLT0020/PoisonTrainingData
  - AMLT0029/DenialOfMLService
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large
    Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.20775v1
    [Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language
    Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: States as Strings as Strategies Steering Language Models with Game-Theoretic
    Solvers review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.01704v2
    [States as Strings as Strategies Steering Language Models with Game-Theoretic
    Solvers]_review.md
  tags:
  - AMLT0005/CreateProxyMLModel
  - AMLT0015/EvadeMLModel
  - AMLT0016/ObtainCapabilities
  - AMLT0017/DevelopCapabilities
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Harnessing Task Overload for Scalable Jailbreak Attacks on Large Language
    Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.04190v1
    [Harnessing Task Overload for Scalable Jailbreak Attacks on Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: You Know What I'm Saying Jailbreak Attack via Implicit Reference review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.03857v2
    [You Know What I'm Saying Jailbreak Attack via Implicit Reference]_review.md
  tags:
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: Not Aligned is Not Malicious Being Careful about Hallucinations of Large
    Language Models' Jailbreak review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.11668v1
    [Not Aligned is Not Malicious Being Careful about Hallucinations of Large Language
    Models' Jailbreak]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Foot In The Door Understanding Large Language Model Jailbreaking via Cognitive
    Psychology review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.15690v1
    [Foot In The Door Understanding Large Language Model Jailbreaking via Cognitive
    Psychology]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Large Language Models Are Involuntary Truth-Tellers Exploiting Fallacy Failure
    for Jailbreak Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.00869v2
    [Large Language Models Are Involuntary Truth-Tellers Exploiting Fallacy Failure
    for Jailbreak Attacks]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0057/LLMDataLeakage
  date: '2024-10-22'
- title: WordGame Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation
    in Query and Response review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.14023v1
    [WordGame Efficient & Effective LLM Jailbreak via Simultaneous Obfuscation in
    Query and Response]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Defending Jailbreak Attack in VLMs via Cross-modality Information Detector
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.21659v2
    [Defending Jailbreak Attack in VLMs via Cross-modality Information Detector]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: SelfDefend LLMs Can Defend Themselves against Jailbreaking in a Practical
    Manner review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.05498v2
    [SelfDefend LLMs Can Defend Themselves against Jailbreaking in a Practical Manner]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.15221v2
    [LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Unlocking Adversarial Suffix Optimization Without Affirmative Phrases Efficient
    Black-box Jailbreaking via LLM as Optimizer review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.11313v1
    [Unlocking Adversarial Suffix Optimization Without Affirmative Phrases Efficient
    Black-box Jailbreaking via LLM as Optimizer]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0017/DevelopCapabilities
  date: '2024-10-22'
- title: GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.13077v1
    [GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety
    Alignment review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.14968v3
    [Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment]_review.md
  tags:
  - AMLT0018/BackdoorMLModel
  - AMLT0015/EvadeMLModel
  - AMLT0020/PoisonTrainingData
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: AttackEval How to Evaluate the Effectiveness of Jailbreak Attacking on Large
    Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2401.09002v5
    [AttackEval How to Evaluate the Effectiveness of Jailbreak Attacking on Large
    Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: JailbreakHunter A Visual Analytics Approach for Jailbreak Prompts Discovery
    from Large-Scale Human-LLM Conversational Datasets review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.03045v1
    [JailbreakHunter A Visual Analytics Approach for Jailbreak Prompts Discovery from
    Large-Scale Human-LLM Conversational Datasets]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0057/LLMDataLeakage
  date: '2024-10-22'
- title: MasterKey Automated Jailbreak Across Multiple Large Language Model Chatbots
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2307.08715v2
    [MasterKey Automated Jailbreak Across Multiple Large Language Model Chatbots]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  date: '2024-10-22'
- title: Jailbreak in pieces Compositional Adversarial Attacks on Multi-Modal Language
    Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2307.14539v2
    [Jailbreak in pieces Compositional Adversarial Attacks on Multi-Modal Language
    Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: A Realistic Threat Model for Large Language Model Jailbreaks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.16222v1
    [A Realistic Threat Model for Large Language Model Jailbreaks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: JailbreakZoo Survey, Landscapes, and Horizons in Jailbreaking Large Language
    and Vision-Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.01599v2
    [JailbreakZoo Survey, Landscapes, and Horizons in Jailbreaking Large Language
    and Vision-Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: From LLMs to MLLMs Exploring the Landscape of Multimodal Jailbreaking review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.14859v1
    [From LLMs to MLLMs Exploring the Landscape of Multimodal Jailbreaking]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Jigsaw Puzzles Splitting Harmful Questions to Jailbreak Large Language Models
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.11459v1
    [Jigsaw Puzzles Splitting Harmful Questions to Jailbreak Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: EEG-Defender Defending against Jailbreak through Early Exit Generation of
    Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.11308v1
    [EEG-Defender Defending against Jailbreak through Early Exit Generation of Large
    Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Weak-to-Strong Jailbreaking on Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2401.17256v2
    [Weak-to-Strong Jailbreaking on Large Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: How Alignment and Jailbreak Work Explain LLM Safety through Intermediate
    Hidden States review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.05644v2
    [How Alignment and Jailbreak Work Explain LLM Safety through Intermediate Hidden
    States]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0018/BackdoorMLModel
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Emerging Vulnerabilities in Frontier Models Multi-Turn Jailbreak Attacks
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2409.00137v1
    [Emerging Vulnerabilities in Frontier Models Multi-Turn Jailbreak Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: ObscurePrompt Jailbreaking Large Language Models via Obscure Input review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.13662v1
    [ObscurePrompt Jailbreaking Large Language Models via Obscure Input]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Universal Jailbreak Backdoors from Poisoned Human Feedback review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2311.14455v4
    [Universal Jailbreak Backdoors from Poisoned Human Feedback]_review.md
  tags:
  - AMLT0018/BackdoorMLModel
  - AMLT0020/PoisonTrainingData
  - AMLT0015/EvadeMLModel
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and
    Their Defenses review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.01288v1
    [Improved Few-Shot Jailbreaking Can Circumvent Aligned Language Models and Their
    Defenses]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Leveraging the Context through Multi-Round Interactions for Jailbreaking
    Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.09177v1
    [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.09326v1
    [Characterizing and Evaluating the Reliability of LLMs against Jailbreak Attacks]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.13077v2
    [GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Hide Your Malicious Goal Into Benign Narratives Jailbreak Large Language
    Models through Neural Carrier Articles review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.11182v1
    [Hide Your Malicious Goal Into Benign Narratives Jailbreak Large Language Models
    through Neural Carrier Articles]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0043/CraftAdversarialData
  - AMLT0015/EvadeMLModel
  - AMLT0005/CreateProxyMLModel
  date: '2024-10-22'
- title: When Do Universal Image Jailbreaks Transfer Between Vision-Language Models
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.15211v1
    [When Do Universal Image Jailbreaks Transfer Between Vision-Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0044/FullMLModelAccess
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Securing Vision-Language Models with a Robust Encoder Against Jailbreak and
    Adversarial Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2409.07353v1
    [Securing Vision-Language Models with a Robust Encoder Against Jailbreak and Adversarial
    Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0018/BackdoorMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Do LLMs Have Political Correctness Analyzing Ethical Biases and Jailbreak
    Vulnerabilities in AI Systems review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.13334v1
    [Do LLMs Have Political Correctness Analyzing Ethical Biases and Jailbreak Vulnerabilities
    in AI Systems]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Perception-guided Jailbreak against Text-to-Image Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.10848v1
    [Perception-guided Jailbreak against Text-to-Image Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0016/ObtainCapabilities
  - AMLT0017/DevelopCapabilities
  date: '2024-10-22'
- title: Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2310.06387v3
    [Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: $textit{MMJ-Bench}$ A Comprehensive Study on Jailbreak Attacks and Defenses
    for Vision Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.08464v3
    [$textit{MMJ-Bench}$ A Comprehensive Study on Jailbreak Attacks and Defenses for
    Vision Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: FlipAttack Jailbreak LLMs via Flipping review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.02832v1
    [FlipAttack Jailbreak LLMs via Flipping]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: When LLM Meets DRL Advancing Jailbreaking Efficiency via DRL-guided Search
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.08705v2
    [When LLM Meets DRL Advancing Jailbreaking Efficiency via DRL-guided Search]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0005/CreateProxyMLModel
  date: '2024-10-22'
- title: Comprehensive Assessment of Jailbreak Attacks Against LLMs review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.05668v1
    [Comprehensive Assessment of Jailbreak Attacks Against LLMs]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Testing the Limits of Jailbreaking Defenses with the Purple Problem review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2403.14725v2
    [Testing the Limits of Jailbreaking Defenses with the Purple Problem]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Don't Say No Jailbreaking LLM by Suppressing Refusal review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.16369v1
    [Don't Say No Jailbreaking LLM by Suppressing Refusal]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Gradient-based Jailbreak Images for Multimodal Fusion Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.03489v1
    [Gradient-based Jailbreak Images for Multimodal Fusion Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0044/FullMLModelAccess
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: MM-SafetyBench A Benchmark for Safety Evaluation of Multimodal Large Language
    Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2311.17600v5
    [MM-SafetyBench A Benchmark for Safety Evaluation of Multimodal Large Language
    Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Human-Interpretable Adversarial Prompt Attack on Large Language Models with
    Situational Context review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.14644v2
    [Human-Interpretable Adversarial Prompt Attack on Large Language Models with Situational
    Context]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: The Butterfly Effect of Altering Prompts How Small Changes and Jailbreaks
    Affect Large Language Model Performance review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2401.03729v3
    [The Butterfly Effect of Altering Prompts How Small Changes and Jailbreaks Affect
    Large Language Model Performance]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0040/MLModelInferenceAPIAccess
  date: '2024-10-22'
- title: Do Anything Now Characterizing and Evaluating In-The-Wild Jailbreak Prompts
    on Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2308.03825v2
    [Do Anything Now Characterizing and Evaluating In-The-Wild Jailbreak Prompts on
    Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0057/LLMDataLeakage
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: Tree of Attacks Jailbreaking Black-Box LLMs Automatically review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2312.02119v2
    [Tree of Attacks Jailbreaking Black-Box LLMs Automatically]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: PARDEN, Can You Repeat That Defending against Jailbreaks via Repetition review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.07932v2
    [PARDEN, Can You Repeat That Defending against Jailbreaks via Repetition]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Jailbreaking ChatGPT via Prompt Engineering An Empirical Study review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2305.13860v2
    [Jailbreaking ChatGPT via Prompt Engineering An Empirical Study]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: DeCE Deceptive Cross-Entropy Loss Designed for Defending Backdoor Attacks
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.08956v1
    [DeCE Deceptive Cross-Entropy Loss Designed for Defending Backdoor Attacks]_review.md
  tags:
  - AMLT0018/BackdoorMLModel
  - AMLT0020/PoisonTrainingData
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: JailbreakLens Visual Analysis of Jailbreak Attacks Against Large Language
    Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.08793v1
    [JailbreakLens Visual Analysis of Jailbreak Attacks Against Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0042/VerifyAttack
  - AMLT0040/MLModelInferenceAPIAccess
  date: '2024-10-22'
- title: Leveraging the Context through Multi-Round Interactions for Jailbreaking
    Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.09177v2
    [Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  date: '2024-10-22'
- title: Tastle Distract Large Language Models for Automatic Jailbreak Attack review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2403.08424v1
    [Tastle Distract Large Language Models for Automatic Jailbreak Attack]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.15221v1
    [LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: SafeAligner Safety Alignment against Jailbreak Attacks via Response Disparity
    Guidance review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.18118v2
    [SafeAligner Safety Alignment against Jailbreak Attacks via Response Disparity
    Guidance]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Fight Back Against Jailbreaking via Prompt Adversarial Tuning review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.06255v3
    [Fight Back Against Jailbreaking via Prompt Adversarial Tuning]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: JailBreakV-28K A Benchmark for Assessing the Robustness of MultiModal Large
    Language Models against Jailbreak Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.03027v3
    [JailBreakV-28K A Benchmark for Assessing the Robustness of MultiModal Large Language
    Models against Jailbreak Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Virtual Context Enhancing Jailbreak Attacks with Special Token Injection
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.19845v2
    [Virtual Context Enhancing Jailbreak Attacks with Special Token Injection]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: RT-Attack Jailbreaking Text-to-Image Models via Random Token review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.13896v2
    [RT-Attack Jailbreaking Text-to-Image Models via Random Token]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0016/ObtainCapabilities
  - AMLT0017/DevelopCapabilities
  date: '2024-10-22'
- title: The Dark Side of Function Calling Pathways to Jailbreaking Large Language
    Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.17915v2
    [The Dark Side of Function Calling Pathways to Jailbreaking Large Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0057/LLMDataLeakage
  date: '2024-10-22'
- title: Jailbreaking LLMs with Arabic Transliteration and Arabizi review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.18725v2
    [Jailbreaking LLMs with Arabic Transliteration and Arabizi]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0057/LLMDataLeakage
  date: '2024-10-22'
- title: STAR SocioTechnical Approach to Red Teaming Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.11757v3
    [STAR SocioTechnical Approach to Red Teaming Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Tricking LLMs into Disobedience Formalizing, Analyzing, and Detecting Jailbreaks
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2305.14965v4
    [Tricking LLMs into Disobedience Formalizing, Analyzing, and Detecting Jailbreaks]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0056/LLMMetaPromptExtraction
  - AMLT0057/LLMDataLeakage
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: CodeChameleon Personalized Encryption Framework for Jailbreaking Large Language
    Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.16717v1
    [CodeChameleon Personalized Encryption Framework for Jailbreaking Large Language
    Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Can Large Language Models Automatically Jailbreak GPT-4V review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.16686v1
    [Can Large Language Models Automatically Jailbreak GPT-4V]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: ArtPrompt ASCII Art-based Jailbreak Attacks against Aligned LLMs review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.11753v4
    [ArtPrompt ASCII Art-based Jailbreak Attacks against Aligned LLMs]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Semantic Mirror Jailbreak Genetic Algorithm Based Jailbreak Prompts Against
    Open-source LLMs review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.14872v2
    [Semantic Mirror Jailbreak Genetic Algorithm Based Jailbreak Prompts Against Open-source
    LLMs]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: When LLM Meets DRL Advancing Jailbreaking Efficiency via DRL-guided Search
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.08705v1
    [When LLM Meets DRL Advancing Jailbreaking Efficiency via DRL-guided Search]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Accelerating Greedy Coordinate Gradient via Probe Sampling review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2403.01251v2
    [Accelerating Greedy Coordinate Gradient via Probe Sampling]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Defending LLMs against Jailbreaking Attacks via Backtranslation review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.16459v3
    [Defending LLMs against Jailbreaking Attacks via Backtranslation]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Figure it Out Analyzing-based Jailbreak Attack on Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.16205v3
    [Figure it Out Analyzing-based Jailbreak Attack on Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: A Comprehensive Study of Jailbreak Attack versus Defense for Large Language
    Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.13457v2
    [A Comprehensive Study of Jailbreak Attack versus Defense for Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0042/VerifyAttack
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Defending Large Language Models Against Jailbreak Attacks via Layer-specific
    Editing review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.18166v2
    [Defending Large Language Models Against Jailbreak Attacks via Layer-specific
    Editing]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Why Are My Prompts Leaked Unraveling Prompt Extraction Threats in Customized
    Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.02416v1
    [Why Are My Prompts Leaked Unraveling Prompt Extraction Threats in Customized
    Large Language Models]_review.md
  tags:
  - AMLT0056/LLMMetaPromptExtraction
  - AMLT0057/LLMDataLeakage
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: MoJE Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for
    Prompt Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2409.17699v3
    [MoJE Mixture of Jailbreak Experts, Naive Tabular Classifiers as Guard for Prompt
    Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: SoP Unlock the Power of Social Facilitation for Automatic Jailbreak Attack
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.01902v1
    [SoP Unlock the Power of Social Facilitation for Automatic Jailbreak Attack]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  - AMLT0016/ObtainCapabilities
  - AMLT0017/DevelopCapabilities
  date: '2024-10-22'
- title: Gradient Cuff Detecting Jailbreak Attacks on Large Language Models by Exploring
    Refusal Loss Landscapes review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2403.00867v2
    [Gradient Cuff Detecting Jailbreak Attacks on Large Language Models by Exploring
    Refusal Loss Landscapes]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Perception-guided Jailbreak against Text-to-Image Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.10848v2
    [Perception-guided Jailbreak against Text-to-Image Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: DrAttack Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.16914v2
    [DrAttack Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.02151v2
    [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Refusal in Language Models Is Mediated by a Single Direction review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.11717v2
    [Refusal in Language Models Is Mediated by a Single Direction]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0044/FullMLModelAccess
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: AVIBench Towards Evaluating the Robustness of Large Vision-Language Model
    on Adversarial Visual-Instructions review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2403.09346v1
    [AVIBench Towards Evaluating the Robustness of Large Vision-Language Model on
    Adversarial Visual-Instructions]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0029/DenialOfMLService
  - AMLT0057/LLMDataLeakage
  date: '2024-10-22'
- title: Images are Achilles' Heel of Alignment Exploiting Visual Vulnerabilities
    for Jailbreaking Multimodal Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2403.09792v2
    [Images are Achilles' Heel of Alignment Exploiting Visual Vulnerabilities for
    Jailbreaking Multimodal Large Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Jailbreak Attacks and Defenses Against Large Language Models A Survey review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.04295v1
    [Jailbreak Attacks and Defenses Against Large Language Models A Survey]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: A StrongREJECT for Empty Jailbreaks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.10260v1
    [A StrongREJECT for Empty Jailbreaks]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Defending Large Language Models Against Jailbreaking Attacks Through Goal
    Prioritization review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2311.09096v2
    [Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: Enhancing Jailbreak Attacks with Diversity Guidance review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2403.00292v2
    [Enhancing Jailbreak Attacks with Diversity Guidance]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0044/FullMLModelAccess
  date: '2024-10-22'
- title: RL-JACK Reinforcement Learning-powered Black-box Jailbreaking Attack against
    LLMs review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.08725v1
    [RL-JACK Reinforcement Learning-powered Black-box Jailbreaking Attack against
    LLMs]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0005/CreateProxyMLModel
  date: '2024-10-22'
- title: RoleBreak Character Hallucination as a Jailbreak Attack in Role-Playing Systems
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2409.16727v1
    [RoleBreak Character Hallucination as a Jailbreak Attack in Role-Playing Systems]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0020/PoisonTrainingData
  date: '2024-10-22'
- title: Low-Resource Languages Jailbreak GPT-4 review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2310.02446v2
    [Low-Resource Languages Jailbreak GPT-4]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Deciphering the Chaos Enhancing Jailbreak Attacks via Adversarial Prompt
    Translation review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.11317v1
    [Deciphering the Chaos Enhancing Jailbreak Attacks via Adversarial Prompt Translation]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Hacc-Man An Arcade Game for Jailbreaking LLMs review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.15902v1
    [Hacc-Man An Arcade Game for Jailbreaking LLMs]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0040/MLModelInferenceAPIAccess
  date: '2024-10-22'
- title: A Cross-Language Investigation into Jailbreak Attacks in Large Language Models
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2401.16765v1
    [A Cross-Language Investigation into Jailbreak Attacks in Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Bag of Tricks Benchmarking of Jailbreak Attacks on LLMs review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.09324v1
    [Bag of Tricks Benchmarking of Jailbreak Attacks on LLMs]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: ALERT A Comprehensive Benchmark for Assessing Large Language Models' Safety
    through Red Teaming review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.08676v3
    [ALERT A Comprehensive Benchmark for Assessing Large Language Models' Safety through
    Red Teaming]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0042/VerifyAttack
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Fluent Student-Teacher Redteaming review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.17447v1
    [Fluent Student-Teacher Redteaming]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0016/ObtainCapabilities
  - AMLT0017/DevelopCapabilities
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Jailbreak Antidote Runtime Safety-Utility Balance via Sparse Representation
    Adjustment in Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.02298v2
    [Jailbreak Antidote Runtime Safety-Utility Balance via Sparse Representation Adjustment
    in Large Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: The Dark Side of Function Calling Pathways to Jailbreaking Large Language
    Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.17915v3
    [The Dark Side of Function Calling Pathways to Jailbreaking Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Jailbreaking LLM-Controlled Robots review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.13691v1
    [Jailbreaking LLM-Controlled Robots]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: JAILJUDGE A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced
    Explanation Evaluation Framework review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.12855v2
    [JAILJUDGE A Comprehensive Jailbreak Judge Benchmark with Multi-Agent Enhanced
    Explanation Evaluation Framework]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Universal and Transferable Adversarial Attacks on Aligned Language Models
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2307.15043v2
    [Universal and Transferable Adversarial Attacks on Aligned Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0057/LLMDataLeakage
  date: '2024-10-22'
- title: Jailbreak Instruction-Tuned LLMs via end-of-sentence MLP Re-weighting review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.10150v1
    [Jailbreak Instruction-Tuned LLMs via end-of-sentence MLP Re-weighting]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  - AMLT0044/FullMLModelAccess
  date: '2024-10-22'
- title: SMILES-Prompting A Novel Approach to LLM Jailbreak Attacks in Chemical Synthesis
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.15641v1
    [SMILES-Prompting A Novel Approach to LLM Jailbreak Attacks in Chemical Synthesis]_review.md
  tags:
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: h4rm3l A Dynamic Benchmark of Composable Jailbreak Attacks for LLM Safety
    Assessment review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.04811v1
    [h4rm3l A Dynamic Benchmark of Composable Jailbreak Attacks for LLM Safety Assessment]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Unleashing Worms and Extracting Data Escalating the Outcome of Attacks against
    RAG-based Inference in Scale and Severity Using Jailbreaking review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2409.08045v1
    [Unleashing Worms and Extracting Data Escalating the Outcome of Attacks against
    RAG-based Inference in Scale and Severity Using Jailbreaking]_review.md
  tags:
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  - AMLT0057/LLMDataLeakage
  - AMLT0015/EvadeMLModel
  - AMLT0020/PoisonTrainingData
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: Image-to-Text Logic Jailbreak Your Imagination can Help You Do Anything review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.02534v2
    [Image-to-Text Logic Jailbreak Your Imagination can Help You Do Anything]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: Is the System Message Really Important to Jailbreaks in Large Language Models
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.14857v2
    [Is the System Message Really Important to Jailbreaks in Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0017/DevelopCapabilities
  date: '2024-10-22'
- title: A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.16155v1
    [A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0018/BackdoorMLModel
  - AMLT0020/PoisonTrainingData
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: EasyJailbreak A Unified Framework for Jailbreaking Large Language Models
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2403.12171v1
    [EasyJailbreak A Unified Framework for Jailbreaking Large Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0016/ObtainCapabilities
  - AMLT0017/DevelopCapabilities
  date: '2024-10-22'
- title: Learning To See But Forgetting To Follow Visual Instruction Tuning Makes
    LLMs More Prone To Jailbreak Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.04403v1
    [Learning To See But Forgetting To Follow Visual Instruction Tuning Makes LLMs
    More Prone To Jailbreak Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0057/LLMDataLeakage
  date: '2024-10-22'
- title: Jailbreaking Prompt Attack A Controllable Adversarial Attack against Diffusion
    Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.02928v2
    [Jailbreaking Prompt Attack A Controllable Adversarial Attack against Diffusion
    Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0016/ObtainCapabilities
  - AMLT0017/DevelopCapabilities
  date: '2024-10-22'
- title: Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and
    Echopraxia review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.05467v1
    [Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  - AMLT0040/MLModelInferenceAPIAccess
  date: '2024-10-22'
- title: Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.04686v1
    [Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: SneakyPrompt Jailbreaking Text-to-image Generative Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2305.12082v3
    [SneakyPrompt Jailbreaking Text-to-image Generative Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0017/DevelopCapabilities
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: A Wolf in Sheep's Clothing Generalized Nested Jailbreak Prompts can Fool
    Large Language Models Easily review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2311.08268v4
    [A Wolf in Sheep's Clothing Generalized Nested Jailbreak Prompts can Fool Large
    Language Models Easily]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0043/CraftAdversarialData
  - AMLT0015/EvadeMLModel
  date: '2024-10-22'
- title: JailbreakBench An Open Robustness Benchmark for Jailbreaking Large Language
    Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.01318v4
    [JailbreakBench An Open Robustness Benchmark for Jailbreaking Large Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  - AMLT0057/LLMDataLeakage
  date: '2024-10-22'
- title: Soft Begging Modular and Efficient Shielding of LLMs against Prompt Injection
    and Jailbreaking based on Prompt Tuning review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.03391v1
    [Soft Begging Modular and Efficient Shielding of LLMs against Prompt Injection
    and Jailbreaking based on Prompt Tuning]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Boosting Jailbreak Attack with Momentum review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.01229v1
    [Boosting Jailbreak Attack with Momentum]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Robust Prompt Optimization for Defending Language Models Against Jailbreaking
    Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2401.17263v4
    [Robust Prompt Optimization for Defending Language Models Against Jailbreaking
    Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: WildTeaming at Scale From In-the-Wild Jailbreaks to (Adversarially) Safer
    Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.18510v1
    [WildTeaming at Scale From In-the-Wild Jailbreaks to (Adversarially) Safer Language
    Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0020/PoisonTrainingData
  date: '2024-10-22'
- title: Aurora-M The First Open Source Multilingual Language Model Red-teamed according
    to the U.S. Executive Order review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.00399v2
    [Aurora-M The First Open Source Multilingual Language Model Red-teamed according
    to the U.S. Executive Order]_review.md
  tags:
  - AMLT0018/BackdoorMLModel
  - AMLT0020/PoisonTrainingData
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Don't Listen To Me Understanding and Exploring Jailbreak Prompts of Large
    Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2403.17336v1
    [Don't Listen To Me Understanding and Exploring Jailbreak Prompts of Large Language
    Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona
    Modulation review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2311.03348v2
    [Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona
    Modulation]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  date: '2024-10-22'
- title: Red Teaming GPT-4V Are GPT-4V Safe Against UniMulti-Modal Jailbreak Attacks
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.03411v1
    [Red Teaming GPT-4V Are GPT-4V Safe Against UniMulti-Modal Jailbreak Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0057/LLMDataLeakage
  date: '2024-10-22'
- title: $textit{MMJ-Bench}$ A Comprehensive Study on Jailbreak Attacks and Defenses
    for Vision Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.08464v2
    [$textit{MMJ-Bench}$ A Comprehensive Study on Jailbreak Attacks and Defenses for
    Vision Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0040/MLModelInferenceAPIAccess
  date: '2024-10-22'
- title: Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.16192v2
    [Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2311.09127v2
    [Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts]_review.md
  tags:
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  - AMLT0056/LLMMetaPromptExtraction
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Faster-GCG Efficient Discrete Optimization Jailbreak Attacks against Aligned
    Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.15362v1
    [Faster-GCG Efficient Discrete Optimization Jailbreak Attacks against Aligned
    Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0016/ObtainCapabilities
  - AMLT0017/DevelopCapabilities
  date: '2024-10-22'
- title: Multi-step Jailbreaking Privacy Attacks on ChatGPT review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2304.05197v3
    [Multi-step Jailbreaking Privacy Attacks on ChatGPT]_review.md
  tags:
  - AMLT0057/LLMDataLeakage
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0040/MLModelInferenceAPIAccess
  date: '2024-10-22'
- title: DeCE Deceptive Cross-Entropy Loss Designed for Defending Backdoor Attacks
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.08956v2
    [DeCE Deceptive Cross-Entropy Loss Designed for Defending Backdoor Attacks]_review.md
  tags:
  - AMLT0018/BackdoorMLModel
  - AMLT0020/PoisonTrainingData
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: Cross-modality Information Check for Detecting Jailbreaking in Multimodal
    Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.21659v4
    [Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large
    Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Hidden You Malicious Goal Into Benign Narratives Jailbreak Large Language
    Models through Logic Chain Injection review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.04849v2
    [Hidden You Malicious Goal Into Benign Narratives Jailbreak Large Language Models
    through Logic Chain Injection]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0043/CraftAdversarialData
  - AMLT0015/EvadeMLModel
  date: '2024-10-22'
- title: AdaPPA Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2409.07503v1
    [AdaPPA Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Functional Homotopy Smoothing Discrete Optimization via Continuous Parameters
    for LLM Jailbreak Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.04234v1
    [Functional Homotopy Smoothing Discrete Optimization via Continuous Parameters
    for LLM Jailbreak Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Jailbreaking Proprietary Large Language Models using Word Substitution Cipher
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.10601v1
    [Jailbreaking Proprietary Large Language Models using Word Substitution Cipher]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Breaking Agents Compromising Autonomous LLM Agents Through Malfunction Amplification
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.20859v1
    [Breaking Agents Compromising Autonomous LLM Agents Through Malfunction Amplification]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0029/DenialOfMLService
  - AMLT0040/MLModelInferenceAPIAccess
  date: '2024-10-22'
- title: RED QUEEN Safeguarding Large Language Models against Concealed Multi-Turn
    Jailbreaking review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2409.17458v1
    [RED QUEEN Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: EnJa Ensemble Jailbreak on Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.03603v1
    [EnJa Ensemble Jailbreak on Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Cognitive Overload Jailbreaking Large Language Models with Overloaded Logical
    Thinking review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2311.09827v2
    [Cognitive Overload Jailbreaking Large Language Models with Overloaded Logical
    Thinking]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language
    Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.13796v1
    [Continuous Embedding Attacks via Clipped Inputs in Jailbreaking Large Language
    Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0040/MLModelInferenceAPIAccess
  date: '2024-10-22'
- title: Don't Say No Jailbreaking LLM by Suppressing Refusal review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.16369v2
    [Don't Say No Jailbreaking LLM by Suppressing Refusal]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: Understanding Jailbreak Success A Study of Latent Space Dynamics in Large
    Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.09289v2
    [Understanding Jailbreak Success A Study of Latent Space Dynamics in Large Language
    Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Medical MLLM is Vulnerable Cross-Modality Jailbreak and Mismatched Attacks
    on Medical Multimodal Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.20775v2
    [Medical MLLM is Vulnerable Cross-Modality Jailbreak and Mismatched Attacks on
    Medical Multimodal Large Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Defending Jailbreak Prompts via In-Context Adversarial Game review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.13148v2
    [Defending Jailbreak Prompts via In-Context Adversarial Game]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Jailbreaking Prompt Attack A Controllable Adversarial Attack against Diffusion
    Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.02928v3
    [Jailbreaking Prompt Attack A Controllable Adversarial Attack against Diffusion
    Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0016/ObtainCapabilities
  - AMLT0017/DevelopCapabilities
  date: '2024-10-22'
- title: Jailbreaking Text-to-Image Models with LLM-Based Agents review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.00523v1
    [Jailbreaking Text-to-Image Models with LLM-Based Agents]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0005/CreateProxyMLModel
  date: '2024-10-22'
- title: Jailbreaker in Jail Moving Target Defense for Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2310.02417v1
    [Jailbreaker in Jail Moving Target Defense for Large Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0029/DenialOfMLService
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: Bag of Tricks Benchmarking of Jailbreak Attacks on LLMs review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.09324v2
    [Bag of Tricks Benchmarking of Jailbreak Attacks on LLMs]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Prefix Guidance A Steering Wheel for Large Language Models to Defend Against
    Jailbreak Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.08924v1
    [Prefix Guidance A Steering Wheel for Large Language Models to Defend Against
    Jailbreak Attacks]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Lockpicking LLMs A Logit-Based Jailbreak Using Token-level Manipulation review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.13068v2
    [Lockpicking LLMs A Logit-Based Jailbreak Using Token-level Manipulation]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0044/FullMLModelAccess
  date: '2024-10-22'
- title: White-box Multimodal Jailbreaks Against Large Vision-Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.17894v2
    [White-box Multimodal Jailbreaks Against Large Vision-Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0044/FullMLModelAccess
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Jailbreaking LLMs with Arabic Transliteration and Arabizi review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.18725v1
    [Jailbreaking LLMs with Arabic Transliteration and Arabizi]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0057/LLMDataLeakage
  - AMLT0015/EvadeMLModel
  date: '2024-10-22'
- title: Fight Back Against Jailbreaking via Prompt Adversarial Tuning review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.06255v2
    [Fight Back Against Jailbreaking via Prompt Adversarial Tuning]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: BlackDAN A Black-Box Multi-Objective Approach for Effective and Contextual
    Jailbreaking of Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.09804v1
    [BlackDAN A Black-Box Multi-Objective Approach for Effective and Contextual Jailbreaking
    of Large Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: AutoJailbreak Exploring Jailbreak Attacks and Defenses through a Dependency
    Lens review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2406.03805v1
    [AutoJailbreak Exploring Jailbreak Attacks and Defenses through a Dependency Lens]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: Jailbreaking Black Box Large Language Models in Twenty Queries review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2310.08419v4
    [Jailbreaking Black Box Large Language Models in Twenty Queries]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: ImgTrojan Jailbreaking Vision-Language Models with ONE Image review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2403.02910v2
    [ImgTrojan Jailbreaking Vision-Language Models with ONE Image]_review.md
  tags:
  - AMLT0018/BackdoorMLModel
  - AMLT0020/PoisonTrainingData
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: h4rm3l A Dynamic Benchmark of Composable Jailbreak Attacks for LLM Safety
    Assessment review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.04811v2
    [h4rm3l A Dynamic Benchmark of Composable Jailbreak Attacks for LLM Safety Assessment]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0017/DevelopCapabilities
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Efficient LLM-Jailbreaking by Introducing Visual Modality review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2405.20015v1
    [Efficient LLM-Jailbreaking by Introducing Visual Modality]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  - AMLT0005/CreateProxyMLModel
  date: '2024-10-22'
- title: Cross-modality Information Check for Detecting Jailbreaking in Multimodal
    Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.21659v3
    [Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large
    Language Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: Mission Impossible A Statistical Perspective on Jailbreaking LLMs review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.01420v1
    [Mission Impossible A Statistical Perspective on Jailbreaking LLMs]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: Subtoxic Questions Dive Into Attitude Change of LLM's Response in Jailbreak
    Attempts review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.08309v1
    [Subtoxic Questions Dive Into Attitude Change of LLM's Response in Jailbreak Attempts]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  date: '2024-10-22'
- title: FigStep Jailbreaking Large Vision-language Models via Typographic Visual
    Prompts review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2311.05608v2
    [FigStep Jailbreaking Large Vision-language Models via Typographic Visual Prompts]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0054/LLMJailbreak
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  date: '2024-10-22'
- title: Don't Listen To Me Understanding and Exploring Jailbreak Prompts of Large
    Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2403.17336v2
    [Don't Listen To Me Understanding and Exploring Jailbreak Prompts of Large Language
    Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2404.02151v3
    [Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0040/MLModelInferenceAPIAccess
  date: '2024-10-22'
- title: PathSeeker Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based
    Jailbreak Approach review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2409.14177v2
    [PathSeeker Exploring LLM Security Vulnerabilities with a Reinforcement Learning-Based
    Jailbreak Approach]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Round Trip Translation Defence against Large Language Model Jailbreaking
    Attacks review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2402.13517v1
    [Round Trip Translation Defence against Large Language Model Jailbreaking Attacks]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
- title: DeepInception Hypnotize Large Language Model to Be Jailbreaker review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2311.03191v4
    [DeepInception Hypnotize Large Language Model to Be Jailbreaker]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0051/LLMPromptInjection
  - AMLT0043/CraftAdversarialData
  date: '2024-10-22'
- title: Badllama 3 removing safety finetuning from Llama 3 in minutes review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2407.01376v1
    [Badllama 3 removing safety finetuning from Llama 3 in minutes]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0018/BackdoorMLModel
  - AMLT0031/ErodeMLModelIntegrity
  - AMLT0043/CraftAdversarialData
  - AMLT0044/FullMLModelAccess
  date: '2024-10-22'
- title: Jailbreaking and Mitigation of Vulnerabilities in Large Language Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2410.15236v1
    [Jailbreaking and Mitigation of Vulnerabilities in Large Language Models]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0051/LLMPromptInjection
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0031/ErodeMLModelIntegrity
  date: '2024-10-22'
- title: Effective and Evasive Fuzz Testing-Driven Jailbreaking Attacks against LLMs
    review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2409.14866v2
    [Effective and Evasive Fuzz Testing-Driven Jailbreaking Attacks against LLMs]_review.md
  tags:
  - AMLT0054/LLMJailbreak
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0040/MLModelInferenceAPIAccess
  - AMLT0042/VerifyAttack
  date: '2024-10-22'
- title: Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language
    Models review
  link: https://raw.githubusercontent.com/dyngnosis/dyngnosis.github.io/main/ml/red-team/2408.04522v1
    [Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large Language
    Models]_review.md
  tags:
  - AMLT0015/EvadeMLModel
  - AMLT0043/CraftAdversarialData
  - AMLT0054/LLMJailbreak
  date: '2024-10-22'
