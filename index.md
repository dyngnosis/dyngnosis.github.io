---
layout: research
title: AI Security Research Hub
---

# AI Security Research Hub

Welcome to the AI Security Research Hub, a comprehensive resource for cutting-edge research in AI security, with a focus on red teaming and jailbreaking attacks on large language models.

## Featured Research Areas

### Red Teaming and Jailbreaking LLMs

Explore our extensive collection of research reviews on red teaming techniques, jailbreaking attacks, and defense mechanisms for large language models:

#### Latest Reviews

1. [Emerging Vulnerabilities in Frontier Models: Multi-Turn Jailbreak Attacks](./ml/red-team/2409.00137v1%20[Emerging%20Vulnerabilities%20in%20Frontier%20Models%20Multi-Turn%20Jailbreak%20Attacks]_review.html)
2. [MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models](./ml/red-team/2311.17600v5%20[MM-SafetyBench%20A%20Benchmark%20for%20Safety%20Evaluation%20of%20Multimodal%20Large%20Language%20Models]_review.html)
3. [Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt](./ml/red-team/2406.04031v2%20[Jailbreak%20Vision%20Language%20Models%20via%20Bi-Modal%20Adversarial%20Prompt]_review.html)

#### Comprehensive Surveys

- [Jailbreak Attacks and Defenses Against Large Language Models: A Survey](./ml/red-team/2407.04295v2%20[Jailbreak%20Attacks%20and%20Defenses%20Against%20Large%20Language%20Models%20A%20Survey]_review.html)
- [JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models](./ml/red-team/2407.01599v2%20[JailbreakZoo%20Survey,%20Landscapes,%20and%20Horizons%20in%20Jailbreaking%20Large%20Language%20and%20Vision-Language%20Models]_review.html)

#### Defense Mechanisms

- [SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding](./ml/red-team/2402.08983v4%20[SafeDecoding%20Defending%20against%20Jailbreak%20Attacks%20via%20Safety-Aware%20Decoding]_review.html)
- [Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks](./ml/red-team/2401.17263v4%20[Robust%20Prompt%20Optimization%20for%20Defending%20Language%20Models%20Against%20Jailbreaking%20Attacks]_review.html)

[View All Red Teaming Reviews](./ml/red-team/)

### Other AI Security Topics

- [Machine Learning Security](./ml/machine-learning.html)
- [Reverse Engineering in AI](./reversing/reverse-engineering.html)
- [Vulnerability Research in AI Systems](./vulns/vulnerability-research.html)
- [AI-Powered Malware Analysis](./malware/malware-research.html)

## Recent Updates

Stay informed about the latest additions to our research collection:

1. [AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks](./ml/red-team/2403.04783v1%20[AutoDefense%20Multi-Agent%20LLM%20Defense%20against%20Jailbreak%20Attacks]_review.html) - Added on September 8, 2024
2. [Jailbreaking Text-to-Image Models with LLM-Based Agents](./ml/red-team/2408.00523v2%20[Jailbreaking%20Text-to-Image%20Models%20with%20LLM-Based%20Agents]_review.html) - Added on September 5, 2024
3. [Adversarial Tuning: Defending Against Jailbreak Attacks for LLMs](./ml/red-team/2406.06622v1%20[Adversarial%20Tuning%20Defending%20Against%20Jailbreak%20Attacks%20for%20LLMs]_review.html) - Added on September 1, 2024

## About This Hub

This research hub is dedicated to advancing the field of AI security through comprehensive reviews of cutting-edge research. Our focus is on red teaming techniques, jailbreaking attacks, and defense mechanisms for large language models, as well as broader topics in AI security.

## Contribute

We welcome contributions from researchers and practitioners in the field of AI security. If you have a paper review or analysis you'd like to contribute, please [contact us](mailto:jeremy@richards.ai).

## Stay Connected

- [Subscribe to our Newsletter](https://example.com/newsletter)
- Follow us on [Twitter](https://twitter.com/aisecurityhub)
- Join our [LinkedIn Group](https://www.linkedin.com/groups/aisecurityresearch)

---

Â© 2024 AI Security Research Hub | [Privacy Policy](./privacy.html) | [Terms of Use](./terms.html)